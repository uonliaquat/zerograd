{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54411de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 995]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f23eb79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['errors',\n",
       " 'Ġ(',\n",
       " 'str',\n",
       " ',',\n",
       " 'Ġoptional',\n",
       " ',',\n",
       " 'Ġdefaults',\n",
       " 'Ġto',\n",
       " 'Par',\n",
       " 'ad',\n",
       " 'igm',\n",
       " 'Ġto',\n",
       " 'Ġfollow',\n",
       " 'Ġwhen',\n",
       " 'Ġdecoding',\n",
       " 'Ġbytes',\n",
       " 'Ġto',\n",
       " 'ĠUTF',\n",
       " '-',\n",
       " '8',\n",
       " '.',\n",
       " 'ĠSee',\n",
       " 'Ġbytes',\n",
       " '.',\n",
       " 'dec',\n",
       " 'ode',\n",
       " 'Ġfor',\n",
       " 'Ġmore',\n",
       " 'Ġinformation',\n",
       " '.',\n",
       " 'Ġun',\n",
       " 'k',\n",
       " '_',\n",
       " 'token',\n",
       " 'Ġ(',\n",
       " 'str',\n",
       " ',',\n",
       " 'Ġoptional',\n",
       " ',',\n",
       " 'Ġdefaults',\n",
       " 'Ġto',\n",
       " 'ĠâĢĶ',\n",
       " 'ĠThe',\n",
       " 'Ġunknown',\n",
       " 'Ġtoken',\n",
       " '.',\n",
       " 'ĠA',\n",
       " 'Ġtoken',\n",
       " 'Ġthat',\n",
       " 'Ġis',\n",
       " 'Ġnot',\n",
       " 'Ġin',\n",
       " 'Ġthe',\n",
       " 'Ġvocabulary',\n",
       " 'Ġcannot',\n",
       " 'Ġbe',\n",
       " 'Ġconverted',\n",
       " 'Ġto',\n",
       " 'Ġan',\n",
       " 'ĠID',\n",
       " 'Ġand',\n",
       " 'Ġis',\n",
       " 'Ġset',\n",
       " 'Ġto',\n",
       " 'Ġbe',\n",
       " 'Ġthis',\n",
       " 'Ġtoken',\n",
       " 'Ġinstead',\n",
       " '.',\n",
       " 'bos',\n",
       " '_',\n",
       " 'token',\n",
       " 'Ġ(',\n",
       " 'str',\n",
       " ',',\n",
       " 'Ġoptional',\n",
       " ',',\n",
       " 'Ġdefaults',\n",
       " 'Ġto',\n",
       " 'Ġ',\n",
       " 'ĠâĢĶ',\n",
       " 'ĠThe',\n",
       " 'Ġbeginning',\n",
       " 'Ġof',\n",
       " 'Ġsequence',\n",
       " 'Ġtoken',\n",
       " '.',\n",
       " 'Ġe',\n",
       " 'os',\n",
       " '_',\n",
       " 'token',\n",
       " 'Ġ(',\n",
       " 'str',\n",
       " ',',\n",
       " 'Ġoptional',\n",
       " ',',\n",
       " 'Ġdefaults',\n",
       " 'Ġto',\n",
       " 'ĠâĢĶ',\n",
       " 'ĠThe',\n",
       " 'Ġend',\n",
       " 'Ġof',\n",
       " 'Ġsequence',\n",
       " 'Ġtoken',\n",
       " '.',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'prefix',\n",
       " '_',\n",
       " 'space',\n",
       " 'Ġ(',\n",
       " 'bool',\n",
       " ',',\n",
       " 'Ġoptional',\n",
       " ',',\n",
       " 'Ġdefaults',\n",
       " 'Ġto',\n",
       " 'ĠFalse',\n",
       " ')',\n",
       " 'ĠâĢĶ',\n",
       " 'ĠWhether',\n",
       " 'Ġor',\n",
       " 'Ġnot',\n",
       " 'Ġto',\n",
       " 'Ġadd',\n",
       " 'Ġan',\n",
       " 'Ġinitial',\n",
       " 'Ġspace',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġinput',\n",
       " '.',\n",
       " 'ĠThis',\n",
       " 'Ġallows',\n",
       " 'Ġto',\n",
       " 'Ġtreat',\n",
       " 'Ġthe',\n",
       " 'Ġleading',\n",
       " 'Ġword',\n",
       " 'Ġjust',\n",
       " 'Ġas',\n",
       " 'Ġany',\n",
       " 'Ġother',\n",
       " 'Ġword',\n",
       " '.',\n",
       " 'Ġ(',\n",
       " 'G',\n",
       " 'PT',\n",
       " '2',\n",
       " 'Ġtoken',\n",
       " 'izer',\n",
       " 'Ġdetect',\n",
       " 'Ġbeginning',\n",
       " 'Ġof',\n",
       " 'Ġwords',\n",
       " 'Ġby',\n",
       " 'Ġthe',\n",
       " 'Ġpreceding',\n",
       " 'Ġspace',\n",
       " ').',\n",
       " 'add',\n",
       " '_',\n",
       " 'bos',\n",
       " '_',\n",
       " 'token',\n",
       " 'Ġ(',\n",
       " 'bool',\n",
       " ',',\n",
       " 'Ġoptional',\n",
       " ',',\n",
       " 'Ġdefaults',\n",
       " 'Ġto',\n",
       " 'ĠFalse',\n",
       " ')',\n",
       " 'ĠâĢĶ',\n",
       " 'ĠWhether',\n",
       " 'Ġor',\n",
       " 'Ġnot',\n",
       " 'Ġto',\n",
       " 'Ġadd',\n",
       " 'Ġan',\n",
       " 'Ġinitial',\n",
       " 'Ġbeginning',\n",
       " 'Ġof',\n",
       " 'Ġsentence',\n",
       " 'Ġtoken',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġinput',\n",
       " '.',\n",
       " 'ĠThis',\n",
       " 'Ġallows',\n",
       " 'Ġto',\n",
       " 'Ġtreat',\n",
       " 'Ġthe',\n",
       " 'Ġleading',\n",
       " 'Ġword',\n",
       " 'Ġjust',\n",
       " 'Ġas',\n",
       " 'Ġany',\n",
       " 'Ġother',\n",
       " 'Ġword',\n",
       " '.',\n",
       " 'Ġvoc',\n",
       " 'ab',\n",
       " 'Ġ(',\n",
       " 'dict',\n",
       " ',',\n",
       " 'Ġoptional',\n",
       " ')',\n",
       " 'ĠâĢĶ',\n",
       " 'ĠCustom',\n",
       " 'Ġvocabulary',\n",
       " 'Ġdictionary',\n",
       " '.',\n",
       " 'ĠIf',\n",
       " 'Ġnot',\n",
       " 'Ġprovided',\n",
       " ',',\n",
       " 'Ġvocabulary',\n",
       " 'Ġis',\n",
       " 'Ġloaded',\n",
       " 'Ġfrom',\n",
       " 'Ġvoc',\n",
       " 'ab',\n",
       " '_',\n",
       " 'file',\n",
       " '.',\n",
       " 'Ġmer',\n",
       " 'ges',\n",
       " 'Ġ(',\n",
       " 'list',\n",
       " ',',\n",
       " 'Ġoptional',\n",
       " ')',\n",
       " 'ĠâĢĶ',\n",
       " 'ĠCustom',\n",
       " 'Ġmer',\n",
       " 'ges',\n",
       " 'Ġlist',\n",
       " '.',\n",
       " 'ĠIf',\n",
       " 'Ġnot',\n",
       " 'Ġprovided',\n",
       " ',',\n",
       " 'Ġmer',\n",
       " 'ges',\n",
       " 'Ġare',\n",
       " 'Ġloaded',\n",
       " 'Ġfrom',\n",
       " 'Ġmer',\n",
       " 'ges',\n",
       " '_',\n",
       " 'file',\n",
       " 'Construct',\n",
       " 'Ġa',\n",
       " 'ĠG',\n",
       " 'PT',\n",
       " '-',\n",
       " '2',\n",
       " 'Ġtoken',\n",
       " 'izer',\n",
       " '.',\n",
       " 'ĠBased',\n",
       " 'Ġon',\n",
       " 'Ġbyte',\n",
       " '-',\n",
       " 'level',\n",
       " 'ĠByte',\n",
       " '-',\n",
       " 'P',\n",
       " 'air',\n",
       " '-',\n",
       " 'Enc',\n",
       " 'oding',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"errors (str, optional, defaults toParadigm to follow when decoding bytes to UTF-8. See bytes.decode for more information. unk_token (str, optional, defaults to — The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this token instead.\" \\\n",
    "\"bos_token (str, optional, defaults to  — The beginning of sequence token. eos_token (str, optional, defaults to — The end of sequence token. add_prefix_space (bool, optional, defaults to False) — Whether or not to add an initial space to the input. This allows to treat the leading word just as any other word. (GPT2 tokenizer detect beginning of words by the preceding space).\" \\\n",
    "\"add_bos_token (bool, optional, defaults to False) — Whether or not to add an initial beginning of sentence token to the input. This allows to treat the leading word just as any other word. \" \\\n",
    "\"vocab (dict, optional) — Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file. \" \\\n",
    "\"merges (list, optional) — Custom merges list. If not provided, merges are loaded from merges_file\" \\\n",
    "\"Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98b3537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15496, 995], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a64ba9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_added_tokens_decoder',\n",
       " '_added_tokens_encoder',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_batch_prepare_for_model',\n",
       " '_call_one',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_pad',\n",
       " '_pad_token_type_id',\n",
       " '_patch_mistral_regex',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_set_model_specific_special_tokens',\n",
       " '_set_processor_class',\n",
       " '_special_tokens_map',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenize',\n",
       " '_update_total_vocab_size',\n",
       " '_update_trie',\n",
       " '_upload_modified_files',\n",
       " 'add_bos_token',\n",
       " 'add_prefix_space',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bpe',\n",
       " 'bpe_ranks',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'byte_decoder',\n",
       " 'byte_encoder',\n",
       " 'cache',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_message_with_chat_template',\n",
       " 'encode_plus',\n",
       " 'encoder',\n",
       " 'errors',\n",
       " 'extra_special_tokens',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_chat_template',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'pat',\n",
       " 'prepare_for_model',\n",
       " 'prepare_for_tokenization',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_chat_templates',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'tokens_trie',\n",
       " 'total_vocab_size',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'verbose',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f08ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
