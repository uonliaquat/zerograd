{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb770f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7586e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_multiple_tensors(filename):\n",
    "    tensors_dict = {}\n",
    "    \n",
    "    # Tracking current tensor state\n",
    "    current_name = \"default_tensor\"\n",
    "    current_metadata = {}\n",
    "    current_data = []\n",
    "    \n",
    "    meta_keys = ['size', 'ndim', 'shape', 'stride', 'elem_size', 'requires_grad']\n",
    "\n",
    "    def finalize_tensor(name, meta, data):\n",
    "        \"\"\"Helper to reshape data and store in the dictionary.\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "        \n",
    "        target_shape = [int(s) for s in meta.get('shape', [len(data)])]\n",
    "        # Using float64 (double) as your images show high precision decimals\n",
    "        tensors_dict[name] = torch.tensor(data, dtype=torch.float64).reshape(target_shape)\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split by comma and remove empty strings/whitespace\n",
    "            parts = [p.strip() for p in line.split(',') if p.strip()]\n",
    "            \n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            label = parts[0].replace(':', '')\n",
    "\n",
    "            # 1. Check if it's a known metadata key\n",
    "            if label in meta_keys:\n",
    "                vals = [float(v) for v in parts[1:]]\n",
    "                current_metadata[label] = vals[0] if len(vals) == 1 else vals\n",
    "            \n",
    "            else:\n",
    "                try:\n",
    "                    # 2. Try to parse as numeric data\n",
    "                    row_data = [float(p) for p in parts]\n",
    "                    current_data.extend(row_data)\n",
    "                except ValueError:\n",
    "                    # 3. If it's a string but NOT metadata, it's a new tensor name\n",
    "                    # Save the previous tensor first\n",
    "                    if current_data:\n",
    "                        finalize_tensor(current_name, current_metadata, current_data)\n",
    "                    \n",
    "                    # Reset for the new tensor\n",
    "                    current_name = label\n",
    "                    current_metadata = {}\n",
    "                    current_data = []\n",
    "\n",
    "    # Finalize the last tensor in the file\n",
    "    finalize_tensor(current_name, current_metadata, current_data)\n",
    "    \n",
    "    return tensors_dict\n",
    "\n",
    "# Usage\n",
    "self_attention_layer_c = load_multiple_tensors('../output/self_attention_layer.csv')\n",
    "input_embeddings_c= load_multiple_tensors('../output/input_embeddings.csv')\n",
    "input_embeddings_c = input_embeddings_c['default_tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5beeeb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W_Query': tensor([[ 0.69,  0.05, -0.82,  0.31],\n",
       "         [-0.17,  0.40,  0.82,  0.52],\n",
       "         [-0.48, -0.91,  0.47, -0.34],\n",
       "         [ 0.27,  0.51,  0.98, -0.27]], dtype=torch.float64),\n",
       " 'W_Key': tensor([[-0.51,  0.97,  0.45,  0.51],\n",
       "         [ 0.30, -0.85,  0.26,  0.77],\n",
       "         [-0.45, -0.13,  0.53, -0.04],\n",
       "         [-0.52, -0.45, -0.28, -0.67]], dtype=torch.float64),\n",
       " 'W_Value': tensor([[-0.03,  0.80,  0.82, -0.88],\n",
       "         [ 0.81,  0.01,  0.03, -0.36],\n",
       "         [ 0.97, -0.01, -0.47, -0.82],\n",
       "         [ 0.90, -0.85,  0.00, -0.23]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_layer_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a38027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SelfAttention:\n",
    "    def __init__(self, self_attention_layer_c):\n",
    "        W_query_weights =   self_attention_layer_c['W_Query']\n",
    "        W_key_weights =     self_attention_layer_c['W_Key']\n",
    "        W_value_weights =   self_attention_layer_c['W_Value']\n",
    "\n",
    "        W_query_weights = W_query_weights.t()\n",
    "        W_key_weights = W_key_weights.t()\n",
    "        W_value_weights = W_value_weights.t()\n",
    "\n",
    "        self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "        self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "        self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "        self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "        self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "        self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "        print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "        print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "        print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "        print(\"===========================================\")\n",
    "\n",
    "        query = self.W_query(x)\n",
    "        key = self.W_key(x)\n",
    "        value = self.W_value(x)\n",
    "        \n",
    "\n",
    "        # print(f\"Query\\nShape: {query.shape}\\n{query}\")\n",
    "        # print(f\"Key\\nShape:   {key.shape}\\n{key}\")\n",
    "        # print(f\"Value\\nShape: {value.shape}\\n{value}\")\n",
    "        # print(\"===========================================\")\n",
    "\n",
    "        key_transposed = key.t()\n",
    "        print(f\"Key transposed\\nShape: {key_transposed.shape}\\n{key_transposed}\")\n",
    "\n",
    "        attention_scores = query @ key_transposed\n",
    "        print(f\"Attention Scores\\nShape: {attention_scores.shape}\\n{attention_scores}\")\n",
    "\n",
    "        attention_scores_scaled = attention_scores * 1/math.sqrt(key.shape[1])\n",
    "        print(f\"Attention Scores Scaled\\nShape: {attention_scores_scaled.shape}\\n{attention_scores_scaled}\")\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores_scaled, dim=1)\n",
    "        print(f\"Attention Weights\\nShape: {attention_weights.shape}\\n{attention_weights}\")\n",
    "\n",
    "        context_vecs = attention_weights @ value\n",
    "        print(f\"Context Vecs\\nShape: {context_vecs.shape}\\n{context_vecs}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56a1f051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings: \n",
      "Shape: torch.Size([6, 4])\n",
      "tensor([[-1.00, -0.74,  0.51, -0.08],\n",
      "        [ 0.07, -0.56, -0.91,  0.36],\n",
      "        [ 0.36,  0.87, -0.23,  0.04],\n",
      "        [ 0.66, -0.93, -0.89,  0.06],\n",
      "        [ 0.34, -0.98, -0.23, -0.87],\n",
      "        [-0.17,  0.37,  0.18,  0.86]], dtype=torch.float64)\n",
      "W_Query\n",
      "Shape: torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.69, -0.17, -0.48,  0.27],\n",
      "        [ 0.05,  0.40, -0.91,  0.51],\n",
      "        [-0.82,  0.82,  0.47,  0.98],\n",
      "        [ 0.31,  0.52, -0.34, -0.27]], dtype=torch.float64, requires_grad=True)\n",
      "W_Key\n",
      "Shape:   torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.51,  0.30, -0.45, -0.52],\n",
      "        [ 0.97, -0.85, -0.13, -0.45],\n",
      "        [ 0.45,  0.26,  0.53, -0.28],\n",
      "        [ 0.51,  0.77, -0.04, -0.67]], dtype=torch.float64, requires_grad=True)\n",
      "W_Value\n",
      "Shape: torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.03,  0.81,  0.97,  0.90],\n",
      "        [ 0.80,  0.01, -0.01, -0.85],\n",
      "        [ 0.82,  0.03, -0.47,  0.00],\n",
      "        [-0.88, -0.36, -0.82, -0.23]], dtype=torch.float64, requires_grad=True)\n",
      "===========================================\n",
      "Key transposed\n",
      "Shape: torch.Size([4, 6])\n",
      "tensor([[ 0.10,  0.02,  0.16, -0.25,  0.09, -0.33],\n",
      "        [-0.37,  0.50, -0.38,  1.52,  1.58, -0.89],\n",
      "        [-0.35, -0.70,  0.26, -0.43,  0.02, -0.13],\n",
      "        [-1.05, -0.60,  0.84, -0.38,  0.01, -0.39]], dtype=torch.float64,\n",
      "       grad_fn=<TBackward0>)\n",
      "Attention Scores\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[ 0.99, -0.19, -0.42, -0.93, -1.42,  1.31],\n",
      "        [ 0.04,  0.86, -0.39,  1.31,  1.30, -0.83],\n",
      "        [-0.98, -0.32,  0.43,  0.46,  0.98, -0.89],\n",
      "        [ 0.49,  1.43, -0.44,  1.22,  0.85, -0.59],\n",
      "        [ 1.06,  1.18, -0.32, -0.07, -0.98,  0.74],\n",
      "        [-0.48, -0.66,  0.06,  0.10,  0.68, -0.47]], dtype=torch.float64,\n",
      "       grad_fn=<MmBackward0>)\n",
      "Attention Scores Scaled\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[ 0.49, -0.10, -0.21, -0.46, -0.71,  0.66],\n",
      "        [ 0.02,  0.43, -0.19,  0.66,  0.65, -0.42],\n",
      "        [-0.49, -0.16,  0.21,  0.23,  0.49, -0.45],\n",
      "        [ 0.25,  0.71, -0.22,  0.61,  0.43, -0.29],\n",
      "        [ 0.53,  0.59, -0.16, -0.04, -0.49,  0.37],\n",
      "        [-0.24, -0.33,  0.03,  0.05,  0.34, -0.24]], dtype=torch.float64,\n",
      "       grad_fn=<DivBackward0>)\n",
      "Attention Weights\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[0.26, 0.14, 0.13, 0.10, 0.08, 0.30],\n",
      "        [0.13, 0.19, 0.10, 0.24, 0.24, 0.08],\n",
      "        [0.10, 0.14, 0.20, 0.20, 0.26, 0.10],\n",
      "        [0.16, 0.25, 0.10, 0.22, 0.19, 0.09],\n",
      "        [0.23, 0.24, 0.12, 0.13, 0.08, 0.20],\n",
      "        [0.14, 0.12, 0.18, 0.18, 0.24, 0.14]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Context Vecs\n",
      "Shape: torch.Size([6, 4])\n",
      "tensor([[-0.03, -0.33, -0.10,  0.23],\n",
      "        [-0.89,  0.17,  0.29,  0.40],\n",
      "        [-0.72,  0.22,  0.30,  0.27],\n",
      "        [-0.80,  0.06,  0.25,  0.43],\n",
      "        [-0.33, -0.23,  0.02,  0.35],\n",
      "        [-0.61,  0.13,  0.21,  0.27]], dtype=torch.float64,\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "self_attention = SelfAttention(self_attention_layer_c)\n",
    "\n",
    "self_attention.forward(input_embeddings_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0b8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
