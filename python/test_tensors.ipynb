{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb770f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3265eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(filename):\n",
    "    metadata = {}\n",
    "    data_values = []\n",
    "    \n",
    "    # List of metadata keys to look for\n",
    "    meta_keys = ['size', 'ndim', 'shape', 'stride', 'elem_size', 'requires_grad']\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # Clean up line: remove spaces, split by comma\n",
    "            parts = [p.strip() for p in line.split(',') if p.strip()]\n",
    "            \n",
    "            if not parts:\n",
    "                continue  # Skip empty lines\n",
    "            \n",
    "            # Check if this row starts with a metadata label\n",
    "            label = parts[0]\n",
    "            \n",
    "            if label in meta_keys:\n",
    "                # If there are numbers after the label, store them\n",
    "                vals = [int(v) for v in parts[1:]]\n",
    "                metadata[label] = vals[0] if len(vals) == 1 else vals\n",
    "            else:\n",
    "                # If it's not a label, check if the parts are numbers (the tensor data)\n",
    "                try:\n",
    "                    row_data = [float(p) for p in parts]\n",
    "                    data_values.extend(row_data)\n",
    "                except ValueError:\n",
    "                    # Not a number and not a metadata key (could be a header or text)\n",
    "                    continue\n",
    "\n",
    "    # Convert to PyTorch Tensor\n",
    "    if 'shape' in metadata:\n",
    "        # Convert floats to ints for reshaping (e.g., 2.0 -> 2)\n",
    "        target_shape = [int(s) for s in metadata['shape']]\n",
    "        tensor = torch.tensor(data_values).reshape(target_shape)\n",
    "        return metadata, tensor\n",
    "    \n",
    "    return metadata, torch.tensor(data_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc1942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_tensor1, tensor1 = load_tensor('../output/tensor1.csv')\n",
    "meta_tensor2, tensor2 = load_tensor('../output/tensor2.csv')\n",
    "meta_output_tensor, output_tensor_c = load_tensor('../output/output_tensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2778f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'size': 24, 'ndim': 3, 'shape': [2, 4, 3], 'stride': [12, 3, 1], 'elem_size': 8, 'requires_grad': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-10.00,  -7.37,   5.11],\n",
       "         [ -0.83,   0.66,  -5.62],\n",
       "         [ -9.06,   3.58,   3.59],\n",
       "         [  8.69,  -2.33,   0.39]],\n",
       "\n",
       "        [[  6.62,  -9.31,  -8.93],\n",
       "         [  0.59,   3.42,  -9.85],\n",
       "         [ -2.33,  -8.66,  -1.65],\n",
       "         [  3.74,   1.78,   8.61]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'size': 12, 'ndim': 2, 'shape': [3, 4], 'stride': [4, 1], 'elem_size': 8, 'requires_grad': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.92,  0.54, -8.16,  3.08],\n",
       "        [-1.68,  4.02,  8.21,  5.24],\n",
       "        [-4.75, -9.05,  4.72, -3.44]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'size': 32, 'ndim': 3, 'shape': [2, 4, 4], 'stride': [16, 4, 1], 'elem_size': 8, 'requires_grad': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -81.14,  -81.31,   45.27,  -86.99],\n",
       "         [  19.88,   53.06,  -14.41,   20.20],\n",
       "         [ -85.77,  -22.94,  120.22,  -21.45],\n",
       "         [  62.26,   -8.21,  -88.24,   13.21]],\n",
       "\n",
       "        [[ 103.90,   46.94, -172.58,    2.24],\n",
       "         [  45.14,  103.21,  -23.25,   53.60],\n",
       "         [   6.25,  -21.18,  -59.86,  -46.94],\n",
       "         [ -18.03,  -68.74,   24.77,   -8.74]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Metadata:\", meta_tensor1)\n",
    "display(tensor1)\n",
    "print(\"Metadata:\", meta_tensor2)\n",
    "display(tensor2)\n",
    "print(\"Metadata:\", meta_output_tensor)\n",
    "display(output_tensor_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d40be1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb3013e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b9ad86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_py = tensor1 @ tensor2\n",
    "output_tensor_py.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e96204d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -81.09,  -81.27,   45.21,  -87.00],\n",
       "         [  19.84,   53.07,  -14.33,   20.23],\n",
       "         [ -85.76,  -22.99,  120.27,  -21.50],\n",
       "         [  62.20,   -8.20,  -88.20,   13.21]],\n",
       "\n",
       "        [[ 103.87,   46.97, -172.60,    2.32],\n",
       "         [  45.12,  103.21,  -23.23,   53.62],\n",
       "         [   6.26,  -21.14,  -59.87,  -46.88],\n",
       "         [ -18.01,  -68.75,   24.73,   -8.77]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd483507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -81.14,  -81.31,   45.27,  -86.99],\n",
       "         [  19.88,   53.06,  -14.41,   20.20],\n",
       "         [ -85.77,  -22.94,  120.22,  -21.45],\n",
       "         [  62.26,   -8.21,  -88.24,   13.21]],\n",
       "\n",
       "        [[ 103.90,   46.94, -172.58,    2.24],\n",
       "         [  45.14,  103.21,  -23.25,   53.60],\n",
       "         [   6.25,  -21.18,  -59.86,  -46.94],\n",
       "         [ -18.03,  -68.74,   24.77,   -8.74]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6ee0d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.05,  0.04, -0.06, -0.01],\n",
       "         [-0.04,  0.01,  0.08,  0.03],\n",
       "         [ 0.01, -0.05,  0.05, -0.05],\n",
       "         [-0.06,  0.01,  0.04,  0.00]],\n",
       "\n",
       "        [[-0.03,  0.03, -0.02,  0.08],\n",
       "         [-0.02, -0.00,  0.02,  0.02],\n",
       "         [ 0.01,  0.04, -0.01,  0.06],\n",
       "         [ 0.02, -0.01, -0.04, -0.03]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_py - output_tensor_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "414e8279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_py.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bba1ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18275f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
