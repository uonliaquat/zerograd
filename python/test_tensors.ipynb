{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb770f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3265eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(filename):\n",
    "    metadata = {}\n",
    "    data_values = []\n",
    "    \n",
    "    # List of metadata keys to look for\n",
    "    meta_keys = ['size', 'ndim', 'shape', 'stride', 'elem_size', 'requires_grad']\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # Clean up line: remove spaces, split by comma\n",
    "            parts = [p.strip() for p in line.split(',') if p.strip()]\n",
    "            \n",
    "            if not parts:\n",
    "                continue  # Skip empty lines\n",
    "            \n",
    "            # Check if this row starts with a metadata label\n",
    "            label = parts[0]\n",
    "            \n",
    "            if label in meta_keys:\n",
    "                # If there are numbers after the label, store them\n",
    "                vals = [int(v) for v in parts[1:]]\n",
    "                metadata[label] = vals[0] if len(vals) == 1 else vals\n",
    "            else:\n",
    "                # If it's not a label, check if the parts are numbers (the tensor data)\n",
    "                try:\n",
    "                    row_data = [float(p) for p in parts]\n",
    "                    data_values.extend(row_data)\n",
    "                except ValueError:\n",
    "                    # Not a number and not a metadata key (could be a header or text)\n",
    "                    continue\n",
    "\n",
    "    # Convert to PyTorch Tensor\n",
    "    if 'shape' in metadata:\n",
    "        # Convert floats to ints for reshaping (e.g., 2.0 -> 2)\n",
    "        target_shape = [int(s) for s in metadata['shape']]\n",
    "        tensor = torch.tensor(data_values).reshape(target_shape)\n",
    "        return metadata, tensor\n",
    "    \n",
    "    return metadata, torch.tensor(data_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dc1942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_tensor1, tensor1 = load_tensor('../output/tensor1.csv')\n",
    "meta_tensor2, tensor2 = load_tensor('../output/tensor2.csv')\n",
    "meta_output_tensor, output_tensor_c = load_tensor('../output/output_tensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f2778f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'size': 24, 'ndim': 2, 'shape': [6, 4], 'stride': [4, 1], 'elem_size': 8, 'requires_grad': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-10.00,  -7.37,   5.11,  -0.83],\n",
       "        [  0.66,  -5.62,  -9.06,   3.58],\n",
       "        [  3.59,   8.69,  -2.33,   0.39],\n",
       "        [  6.62,  -9.31,  -8.93,   0.59],\n",
       "        [  3.42,  -9.85,  -2.33,  -8.66],\n",
       "        [ -1.65,   3.74,   1.78,   8.61]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'size': 24, 'ndim': 2, 'shape': [4, 6], 'stride': [6, 1], 'elem_size': 8, 'requires_grad': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.92,  0.54, -8.16,  3.08, -1.68,  4.02],\n",
       "        [ 8.21,  5.24, -4.75, -9.05,  4.72, -3.44],\n",
       "        [ 2.65,  5.13,  9.82, -2.69, -5.06,  9.65],\n",
       "        [ 4.45,  5.07,  3.03, -8.55,  2.63,  7.69]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'size': 36, 'ndim': 2, 'shape': [6, 6], 'stride': [6, 1], 'elem_size': 8, 'requires_grad': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-119.83,  -22.00,  164.32,   29.21,  -46.04,   28.05],\n",
       "        [ -49.69,  -57.45,  -56.77,   46.72,   27.61,  -37.96],\n",
       "        [  91.72,   37.54,  -92.27,  -64.69,   47.83,  -34.94],\n",
       "        [ -51.61,  -88.04,  -95.70,  123.60,   -8.33,  -23.01],\n",
       "        [-101.87, -105.64,  -30.31,  179.97,  -63.25,  -41.56],\n",
       "        [  62.29,   71.45,   39.28, -117.25,   34.07,   63.94]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Metadata:\", meta_tensor1)\n",
    "display(tensor1)\n",
    "print(\"Metadata:\", meta_tensor2)\n",
    "display(tensor2)\n",
    "print(\"Metadata:\", meta_output_tensor)\n",
    "display(output_tensor_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d40be1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edb3013e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0b9ad86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_py = tensor1 @ tensor2\n",
    "output_tensor_py.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e96204d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-119.86,  -22.01,  164.27,   29.25,  -46.03,   28.08],\n",
       "        [ -49.65,  -57.42,  -56.81,   46.66,   27.62,  -37.91],\n",
       "        [  91.75,   37.50,  -92.27,  -64.65,   47.80,  -34.95],\n",
       "        [ -51.66,  -88.03,  -95.70,  123.62,   -8.33,  -23.00],\n",
       "        [-101.91, -105.63,  -30.24,  179.99,  -63.22,  -41.45],\n",
       "        [  62.32,   71.49,   39.27, -117.33,   34.06,   63.89]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd483507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-119.83,  -22.00,  164.32,   29.21,  -46.04,   28.05],\n",
       "        [ -49.69,  -57.45,  -56.77,   46.72,   27.61,  -37.96],\n",
       "        [  91.72,   37.54,  -92.27,  -64.69,   47.83,  -34.94],\n",
       "        [ -51.61,  -88.04,  -95.70,  123.60,   -8.33,  -23.01],\n",
       "        [-101.87, -105.64,  -30.31,  179.97,  -63.25,  -41.56],\n",
       "        [  62.29,   71.45,   39.28, -117.25,   34.07,   63.94]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6ee0d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.03, -0.01, -0.05,  0.04,  0.01,  0.03],\n",
       "        [ 0.04,  0.03, -0.04, -0.06,  0.01,  0.05],\n",
       "        [ 0.03, -0.04, -0.00,  0.04, -0.03, -0.01],\n",
       "        [-0.05,  0.01, -0.00,  0.02,  0.00,  0.01],\n",
       "        [-0.04,  0.01,  0.07,  0.02,  0.03,  0.11],\n",
       "        [ 0.03,  0.04, -0.01, -0.08, -0.01, -0.05]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_py - output_tensor_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "414e8279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_py.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bba1ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18275f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
