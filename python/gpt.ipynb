{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "eb770f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "7586e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensors(filename):\n",
    "    tensors_dict = {}\n",
    "    \n",
    "    # Tracking current tensor state\n",
    "    current_name = \"default_tensor\"\n",
    "    current_metadata = {}\n",
    "    current_data = []\n",
    "    \n",
    "    meta_keys = ['size', 'ndim', 'shape', 'stride', 'elem_size', 'requires_grad']\n",
    "\n",
    "    def finalize_tensor(name, meta, data):\n",
    "        \"\"\"Helper to reshape data and store in the dictionary.\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "        \n",
    "        target_shape = [int(s) for s in meta.get('shape', [len(data)])]\n",
    "        # Using float64 (double) as your images show high precision decimals\n",
    "        # if name == current_name:\n",
    "        #     tensors_dict = torch.tensor(data, dtype=torch.float32).reshape(target_shape)\n",
    "        # else:\n",
    "        tensors_dict[name] = torch.tensor(data, dtype=torch.float32).reshape(target_shape)\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split by comma and remove empty strings/whitespace\n",
    "            parts = [p.strip() for p in line.split(',') if p.strip()]\n",
    "            \n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            label = parts[0].replace(':', '')\n",
    "\n",
    "            # 1. Check if it's a known metadata key\n",
    "            if label in meta_keys:\n",
    "                vals = [float(v) for v in parts[1:]]\n",
    "                current_metadata[label] = vals[0] if len(vals) == 1 else vals\n",
    "            \n",
    "            else:\n",
    "                try:\n",
    "                    # 2. Try to parse as numeric data\n",
    "                    row_data = [float(p) for p in parts]\n",
    "                    current_data.extend(row_data)\n",
    "                except ValueError:\n",
    "                    # 3. If it's a string but NOT metadata, it's a new tensor name\n",
    "                    # Save the previous tensor first\n",
    "                    if current_data:\n",
    "                        finalize_tensor(current_name, current_metadata, current_data)\n",
    "                    \n",
    "                    # Reset for the new tensor\n",
    "                    current_name = label\n",
    "                    current_metadata = {}\n",
    "                    current_data = []\n",
    "\n",
    "    # Finalize the last tensor in the file\n",
    "    finalize_tensor(current_name, current_metadata, current_data)\n",
    "    if \"default_tensor\" in tensors_dict.keys():\n",
    "        tensors_dict = tensors_dict[\"default_tensor\"]\n",
    "    return tensors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d3c7a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_model(path):\n",
    "    gpt_model = {}\n",
    "    folder_path = Path(path)\n",
    "    for file in folder_path.iterdir():\n",
    "        if file.is_file():  # Ensure it's a file and not a subfolder\n",
    "            filename = Path(file.name).stem\n",
    "            # print(f\"Filename: {filename}\")\n",
    "            gpt_model[filename] = load_tensors(file)\n",
    "            # print(f\"Full Path: {file}\")\n",
    "    return gpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7fa39cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Weights': tensor([[-1.00, -0.74,  0.51, -0.08,  0.07, -0.56],\n",
       "         [-0.91,  0.36,  0.36,  0.87, -0.23,  0.04],\n",
       "         [ 0.66, -0.93, -0.89,  0.06,  0.34, -0.98],\n",
       "         [-0.23, -0.87, -0.17,  0.37,  0.18,  0.86],\n",
       "         [ 0.69,  0.05, -0.82,  0.31, -0.17,  0.40],\n",
       "         [ 0.82,  0.52, -0.48, -0.91,  0.47, -0.34]]),\n",
       " 'Output': tensor([[[-0.91,  0.36,  0.36,  0.87, -0.23,  0.04],\n",
       "          [ 0.66, -0.93, -0.89,  0.06,  0.34, -0.98],\n",
       "          [-1.00, -0.74,  0.51, -0.08,  0.07, -0.56],\n",
       "          [ 0.66, -0.93, -0.89,  0.06,  0.34, -0.98],\n",
       "          [ 0.82,  0.52, -0.48, -0.91,  0.47, -0.34],\n",
       "          [-0.23, -0.87, -0.17,  0.37,  0.18,  0.86]],\n",
       " \n",
       "         [[-1.00, -0.74,  0.51, -0.08,  0.07, -0.56],\n",
       "          [-1.00, -0.74,  0.51, -0.08,  0.07, -0.56],\n",
       "          [ 0.69,  0.05, -0.82,  0.31, -0.17,  0.40],\n",
       "          [-0.23, -0.87, -0.17,  0.37,  0.18,  0.86],\n",
       "          [ 0.66, -0.93, -0.89,  0.06,  0.34, -0.98],\n",
       "          [-0.91,  0.36,  0.36,  0.87, -0.23,  0.04]]])}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model_c = load_gpt_model(\"/Users/uonliaquat/workspace/zerograd/models\")\n",
    "gpt_model_c[\"gpt_model.token_embed_layer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "3f8fdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = '../models'\n",
    "# gpt_model_c = {\n",
    "#     \"input_tokens\":  load_tensors(f'{base_path}/input_tokens.csv')['default_tensor'].long(),\n",
    "#     \"token_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.token_embed_layer.csv'),\n",
    "#     \"pos_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.pos_embed_layer.csv'),\n",
    "#     \"position_indicies\":  load_tensors(f'{base_path}/gpt_model.workspace.position_indicies.csv')['default_tensor'].long(),\n",
    "#     \"input_embeddings\": load_tensors(f'{base_path}/gpt_model.workspace.input_embeddings.csv')['default_tensor']\n",
    "# }\n",
    "# gpt_model_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b98d9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_within_tolerance(a, b, atol):\n",
    "    if a.shape != b.shape:\n",
    "        print(\"Shape mismatch:\", a.shape, b.shape)\n",
    "        return False\n",
    "\n",
    "    max_diff = round((a - b).abs().max().item(), 2)\n",
    "    print(\"max |diff|:\", max_diff)\n",
    "\n",
    "    return max_diff <= atol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12b649",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in gpt_model_c.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "cc403fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key(target_dict, substring):\n",
    "    \"\"\"\n",
    "    Returns the first key that contains the substring.\n",
    "    Returns None if no match is found.\n",
    "    \"\"\"\n",
    "    return next((k for k in target_dict if substring in k), None)\n",
    "\n",
    "# Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b4953a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, self_attention_layer_c, num_heads):\n",
    "        super().__init__()\n",
    "        W_query_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'w_query')]\n",
    "        W_key_weights =     self_attention_layer_c[find_key(self_attention_layer_c, 'w_key')]\n",
    "        W_value_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'w_value')]\n",
    "        head_proj_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'heads_proj')]\n",
    "\n",
    "        print(\"W_query_weights\\n \", W_query_weights)\n",
    "        # W_query_weights = W_query_weights.t()\n",
    "        # W_key_weights = W_key_weights.t()\n",
    "        # W_value_weights = W_value_weights.t()\n",
    "        # head_proj_weights = head_proj_weights.t()\n",
    "\n",
    "        # self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "        # self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "        # self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "        # self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "        # self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "        # self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "\n",
    "        # print(head_proj_weights.shape[0], head_proj_weights.shape[1])\n",
    "        # self.heads_proj = nn.Linear(head_proj_weights.shape[0], head_proj_weights.shape[1], bias=False)\n",
    "        # self.heads_proj.weight = nn.Parameter(head_proj_weights)\n",
    "\n",
    "        # self.num_heads = num_heads\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "        print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "        print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "        print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "        print(f\"heads_proj\\nShape: {self.heads_proj.weight.shape}\\n{self.heads_proj.weight}\")\n",
    "        print(\"===========================================\")\n",
    "\n",
    "        query = self.W_query(x)\n",
    "        key = self.W_key(x)\n",
    "        value = self.W_value(x)\n",
    "        \n",
    "\n",
    "        print(f\"queries\\nShape: {query.shape}\\n{query}\")\n",
    "        print(f\"keys\\nShape:   {key.shape}\\n{key}\")\n",
    "        print(f\"values\\nShape: {value.shape}\\n{value}\")\n",
    "        # print(\"===========================================\")\n",
    "\n",
    "        queries_chnuks = torch.chunk(query, self.num_heads, 1)\n",
    "        keys_chnuks = torch.chunk(key , self.num_heads, 1)\n",
    "        values_chnuks = torch.chunk(value, self.num_heads, 1)\n",
    "\n",
    "        # print(f\"queries_chnuks\\nShape: {queries_chnuks[0].shape}\\n{queries_chnuks[0]}\")\n",
    "        # print(f\"keys_chnuks\\nShape: {keys_chnuks[0].shape}\\n{keys_chnuks[0]}\")\n",
    "        # print(f\"values_chnuks\\nShape: {values_chnuks[0].shape}\\n{values_chnuks[0]}\")\n",
    "        # print(\"===========================================\")\n",
    "\n",
    "\n",
    "        context_vecs = []\n",
    "        for head in range(0, self.num_heads):\n",
    "            print(f\"=================================== HEAD {head} ===================================\\n\")\n",
    "            key_transposed = keys_chnuks[head].t()\n",
    "            print(f\"Key transposed\\nShape: {key_transposed.shape}\\n{key_transposed}\")\n",
    "\n",
    "            attention_scores = queries_chnuks[head] @ key_transposed\n",
    "            print(f\"Attention Scores\\nShape: {attention_scores.shape}\\n{attention_scores}\")\n",
    "\n",
    "            attention_scores_scaled = attention_scores * 1/math.sqrt(keys_chnuks[head].shape[1])\n",
    "            print(f\"Attention Scores Scaled\\nShape: {attention_scores_scaled.shape}\\n{attention_scores_scaled}\")\n",
    "\n",
    "            attention_weights = F.softmax(attention_scores_scaled, dim=1)\n",
    "            print(f\"Attention Weights\\nShape: {attention_weights.shape}\\n{attention_weights}\")\n",
    "            context_vec = attention_weights @ values_chnuks[head]\n",
    "            print(f\"Context Vec\\nShape: {context_vec.shape}\\n{context_vec}\")\n",
    "            context_vecs.append(context_vec)\n",
    "            print(f\"=================================================================================\\n\")\n",
    "\n",
    "        concat_heads = torch.cat(context_vecs, dim=1)\n",
    "        print(f\"concat_heads\\nShape: {concat_heads.shape}\\n{concat_heads}\")\n",
    "        print(self.heads_proj.weight)\n",
    "        projected_context_vecs = self.heads_proj(concat_heads)\n",
    "        print(f\"projected_context_vecs\\nShape: {projected_context_vecs.shape}\\n{projected_context_vecs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "b41c3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, feed_forward_network_c):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(2, 5)\n",
    "        self.output = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "ff63f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, transformer_layer_c, n_heads):\n",
    "        super().__init__() \n",
    "        self_attention_layer_c = {k: v for k, v in transformer_layer_c.items() if \"self_attention_layer\" in k}\n",
    "        feed_forward_network_c = {k: v for k, v in transformer_layer_c.items() if \"feed_forward_network\" in k}\n",
    "        self.self_attention_multi_head  = SelfAttentionMultiHead(self_attention_layer_c, n_heads)\n",
    "        self.feed_forward_network       = FeedForwardNetwork(feed_forward_network_c)\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "1b426a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, transformer_block_c, n_layers, n_heads):\n",
    "        super().__init__()\n",
    "        self.transformer_layers = {}\n",
    "        for layer_no in range(0, n_layers):\n",
    "            transformer_layer_c = {k: v for k, v in transformer_block_c.items() if k.startswith(f\"transformer_layer_{layer_no}\")}\n",
    "            self.transformer_layers[layer_no] = Transformer(transformer_layer_c, n_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "d47f7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, gpt_model_c, n_layers, n_heads):\n",
    "        super().__init__()      \n",
    "        num_token_embeddings            = gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape[0]\n",
    "        token_embedding_dim             = gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape[1]\n",
    "        num_pos_embeddings              = gpt_model_c['gpt_model.pos_embed_layer']['Weights'].shape[0]\n",
    "        pos_embedding_dim               = gpt_model_c['gpt_model.pos_embed_layer']['Weights'].shape[1]\n",
    "\n",
    "        self.token_embeddings_layer     = nn.Embedding(num_embeddings=num_token_embeddings, embedding_dim=token_embedding_dim)\n",
    "        self.pos_embeddings_layer       = nn.Embedding(num_embeddings=num_pos_embeddings, embedding_dim=pos_embedding_dim)\n",
    "\n",
    "        self.token_embeddings_layer.weight.data.copy_(gpt_model_c['gpt_model.token_embed_layer']['Weights'])\n",
    "        self.pos_embeddings_layer.weight.data.copy_(gpt_model_c['gpt_model.pos_embed_layer']['Weights'])\n",
    "        \n",
    "        assert(gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape == self.token_embeddings_layer.weight.shape)\n",
    "        assert(gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape == self.pos_embeddings_layer.weight.shape)\n",
    "\n",
    "        \n",
    "        self.token_embeddings_c         = gpt_model_c['gpt_model.token_embed_layer']['Output']\n",
    "        self.pos_embeddings_c           = gpt_model_c['gpt_model.pos_embed_layer']['Output']\n",
    "        self.input_embeddings_c         = gpt_model_c['gpt_model.workspace.input_embeddings']\n",
    "        self.position_indicies_c        = gpt_model_c['gpt_model.workspace.position_indicies'].long()\n",
    "\n",
    "        transformer_block_c = {k: v for k, v in gpt_model_c.items() if \"transformer_layer\" in k}\n",
    "        self.transformer_block = TransformerBlock(transformer_block_c, n_layers, n_heads)\n",
    "        self.atol = 0.02\n",
    "\n",
    "    def forward(self, input_tokens_c):\n",
    "        print(f\"x.shape:    {input_tokens_c.shape}\")\n",
    "        token_embeddings    = self.token_embeddings_layer(input_tokens_c)[0]\n",
    "        pos_embeddings      = self.pos_embeddings_layer(self.position_indicies_c)[0]\n",
    "        input_embeddings    = token_embeddings + pos_embeddings\n",
    "\n",
    "        print(f\"token_embeddings.shape: {token_embeddings.shape}\")\n",
    "        print(f\"pos_embeddings.shape:   {pos_embeddings.shape}\")\n",
    "        print(f\"input_embeddings.shape: {input_embeddings.shape}\")\n",
    "\n",
    "        token_embeddings_matched    = tensors_within_tolerance(token_embeddings,    self.token_embeddings_c,    self.atol)\n",
    "        pos_embeddings_matched      = tensors_within_tolerance(pos_embeddings,      self.pos_embeddings_c,      self.atol)\n",
    "        input_embeddings_matched    = tensors_within_tolerance(input_embeddings,    self.input_embeddings_c,    self.atol)\n",
    "\n",
    "        print(f\"token_embeddings_matched:   {token_embeddings_matched}\")\n",
    "        print(f\"pos_embeddings_matched:     {pos_embeddings_matched}\")\n",
    "        print(f\"input_embeddings_matched:   {input_embeddings_matched}\")\n",
    "\n",
    "        assert(\n",
    "            token_embeddings_matched and \n",
    "            pos_embeddings_matched and \n",
    "            input_embeddings_matched\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        # print(token_embeddings - self.token_embed_layer_output_c)\n",
    "        return input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "2bd660d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query_weights\n",
      "  {}\n",
      "x.shape:    torch.Size([1, 2, 6])\n",
      "token_embeddings.shape: torch.Size([2, 6, 6])\n",
      "pos_embeddings.shape:   torch.Size([2, 6, 6])\n",
      "input_embeddings.shape: torch.Size([2, 6, 6])\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.01\n",
      "token_embeddings_matched:   True\n",
      "pos_embeddings_matched:     True\n",
      "input_embeddings_matched:   True\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT(gpt_model_c=gpt_model_c, n_layers=1, n_heads=1)\n",
    "input_embeddings = gpt(gpt_model_c['input_tokens'].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "65ef5fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_model.pos_embed_layer\n",
      "transformer_layer_0__feed_forward_network_ouput\n",
      "transformer_layer_0__feed_forward_network_input\n",
      "input_tokens\n",
      "transformer_layer_0__self_attention_layer_w_key\n",
      "transformer_layer_0__self_attention_layer_w_query\n",
      "gpt_model.token_embed_layer\n",
      "transformer_layer_0__self_attention_layer_w_value\n",
      "gpt_model.workspace.input_embeddings\n",
      "gpt_model.workspace.position_indicies\n",
      "transformer_layer_0__self_attention_layer_heads_proj\n"
     ]
    }
   ],
   "source": [
    "for key in gpt_model_c.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "3efab6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model_c['transformer_layer_0__self_attention_layer_w_query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8a38027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SelfAttention:\n",
    "#     def __init__(self, self_attention_layer_c):\n",
    "#         W_query_weights =   self_attention_layer_c['W_Query']\n",
    "#         W_key_weights =     self_attention_layer_c['W_Key']\n",
    "#         W_value_weights =   self_attention_layer_c['W_Value']\n",
    "\n",
    "#         W_query_weights = W_query_weights.t()\n",
    "#         W_key_weights = W_key_weights.t()\n",
    "#         W_value_weights = W_value_weights.t()\n",
    "\n",
    "#         self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "#         self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "#         self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "#         self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "#         self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "#         self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "#         print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "#         print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "#         print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "#         print(\"===========================================\")\n",
    "\n",
    "#         query = self.W_query(x)\n",
    "#         key = self.W_key(x)\n",
    "#         value = self.W_value(x)\n",
    "        \n",
    "\n",
    "#         # print(f\"Query\\nShape: {query.shape}\\n{query}\")\n",
    "#         # print(f\"Key\\nShape:   {key.shape}\\n{key}\")\n",
    "#         # print(f\"Value\\nShape: {value.shape}\\n{value}\")\n",
    "#         # print(\"===========================================\")\n",
    "\n",
    "#         key_transposed = key.t()\n",
    "#         print(f\"Key transposed\\nShape: {key_transposed.shape}\\n{key_transposed}\")\n",
    "\n",
    "#         attention_scores = query @ key_transposed\n",
    "#         print(f\"Attention Scores\\nShape: {attention_scores.shape}\\n{attention_scores}\")\n",
    "\n",
    "#         attention_scores_scaled = attention_scores * 1/math.sqrt(key.shape[1])\n",
    "#         print(f\"Attention Scores Scaled\\nShape: {attention_scores_scaled.shape}\\n{attention_scores_scaled}\")\n",
    "\n",
    "#         attention_weights = F.softmax(attention_scores_scaled, dim=1)\n",
    "#         print(f\"Attention Weights\\nShape: {attention_weights.shape}\\n{attention_weights}\")\n",
    "\n",
    "#         context_vecs = attention_weights @ value\n",
    "#         print(f\"Context Vecs\\nShape: {context_vecs.shape}\\n{context_vecs}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "56a1f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_attention = SelfAttention(self_attention_layer_c)\n",
    "\n",
    "# self_attention.forward(input_embeddings_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eac5d4",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e3e0b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SelfAttentionMultiHead(nn.Module):\n",
    "#     def __init__(self, self_attention_layer_c, num_heads):\n",
    "#         super().__init__()\n",
    "#         W_query_weights =   self_attention_layer_c['W_Query']\n",
    "#         W_key_weights =     self_attention_layer_c['W_Key']\n",
    "#         W_value_weights =   self_attention_layer_c['W_Value']\n",
    "#         head_proj_weights =   self_attention_layer_c['heads_proj']\n",
    "\n",
    "#         W_query_weights = W_query_weights.t()\n",
    "#         W_key_weights = W_key_weights.t()\n",
    "#         W_value_weights = W_value_weights.t()\n",
    "#         head_proj_weights = head_proj_weights.t()\n",
    "\n",
    "#         self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "#         self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "#         self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "#         self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "#         self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "#         self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "\n",
    "#         print(head_proj_weights.shape[0], head_proj_weights.shape[1])\n",
    "#         self.heads_proj = nn.Linear(head_proj_weights.shape[0], head_proj_weights.shape[1], bias=False)\n",
    "#         self.heads_proj.weight = nn.Parameter(head_proj_weights)\n",
    "\n",
    "#         self.num_heads = num_heads\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "#         print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "#         print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "#         print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "#         print(f\"heads_proj\\nShape: {self.heads_proj.weight.shape}\\n{self.heads_proj.weight}\")\n",
    "#         print(\"===========================================\")\n",
    "\n",
    "#         query = self.W_query(x)\n",
    "#         key = self.W_key(x)\n",
    "#         value = self.W_value(x)\n",
    "        \n",
    "\n",
    "#         print(f\"queries\\nShape: {query.shape}\\n{query}\")\n",
    "#         print(f\"keys\\nShape:   {key.shape}\\n{key}\")\n",
    "#         print(f\"values\\nShape: {value.shape}\\n{value}\")\n",
    "#         # print(\"===========================================\")\n",
    "\n",
    "#         queries_chnuks = torch.chunk(query, self.num_heads, 1)\n",
    "#         keys_chnuks = torch.chunk(key , self.num_heads, 1)\n",
    "#         values_chnuks = torch.chunk(value, self.num_heads, 1)\n",
    "\n",
    "#         # print(f\"queries_chnuks\\nShape: {queries_chnuks[0].shape}\\n{queries_chnuks[0]}\")\n",
    "#         # print(f\"keys_chnuks\\nShape: {keys_chnuks[0].shape}\\n{keys_chnuks[0]}\")\n",
    "#         # print(f\"values_chnuks\\nShape: {values_chnuks[0].shape}\\n{values_chnuks[0]}\")\n",
    "#         # print(\"===========================================\")\n",
    "\n",
    "\n",
    "#         context_vecs = []\n",
    "#         for head in range(0, self.num_heads):\n",
    "#             print(f\"=================================== HEAD {head} ===================================\\n\")\n",
    "#             key_transposed = keys_chnuks[head].t()\n",
    "#             print(f\"Key transposed\\nShape: {key_transposed.shape}\\n{key_transposed}\")\n",
    "\n",
    "#             attention_scores = queries_chnuks[head] @ key_transposed\n",
    "#             print(f\"Attention Scores\\nShape: {attention_scores.shape}\\n{attention_scores}\")\n",
    "\n",
    "#             attention_scores_scaled = attention_scores * 1/math.sqrt(keys_chnuks[head].shape[1])\n",
    "#             print(f\"Attention Scores Scaled\\nShape: {attention_scores_scaled.shape}\\n{attention_scores_scaled}\")\n",
    "\n",
    "#             attention_weights = F.softmax(attention_scores_scaled, dim=1)\n",
    "#             print(f\"Attention Weights\\nShape: {attention_weights.shape}\\n{attention_weights}\")\n",
    "#             context_vec = attention_weights @ values_chnuks[head]\n",
    "#             print(f\"Context Vec\\nShape: {context_vec.shape}\\n{context_vec}\")\n",
    "#             context_vecs.append(context_vec)\n",
    "#             print(f\"=================================================================================\\n\")\n",
    "\n",
    "#         concat_heads = torch.cat(context_vecs, dim=1)\n",
    "#         print(f\"concat_heads\\nShape: {concat_heads.shape}\\n{concat_heads}\")\n",
    "#         print(self.heads_proj.weight)\n",
    "#         projected_context_vecs = self.heads_proj(concat_heads)\n",
    "#         print(f\"projected_context_vecs\\nShape: {projected_context_vecs.shape}\\n{projected_context_vecs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ae2e1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_attention = SelfAttentionMultiHead(self_attention_layer_c, 2)\n",
    "\n",
    "# self_attention.forward(input_embeddings_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefd18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
