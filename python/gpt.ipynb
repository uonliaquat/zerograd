{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "eb770f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7586e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensors(filename):\n",
    "    tensors_dict = {}\n",
    "    \n",
    "    # Tracking current tensor state\n",
    "    current_name = \"default_tensor\"\n",
    "    current_metadata = {}\n",
    "    current_data = []\n",
    "    \n",
    "    meta_keys = ['size', 'ndim', 'shape', 'stride', 'elem_size', 'requires_grad']\n",
    "\n",
    "    def finalize_tensor(name, meta, data):\n",
    "        \"\"\"Helper to reshape data and store in the dictionary.\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "        \n",
    "        target_shape = [int(s) for s in meta.get('shape', [len(data)])]\n",
    "        # Using float64 (double) as your images show high precision decimals\n",
    "        # if name == current_name:\n",
    "        #     tensors_dict = torch.tensor(data, dtype=torch.float32).reshape(target_shape)\n",
    "        # else:\n",
    "        tensors_dict[name] = torch.tensor(data, dtype=torch.float64).reshape(target_shape)\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split by comma and remove empty strings/whitespace\n",
    "            parts = [p.strip() for p in line.split(',') if p.strip()]\n",
    "            \n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            label = parts[0].replace(':', '')\n",
    "\n",
    "            # 1. Check if it's a known metadata key\n",
    "            if label in meta_keys:\n",
    "                vals = [float(v) for v in parts[1:]]\n",
    "                current_metadata[label] = vals[0] if len(vals) == 1 else vals\n",
    "            \n",
    "            else:\n",
    "                try:\n",
    "                    # 2. Try to parse as numeric data\n",
    "                    row_data = [float(p) for p in parts]\n",
    "                    current_data.extend(row_data)\n",
    "                except ValueError:\n",
    "                    # 3. If it's a string but NOT metadata, it's a new tensor name\n",
    "                    # Save the previous tensor first\n",
    "                    if current_data:\n",
    "                        finalize_tensor(current_name, current_metadata, current_data)\n",
    "                    \n",
    "                    # Reset for the new tensor\n",
    "                    current_name = label\n",
    "                    current_metadata = {}\n",
    "                    current_data = []\n",
    "\n",
    "    # Finalize the last tensor in the file\n",
    "    finalize_tensor(current_name, current_metadata, current_data)\n",
    "    if \"default_tensor\" in tensors_dict.keys():\n",
    "        tensors_dict = tensors_dict[\"default_tensor\"]\n",
    "    return tensors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3c7a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_model(path):\n",
    "    gpt_model = {}\n",
    "    folder_path = Path(path)\n",
    "    for file in folder_path.iterdir():\n",
    "        if file.is_file():  # Ensure it's a file and not a subfolder\n",
    "            filename = Path(file.name).stem\n",
    "            # print(f\"Filename: {filename}\")\n",
    "            gpt_model[filename] = load_tensors(file)\n",
    "            # print(f\"Full Path: {file}\")\n",
    "    return gpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fa39cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Weights': tensor([[ 0.85,  0.90, -0.04,  ..., -0.58, -0.18,  0.63],\n",
       "         [ 0.28, -0.13, -0.90,  ..., -0.52,  0.49,  0.62],\n",
       "         [ 0.62,  0.66, -0.76,  ...,  0.83,  0.33, -0.73],\n",
       "         ...,\n",
       "         [ 0.94,  0.39,  0.47,  ...,  0.83,  0.61, -0.22],\n",
       "         [-0.61,  0.08, -0.45,  ...,  0.30, -0.30, -0.89],\n",
       "         [-0.06,  0.91, -0.10,  ..., -0.90,  0.63, -0.89]]),\n",
       " 'Output': tensor([[[    -2.76,     -0.48,      5.78,      0.43,      5.98,      6.62,\n",
       "               -6.77,      1.45,      6.02,     -8.07,      5.96,     14.18,\n",
       "               -1.58,      3.50,      8.39,      1.56,      2.96,    -13.13,\n",
       "               -3.33,      4.71,      5.05,      7.32,      7.52,      9.71,\n",
       "                4.27,    -10.34,      2.62,      6.44,    -10.13,     -6.80,\n",
       "               -1.95,    -16.84],\n",
       "          [    -0.42,     -5.68,     -5.70,     10.29,     12.53,      4.34,\n",
       "                4.55,      1.88,      2.43,     -8.14,     -1.08,      5.87,\n",
       "               10.10,     -5.06,     13.84,     -8.20,      9.89,     -2.25,\n",
       "                2.49,     15.63,      1.92,     -4.66,     -4.83,      1.01,\n",
       "               -8.26,    -26.62,      0.27,     -5.13,    -15.58,     20.13,\n",
       "               -2.94,    -10.78],\n",
       "          [     5.83,      0.99,      5.05,    -12.93,     -2.88,     -2.98,\n",
       "                8.35,    -17.04,     14.30,     -3.02,      6.21,     -3.88,\n",
       "               -4.28,      1.96,      3.64,     -4.51,     15.86,    -17.62,\n",
       "               -8.18,     -3.88,     -1.62,     -0.01,      3.31,      3.31,\n",
       "               -3.51,     -6.35,      4.97,     -9.78,    -11.61,     11.94,\n",
       "                8.57,      0.89],\n",
       "          [    -0.41,     -5.68,     -5.71,     10.28,     12.54,      4.34,\n",
       "                4.55,      1.89,      2.43,     -8.15,     -1.09,      5.88,\n",
       "               10.11,     -5.05,     13.85,     -8.20,      9.90,     -2.26,\n",
       "                2.49,     15.63,      1.92,     -4.67,     -4.82,      1.01,\n",
       "               -8.26,    -26.61,      0.27,     -5.14,    -15.58,     20.14,\n",
       "               -2.94,    -10.80],\n",
       "          [     0.35,     -5.77,      2.68,     11.69,      4.98,      5.03,\n",
       "                4.83,    -14.95,      2.84,    -11.67,     -5.56,      3.60,\n",
       "               -1.14,    -16.67,     11.59,    -11.69,      4.67,      5.24,\n",
       "               -3.30,     16.64,     11.00,     -2.87,     -0.18,     -1.56,\n",
       "               -4.93,    -28.03,     -4.91,     -6.87,     -3.94,      1.93,\n",
       "               -9.40,     -1.32],\n",
       "          [    -7.27,     13.47,     -5.78,      1.36,      7.17,     18.35,\n",
       "               -1.35,      0.50,      6.18,     -5.75,     -0.48,      2.46,\n",
       "                6.36,      2.86,      1.33,      7.77,     15.07,     -8.86,\n",
       "              -15.53,     -1.63,     13.30,     -6.99,      0.17,      4.41,\n",
       "                3.27,     -5.59,     16.74,      0.69,    -11.52,      6.17,\n",
       "               -2.09,    -12.23]],\n",
       " \n",
       "         [[     4.09,     -1.32,     -8.94,      9.43,      3.54,     -0.70,\n",
       "              -10.01,      6.25,    -16.73,     -9.70,    -11.20,     12.82,\n",
       "               -0.65,     -7.20,     17.35,    -11.93,      8.54,      7.34,\n",
       "                4.14,      9.48,      2.44,     -8.48,     -1.80,      3.47,\n",
       "               -9.80,    -23.57,     -3.82,     -1.91,     -6.12,    -11.36,\n",
       "              -10.66,     -2.72],\n",
       "          [    -2.70,     -6.07,     -7.34,      6.48,     -4.34,     -3.55,\n",
       "               -6.90,     10.96,     -6.31,     -6.55,     -8.63,      5.38,\n",
       "               -0.61,     -2.66,      1.98,      3.04,      0.82,      0.97,\n",
       "                3.89,     -6.33,     -4.52,      5.39,     -2.56,     -1.90,\n",
       "                1.13,    -10.30,     -6.70,      4.53,     -2.14,    -12.02,\n",
       "               -8.37,     -0.15],\n",
       "          [    -7.71,     -2.25,     -3.94,      0.97,     -0.73,      3.33,\n",
       "                5.29,     -2.04,     14.67,     -1.05,      4.77,      0.00,\n",
       "               -5.43,      1.34,     -4.24,     15.54,     12.26,    -16.74,\n",
       "               -0.35,     -8.26,     -3.37,     15.06,     -7.82,     -3.84,\n",
       "                1.44,     -5.63,      4.67,     -5.33,     -9.26,      7.49,\n",
       "                3.26,    -14.54],\n",
       "          [     3.49,      1.55,     -3.94,     -3.53,     -1.94,      8.62,\n",
       "               -4.48,     -5.53,     -2.52,     -1.13,     -9.07,      2.76,\n",
       "                3.20,      7.78,      6.43,     -8.93,      9.69,     -1.46,\n",
       "               -4.68,     -0.99,      0.60,    -16.26,      1.30,     -2.45,\n",
       "                2.00,      5.05,     -2.49,      0.74,      0.25,      1.22,\n",
       "               -7.19,     14.36],\n",
       "          [     1.77,    -12.76,     -4.74,     25.63,      2.61,     -1.23,\n",
       "               -9.40,      0.05,    -17.62,     -6.04,     -5.45,      8.92,\n",
       "                6.23,     -9.50,      2.95,    -19.99,      6.77,     14.73,\n",
       "               -5.23,     12.97,     -4.11,     -0.56,     -4.97,      8.44,\n",
       "              -19.38,    -29.11,    -10.07,     12.48,    -10.30,     -2.07,\n",
       "               -9.29,      6.83],\n",
       "          [     2.83,      6.32,      8.39,     -6.35,      2.54,      1.85,\n",
       "                0.32,    -11.69,     19.77,     -9.22,     13.46,      4.13,\n",
       "               -5.79,      4.77,      7.33,     -5.20,     12.59,    -14.88,\n",
       "              -13.57,      4.13,      1.32,     -1.44,      3.61,      0.93,\n",
       "                0.16,    -10.18,      7.36,     -2.01,     -9.38,     10.59,\n",
       "                5.86,     -0.94]]])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model_c = load_gpt_model(\"/Users/uonliaquat/workspace/zerograd/models\")\n",
    "gpt_model_c[\"transformer_layer_0__self_attention_layer_heads_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f8fdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = '../models'\n",
    "# gpt_model_c = {\n",
    "#     \"input_tokens\":  load_tensors(f'{base_path}/input_tokens.csv')['default_tensor'].long(),\n",
    "#     \"token_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.token_embed_layer.csv'),\n",
    "#     \"pos_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.pos_embed_layer.csv'),\n",
    "#     \"position_indicies\":  load_tensors(f'{base_path}/gpt_model.workspace.position_indicies.csv')['default_tensor'].long(),\n",
    "#     \"input_embeddings\": load_tensors(f'{base_path}/gpt_model.workspace.input_embeddings.csv')['default_tensor']\n",
    "# }\n",
    "# gpt_model_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b98d9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_within_tolerance(a, b, atol):\n",
    "    if a.shape != b.shape:\n",
    "        print(\"Shape mismatch:\", a.shape, b.shape)\n",
    "        return False\n",
    "\n",
    "    max_diff = round((a - b).abs().max().item(), 2)\n",
    "    print(\"max |diff|:\", max_diff)\n",
    "\n",
    "    return max_diff <= atol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12b649",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efcd23ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_layer_0__self_attention_layer_attention_scores_0\n",
      "gpt_model.pos_embed_layer\n",
      "transformer_layer_0__self_attention_layer_attention_scores_1\n",
      "input_tokens\n",
      "transformer_layer_0__self_attention_layer_context_vecs_0\n",
      "transformer_layer_0__self_attention_layer_w_key\n",
      "transformer_layer_0__self_attention_layer_context_vecs_1\n",
      "transformer_layer_0__self_attention_layer_w_query\n",
      "gpt_model.token_embed_layer\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_0\n",
      "gpt_model.workspace.position_indices\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_1\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_0\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_1\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_0\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_1\n",
      "transformer_layer_0__self_attention_layer_w_value\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_0\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_1\n",
      "transformer_layer_0__self_attention_layer_attention_weights_1\n",
      "gpt_model.workspace.input_embeddings\n",
      "transformer_layer_0__self_attention_layer_attention_weights_0\n",
      "transformer_layer_0__self_attention_layer_values_chunks_0\n",
      "transformer_layer_0__self_attention_layer_concat_heads\n",
      "transformer_layer_0__self_attention_layer_heads_proj\n",
      "transformer_layer_0__feed_forward_network_layer2\n",
      "transformer_layer_0__self_attention_layer_values_chunks_1\n",
      "transformer_layer_0__feed_forward_network_layer1\n"
     ]
    }
   ],
   "source": [
    "for key in gpt_model_c.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc403fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key(target_dict, substring):\n",
    "    \"\"\"\n",
    "    Returns the first key that contains the substring.\n",
    "    Returns None if no match is found.\n",
    "    \"\"\"\n",
    "    return next((k for k in target_dict if substring in k), None)\n",
    "\n",
    "# Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4953a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_transposed_global = None\n",
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, self_attention_layer_c, n_heads, atol):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.atol = atol\n",
    "        self.self_attention_layer_c = self_attention_layer_c\n",
    "        \n",
    "        W_query_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'w_query')]['Weights']\n",
    "        W_key_weights =     self_attention_layer_c[find_key(self_attention_layer_c, 'w_key')]['Weights']\n",
    "        W_value_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'w_value')]['Weights']\n",
    "        head_proj_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'heads_proj')]['Weights']\n",
    "\n",
    "        # print(\"W_query_weights\\n \", W_query_weights)\n",
    "        W_query_weights = W_query_weights.t()\n",
    "        W_key_weights = W_key_weights.t()\n",
    "        W_value_weights = W_value_weights.t()\n",
    "        head_proj_weights = head_proj_weights.t()\n",
    "\n",
    "        self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "        self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "        self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "        self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "        self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "        self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "\n",
    "        self.heads_proj = nn.Linear(head_proj_weights.shape[0], head_proj_weights.shape[1], bias=False)\n",
    "        self.heads_proj.weight = nn.Parameter(head_proj_weights)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "        # print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "        # print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "        # print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "        # print(f\"heads_proj\\nShape: {self.heads_proj.weight.shape}\\n{self.heads_proj.weight}\")\n",
    "        print(\"===========================================\")\n",
    "\n",
    "        query = self.W_query(x)\n",
    "        key = self.W_key(x)\n",
    "        value = self.W_value(x)\n",
    "        self.layer_name = 'self_attention_layer'\n",
    "        query_matched = tensors_within_tolerance(query, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_query')]['Output'], self.atol)\n",
    "        key_matched = tensors_within_tolerance(key,  self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_key')]['Output'], self.atol)\n",
    "        value_matched = tensors_within_tolerance(value,  self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_value')]['Output'], self.atol)\n",
    "\n",
    "        print(f\"query_matched:   {query_matched}\")\n",
    "        print(f\"key_matched:     {key_matched}\")\n",
    "        print(f\"value_matched:   {value_matched}\")\n",
    "        # print(\"Python\", query)\n",
    "        # print(\"C\", self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_query')]['Output'])\n",
    "\n",
    "        queries_chnuks  = torch.chunk(query, self.n_heads, -1)\n",
    "        keys_chnuks     = torch.chunk(key , self.n_heads, -1)\n",
    "        values_chnuks   = torch.chunk(value, self.n_heads, -1)\n",
    "\n",
    "        for head in range(0, self.n_heads):\n",
    "            #print(self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_queries_chunks_{head}')])\n",
    "            #print(self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_queries_chunks_{head}')]['Output'].shape)\n",
    "            query_chnuks_matched    = tensors_within_tolerance(queries_chnuks[head],  self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_queries_chunks_{head}')], self.atol)\n",
    "            key_chnuks_matched      = tensors_within_tolerance(keys_chnuks[head],     self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_keys_chunks_{head}')], self.atol)\n",
    "            value_chnuks_matched    = tensors_within_tolerance(values_chnuks[head],   self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_values_chunks_{head}')], self.atol)\n",
    "            print(f\"query_chnuks_matched:   {query_chnuks_matched}\")\n",
    "            print(f\"key_chnuks_matched:     {key_chnuks_matched}\")\n",
    "            print(f\"value_chnuks_matched:   {value_chnuks_matched}\")\n",
    "\n",
    "        context_vecs = []\n",
    "        for head in range(0, self.n_heads):\n",
    "            #print(self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_context_vecs_{head}')])\n",
    "            \n",
    "            print(f\"=================================== HEAD {head} ===================================\\n\")\n",
    "            key_transposed = keys_chnuks[head].transpose(1, 2)\n",
    "            key_transposed_matched = tensors_within_tolerance(key_transposed, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_keys_transposed_{head}')], self.atol)\n",
    "            print(f\"key_transposed_matched:   {key_transposed_matched}\")\n",
    "\n",
    "            attention_scores = queries_chnuks[head] @ key_transposed\n",
    "            attention_scores_matched = tensors_within_tolerance(attention_scores, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_attention_scores_{head}')], self.atol)\n",
    "            print(f\"attention_scores_matched:   {attention_scores_matched}\")\n",
    "\n",
    "\n",
    "            attention_scores_scaled = attention_scores * 1/math.sqrt(keys_chnuks[head].shape[1])\n",
    "            attention_scores_scaled_matched = tensors_within_tolerance(attention_scores_scaled, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_attention_scores_scaled_{head}')], self.atol)\n",
    "            print(f\"attention_scores_scaled_matched:   {attention_scores_scaled_matched}\")\n",
    "\n",
    "            attention_weights = F.softmax(attention_scores_scaled, dim=-1)\n",
    "            attention_weights_matched = tensors_within_tolerance(attention_weights, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_attention_weights_{head}')], self.atol)\n",
    "            print(f\"attention_weights_matched:   {attention_weights_matched}\")\n",
    "\n",
    "            context_vec = attention_weights @ values_chnuks[head]\n",
    "            context_vec_matched = tensors_within_tolerance(context_vec, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_context_vecs_{head}')], self.atol)\n",
    "            print(f\"context_vec_matched:   {context_vec_matched}\")\n",
    "\n",
    "            context_vecs.append(context_vec)\n",
    "        #     print(f\"=================================================================================\\n\")\n",
    "\n",
    "        concat_heads = torch.cat(context_vecs, dim=-1)\n",
    "        concat_heads_matched = tensors_within_tolerance(concat_heads, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_concat_heads')], self.atol)\n",
    "        print(f\"concat_heads_matched:   {concat_heads_matched}\")\n",
    "\n",
    "        projected_context_vecs = self.heads_proj(concat_heads)\n",
    "        projected_context_vecs_matched = tensors_within_tolerance(projected_context_vecs, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_heads_proj')]['Output'], self.atol)\n",
    "        print(f\"projected_context_vecs_matched:   {projected_context_vecs_matched}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b41c3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, feed_forward_network_c):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(2, 5)\n",
    "        self.output = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff63f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, transformer_layer_c, n_heads, atol):\n",
    "        super().__init__() \n",
    "        self.n_heads = n_heads\n",
    "        self.atol = atol\n",
    "        self_attention_layer_c = {k: v for k, v in transformer_layer_c.items() if \"self_attention_layer\" in k}\n",
    "        feed_forward_network_c = {k: v for k, v in transformer_layer_c.items() if \"feed_forward_network\" in k}\n",
    "        self.self_attention_multi_head  = SelfAttentionMultiHead(self_attention_layer_c, n_heads, atol)\n",
    "        #self.feed_forward_network       = FeedForwardNetwork(feed_forward_network_c)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention_multi_head(x)\n",
    "        #x = self.feed_forward_network(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b426a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, transformer_block_c, n_layers, n_heads, atol):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.atol = atol\n",
    "        self.transformer_layers = {}\n",
    "        for layer_no in range(0, n_layers):\n",
    "            transformer_layer_c = {k: v for k, v in transformer_block_c.items() if k.startswith(f\"transformer_layer_{layer_no}\")}\n",
    "            self.transformer_layers[layer_no] = Transformer(transformer_layer_c, n_heads, atol)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer_no in range(0, self.n_layers):\n",
    "            x = self.transformer_layers[layer_no](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d47f7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, gpt_model_c, n_layers, n_heads, atol):\n",
    "        super().__init__()   \n",
    "        self.atol = atol\n",
    "           \n",
    "        num_token_embeddings            = gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape[0]\n",
    "        token_embedding_dim             = gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape[1]\n",
    "        num_pos_embeddings              = gpt_model_c['gpt_model.pos_embed_layer']['Weights'].shape[0]\n",
    "        pos_embedding_dim               = gpt_model_c['gpt_model.pos_embed_layer']['Weights'].shape[1]\n",
    "\n",
    "        print(f\"num_token_embeddings: {num_token_embeddings}\")\n",
    "        print(f\"token_embedding_dim: {token_embedding_dim}\")\n",
    "        print(f\"num_pos_embeddings: {num_pos_embeddings}\")\n",
    "        print(f\"pos_embedding_dim: {pos_embedding_dim}\")\n",
    "\n",
    "        self.token_embeddings_layer     = nn.Embedding(num_embeddings=num_token_embeddings, embedding_dim=token_embedding_dim)\n",
    "        self.pos_embeddings_layer       = nn.Embedding(num_embeddings=num_pos_embeddings, embedding_dim=pos_embedding_dim)\n",
    "\n",
    "        self.token_embeddings_layer.weight.data.copy_(gpt_model_c['gpt_model.token_embed_layer']['Weights'])\n",
    "        self.pos_embeddings_layer.weight.data.copy_(gpt_model_c['gpt_model.pos_embed_layer']['Weights'])\n",
    "        \n",
    " \n",
    "        assert(gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape == self.token_embeddings_layer.weight.shape)\n",
    "        assert(gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape == self.pos_embeddings_layer.weight.shape)\n",
    "\n",
    "        \n",
    "        self.token_embeddings_c         = gpt_model_c['gpt_model.token_embed_layer']['Output']\n",
    "        self.pos_embeddings_c           = gpt_model_c['gpt_model.pos_embed_layer']['Output']\n",
    "        self.input_embeddings_c         = gpt_model_c['gpt_model.workspace.input_embeddings']\n",
    "        self.position_indicies_c        = gpt_model_c['gpt_model.workspace.position_indices'].long()\n",
    "\n",
    "        transformer_block_c = {k: v for k, v in gpt_model_c.items() if \"transformer_layer\" in k}\n",
    "        self.transformer_block = TransformerBlock(transformer_block_c, n_layers, n_heads, self.atol)\n",
    "\n",
    "    def forward(self, input_tokens_c):\n",
    "        #print(f\"x.shape:    {input_tokens_c.shape}\")\n",
    "        token_embeddings    = self.token_embeddings_layer(input_tokens_c)[0]\n",
    "        print(self.position_indicies_c)\n",
    "        pos_embeddings      = self.pos_embeddings_layer(self.position_indicies_c)[0]\n",
    "        input_embeddings    = token_embeddings + pos_embeddings\n",
    "\n",
    "\n",
    "        token_embeddings_matched    = tensors_within_tolerance(token_embeddings,    self.token_embeddings_c,    self.atol)\n",
    "        pos_embeddings_matched      = tensors_within_tolerance(pos_embeddings,      self.pos_embeddings_c,      self.atol)\n",
    "        input_embeddings_matched    = tensors_within_tolerance(input_embeddings,    self.input_embeddings_c,    self.atol)\n",
    "\n",
    "        print(f\"token_embeddings_matched:   {token_embeddings_matched}\")\n",
    "        print(f\"pos_embeddings_matched:     {pos_embeddings_matched}\")\n",
    "        print(f\"input_embeddings_matched:   {input_embeddings_matched}\")\n",
    "\n",
    "\n",
    "        # print(\"***********Token Embeddings***********\\n\")\n",
    "        # print(self.token_embeddings_c[0], \"\\n\")\n",
    "        # print(token_embeddings[0], \"\\n\\n\")\n",
    "\n",
    "        # print(\"***********Pose Embeddings***********\\n\")\n",
    "        # print(self.pos_embeddings_c[0], \"\\n\")\n",
    "        # print(pos_embeddings[0], \"\\n\\n\")\n",
    "\n",
    "\n",
    "        # print(\"***********Input Embeddings***********\\n\")\n",
    "        # print(self.input_embeddings_c[0], \"\\n\")\n",
    "        # print(input_embeddings[0], \"\\n\\n\")\n",
    "\n",
    "        # assert(\n",
    "        #     token_embeddings_matched and \n",
    "        #     pos_embeddings_matched and \n",
    "        #     input_embeddings_matched\n",
    "        # )\n",
    "        \n",
    "        contextual_embddings = self.transformer_block(input_embeddings)\n",
    "\n",
    "\n",
    "        # print(token_embeddings - self.token_embed_layer_output_c)\n",
    "        return contextual_embddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2bd660d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_token_embeddings: 6\n",
      "token_embedding_dim: 32\n",
      "num_pos_embeddings: 6\n",
      "pos_embedding_dim: 32\n",
      "tensor([[[0, 1, 2, 3, 4, 5],\n",
      "         [0, 1, 2, 3, 4, 5]]])\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "token_embeddings_matched:   True\n",
      "pos_embeddings_matched:     True\n",
      "input_embeddings_matched:   True\n",
      "===========================================\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "query_matched:   True\n",
      "key_matched:     True\n",
      "value_matched:   True\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "query_chnuks_matched:   True\n",
      "key_chnuks_matched:     True\n",
      "value_chnuks_matched:   True\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "query_chnuks_matched:   True\n",
      "key_chnuks_matched:     True\n",
      "value_chnuks_matched:   True\n",
      "=================================== HEAD 0 ===================================\n",
      "\n",
      "max |diff|: 0.0\n",
      "key_transposed_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_scaled_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_weights_matched:   True\n",
      "max |diff|: 0.0\n",
      "context_vec_matched:   True\n",
      "=================================== HEAD 1 ===================================\n",
      "\n",
      "max |diff|: 0.0\n",
      "key_transposed_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_scaled_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_weights_matched:   True\n",
      "max |diff|: 0.0\n",
      "context_vec_matched:   True\n",
      "max |diff|: 0.0\n",
      "concat_heads_matched:   True\n",
      "max |diff|: 0.0\n",
      "projected_context_vecs_matched:   True\n"
     ]
    }
   ],
   "source": [
    "atol = 0.00000001\n",
    "gpt = GPT(gpt_model_c=gpt_model_c, n_layers=1, n_heads=2, atol=atol)\n",
    "input_embeddings = gpt(gpt_model_c['input_tokens'].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65ef5fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_layer_0__self_attention_layer_context_vecs_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_0\n",
      "gpt_model.pos_embed_layer\n",
      "transformer_layer_0__self_attention_layer_attention_scores_1\n",
      "transformer_layer_0__self_attention_layer_context_vecs_2\n",
      "input_tokens\n",
      "transformer_layer_0__self_attention_layer_context_vecs_0\n",
      "transformer_layer_0__self_attention_layer_w_key\n",
      "transformer_layer_0__self_attention_layer_attention_scores_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_2\n",
      "transformer_layer_0__self_attention_layer_context_vecs_1\n",
      "transformer_layer_0__self_attention_layer_context_vecs_5\n",
      "transformer_layer_0__self_attention_layer_values_chunks_9\n",
      "transformer_layer_0__self_attention_layer_attention_scores_6\n",
      "transformer_layer_0__self_attention_layer_attention_scores_7\n",
      "transformer_layer_0__self_attention_layer_values_chunks_8\n",
      "transformer_layer_0__self_attention_layer_context_vecs_4\n",
      "transformer_layer_0__self_attention_layer_context_vecs_6\n",
      "transformer_layer_0__self_attention_layer_attention_scores_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_4\n",
      "transformer_layer_0__self_attention_layer_w_query\n",
      "transformer_layer_0__self_attention_layer_context_vecs_7\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_6\n",
      "gpt_model.token_embed_layer\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_4\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_7\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_4\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_6\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_9\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_5\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_7\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_6\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_8\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_4\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_7\n",
      "transformer_layer_0__self_attention_layer_attention_weights_8\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_0\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_2\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_1\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_2\n",
      "transformer_layer_0__self_attention_layer_attention_weights_9\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_0\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_3\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_1\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_0\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_2\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_1\n",
      "transformer_layer_0__self_attention_layer_attention_weights_7\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_3\n",
      "transformer_layer_0__self_attention_layer_w_value\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_2\n",
      "transformer_layer_0__self_attention_layer_attention_weights_6\n",
      "transformer_layer_0__self_attention_layer_attention_weights_4\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_0\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_1\n",
      "transformer_layer_0__self_attention_layer_attention_weights_5\n",
      "transformer_layer_0__self_attention_layer_attention_weights_1\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_9\n",
      "gpt_model.workspace.input_embeddings\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_4\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_8\n",
      "transformer_layer_0__self_attention_layer_attention_weights_0\n",
      "transformer_layer_0__self_attention_layer_attention_weights_2\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_9\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_6\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_8\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_9\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_7\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_8\n",
      "transformer_layer_0__self_attention_layer_attention_weights_3\n",
      "transformer_layer_0__self_attention_layer_values_chunks_6\n",
      "transformer_layer_0__self_attention_layer_attention_scores_9\n",
      "transformer_layer_0__self_attention_layer_attention_scores_8\n",
      "transformer_layer_0__self_attention_layer_values_chunks_7\n",
      "transformer_layer_0__self_attention_layer_context_vecs_9\n",
      "transformer_layer_0__self_attention_layer_values_chunks_5\n",
      "transformer_layer_0__self_attention_layer_values_chunks_4\n",
      "transformer_layer_0__self_attention_layer_context_vecs_8\n",
      "gpt_model.workspace.position_indicies\n",
      "transformer_layer_0__self_attention_layer_values_chunks_0\n",
      "transformer_layer_0__self_attention_layer_concat_heads\n",
      "transformer_layer_0__self_attention_layer_heads_proj\n",
      "transformer_layer_0__feed_forward_network_layer2\n",
      "transformer_layer_0__self_attention_layer_values_chunks_1\n",
      "transformer_layer_0__self_attention_layer_values_chunks_3\n",
      "transformer_layer_0__feed_forward_network_layer1\n",
      "transformer_layer_0__self_attention_layer_values_chunks_2\n"
     ]
    }
   ],
   "source": [
    "for key in gpt_model_c.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "3efab6b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[595], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkey_transposed_global\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "key_transposed_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8a38027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SelfAttention:\n",
    "#     def __init__(self, self_attention_layer_c):\n",
    "#         W_query_weights =   self_attention_layer_c['W_Query']\n",
    "#         W_key_weights =     self_attention_layer_c['W_Key']\n",
    "#         W_value_weights =   self_attention_layer_c['W_Value']\n",
    "\n",
    "#         W_query_weights = W_query_weights.t()\n",
    "#         W_key_weights = W_key_weights.t()\n",
    "#         W_value_weights = W_value_weights.t()\n",
    "\n",
    "#         self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "#         self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "#         self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "#         self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "#         self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "#         self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "#         print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "#         print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "#         print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "#         print(\"===========================================\")\n",
    "\n",
    "#         query = self.W_query(x)\n",
    "#         key = self.W_key(x)\n",
    "#         value = self.W_value(x)\n",
    "        \n",
    "\n",
    "#         # print(f\"Query\\nShape: {query.shape}\\n{query}\")\n",
    "#         # print(f\"Key\\nShape:   {key.shape}\\n{key}\")\n",
    "#         # print(f\"Value\\nShape: {value.shape}\\n{value}\")\n",
    "#         # print(\"===========================================\")\n",
    "\n",
    "#         key_transposed = key.t()\n",
    "#         print(f\"Key transposed\\nShape: {key_transposed.shape}\\n{key_transposed}\")\n",
    "\n",
    "#         attention_scores = query @ key_transposed\n",
    "#         print(f\"Attention Scores\\nShape: {attention_scores.shape}\\n{attention_scores}\")\n",
    "\n",
    "#         attention_scores_scaled = attention_scores * 1/math.sqrt(key.shape[1])\n",
    "#         print(f\"Attention Scores Scaled\\nShape: {attention_scores_scaled.shape}\\n{attention_scores_scaled}\")\n",
    "\n",
    "#         attention_weights = F.softmax(attention_scores_scaled, dim=1)\n",
    "#         print(f\"Attention Weights\\nShape: {attention_weights.shape}\\n{attention_weights}\")\n",
    "\n",
    "#         context_vecs = attention_weights @ value\n",
    "#         print(f\"Context Vecs\\nShape: {context_vecs.shape}\\n{context_vecs}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "56a1f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_attention = SelfAttention(self_attention_layer_c)\n",
    "\n",
    "# self_attention.forward(input_embeddings_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eac5d4",
   "metadata": {},
   "source": [
    "## Safe Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "e3e0b8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.0.attn.bias\n",
      "h.0.attn.c_attn.bias\n",
      "h.0.attn.c_attn.weight\n",
      "h.0.attn.c_proj.bias\n",
      "h.0.attn.c_proj.weight\n",
      "h.0.ln_1.bias\n",
      "h.0.ln_1.weight\n",
      "h.0.ln_2.bias\n",
      "h.0.ln_2.weight\n",
      "h.0.mlp.c_fc.bias\n",
      "h.0.mlp.c_fc.weight\n",
      "h.0.mlp.c_proj.bias\n",
      "h.0.mlp.c_proj.weight\n",
      "h.1.attn.bias\n",
      "h.1.attn.c_attn.bias\n",
      "h.1.attn.c_attn.weight\n",
      "h.1.attn.c_proj.bias\n",
      "h.1.attn.c_proj.weight\n",
      "h.1.ln_1.bias\n",
      "h.1.ln_1.weight\n",
      "h.1.ln_2.bias\n",
      "h.1.ln_2.weight\n",
      "h.1.mlp.c_fc.bias\n",
      "h.1.mlp.c_fc.weight\n",
      "h.1.mlp.c_proj.bias\n",
      "h.1.mlp.c_proj.weight\n",
      "h.10.attn.bias\n",
      "h.10.attn.c_attn.bias\n",
      "h.10.attn.c_attn.weight\n",
      "h.10.attn.c_proj.bias\n",
      "h.10.attn.c_proj.weight\n",
      "h.10.ln_1.bias\n",
      "h.10.ln_1.weight\n",
      "h.10.ln_2.bias\n",
      "h.10.ln_2.weight\n",
      "h.10.mlp.c_fc.bias\n",
      "h.10.mlp.c_fc.weight\n",
      "h.10.mlp.c_proj.bias\n",
      "h.10.mlp.c_proj.weight\n",
      "h.11.attn.bias\n",
      "h.11.attn.c_attn.bias\n",
      "h.11.attn.c_attn.weight\n",
      "h.11.attn.c_proj.bias\n",
      "h.11.attn.c_proj.weight\n",
      "h.11.ln_1.bias\n",
      "h.11.ln_1.weight\n",
      "h.11.ln_2.bias\n",
      "h.11.ln_2.weight\n",
      "h.11.mlp.c_fc.bias\n",
      "h.11.mlp.c_fc.weight\n",
      "h.11.mlp.c_proj.bias\n",
      "h.11.mlp.c_proj.weight\n",
      "h.2.attn.bias\n",
      "h.2.attn.c_attn.bias\n",
      "h.2.attn.c_attn.weight\n",
      "h.2.attn.c_proj.bias\n",
      "h.2.attn.c_proj.weight\n",
      "h.2.ln_1.bias\n",
      "h.2.ln_1.weight\n",
      "h.2.ln_2.bias\n",
      "h.2.ln_2.weight\n",
      "h.2.mlp.c_fc.bias\n",
      "h.2.mlp.c_fc.weight\n",
      "h.2.mlp.c_proj.bias\n",
      "h.2.mlp.c_proj.weight\n",
      "h.3.attn.bias\n",
      "h.3.attn.c_attn.bias\n",
      "h.3.attn.c_attn.weight\n",
      "h.3.attn.c_proj.bias\n",
      "h.3.attn.c_proj.weight\n",
      "h.3.ln_1.bias\n",
      "h.3.ln_1.weight\n",
      "h.3.ln_2.bias\n",
      "h.3.ln_2.weight\n",
      "h.3.mlp.c_fc.bias\n",
      "h.3.mlp.c_fc.weight\n",
      "h.3.mlp.c_proj.bias\n",
      "h.3.mlp.c_proj.weight\n",
      "h.4.attn.bias\n",
      "h.4.attn.c_attn.bias\n",
      "h.4.attn.c_attn.weight\n",
      "h.4.attn.c_proj.bias\n",
      "h.4.attn.c_proj.weight\n",
      "h.4.ln_1.bias\n",
      "h.4.ln_1.weight\n",
      "h.4.ln_2.bias\n",
      "h.4.ln_2.weight\n",
      "h.4.mlp.c_fc.bias\n",
      "h.4.mlp.c_fc.weight\n",
      "h.4.mlp.c_proj.bias\n",
      "h.4.mlp.c_proj.weight\n",
      "h.5.attn.bias\n",
      "h.5.attn.c_attn.bias\n",
      "h.5.attn.c_attn.weight\n",
      "h.5.attn.c_proj.bias\n",
      "h.5.attn.c_proj.weight\n",
      "h.5.ln_1.bias\n",
      "h.5.ln_1.weight\n",
      "h.5.ln_2.bias\n",
      "h.5.ln_2.weight\n",
      "h.5.mlp.c_fc.bias\n",
      "h.5.mlp.c_fc.weight\n",
      "h.5.mlp.c_proj.bias\n",
      "h.5.mlp.c_proj.weight\n",
      "h.6.attn.bias\n",
      "h.6.attn.c_attn.bias\n",
      "h.6.attn.c_attn.weight\n",
      "h.6.attn.c_proj.bias\n",
      "h.6.attn.c_proj.weight\n",
      "h.6.ln_1.bias\n",
      "h.6.ln_1.weight\n",
      "h.6.ln_2.bias\n",
      "h.6.ln_2.weight\n",
      "h.6.mlp.c_fc.bias\n",
      "h.6.mlp.c_fc.weight\n",
      "h.6.mlp.c_proj.bias\n",
      "h.6.mlp.c_proj.weight\n",
      "h.7.attn.bias\n",
      "h.7.attn.c_attn.bias\n",
      "h.7.attn.c_attn.weight\n",
      "h.7.attn.c_proj.bias\n",
      "h.7.attn.c_proj.weight\n",
      "h.7.ln_1.bias\n",
      "h.7.ln_1.weight\n",
      "h.7.ln_2.bias\n",
      "h.7.ln_2.weight\n",
      "h.7.mlp.c_fc.bias\n",
      "h.7.mlp.c_fc.weight\n",
      "h.7.mlp.c_proj.bias\n",
      "h.7.mlp.c_proj.weight\n",
      "h.8.attn.bias\n",
      "h.8.attn.c_attn.bias\n",
      "h.8.attn.c_attn.weight\n",
      "h.8.attn.c_proj.bias\n",
      "h.8.attn.c_proj.weight\n",
      "h.8.ln_1.bias\n",
      "h.8.ln_1.weight\n",
      "h.8.ln_2.bias\n",
      "h.8.ln_2.weight\n",
      "h.8.mlp.c_fc.bias\n",
      "h.8.mlp.c_fc.weight\n",
      "h.8.mlp.c_proj.bias\n",
      "h.8.mlp.c_proj.weight\n",
      "h.9.attn.bias\n",
      "h.9.attn.c_attn.bias\n",
      "h.9.attn.c_attn.weight\n",
      "h.9.attn.c_proj.bias\n",
      "h.9.attn.c_proj.weight\n",
      "h.9.ln_1.bias\n",
      "h.9.ln_1.weight\n",
      "h.9.ln_2.bias\n",
      "h.9.ln_2.weight\n",
      "h.9.mlp.c_fc.bias\n",
      "h.9.mlp.c_fc.weight\n",
      "h.9.mlp.c_proj.bias\n",
      "h.9.mlp.c_proj.weight\n",
      "ln_f.bias\n",
      "ln_f.weight\n",
      "wpe.weight\n",
      "wte.weight\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/Users/uonliaquat/Downloads/model.safetensors\"\n",
    "\n",
    "with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    keys = list(f.keys())\n",
    "\n",
    "for key in keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "c0e45d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte: torch.Size([1, 4, 768])\n",
      "wpe: torch.Size([1, 4, 768])\n",
      "drop: torch.Size([1, 4, 768])\n",
      "h.0.ln_1: torch.Size([1, 4, 768])\n",
      "h.0.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.0.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.0.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.0.attn: tuple\n",
      "h.0.ln_2: torch.Size([1, 4, 768])\n",
      "h.0.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.0.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.0.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.0.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.0.mlp: torch.Size([1, 4, 768])\n",
      "h.0: tuple\n",
      "h.1.ln_1: torch.Size([1, 4, 768])\n",
      "h.1.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.1.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.1.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.1.attn: tuple\n",
      "h.1.ln_2: torch.Size([1, 4, 768])\n",
      "h.1.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.1.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.1.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.1.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.1.mlp: torch.Size([1, 4, 768])\n",
      "h.1: tuple\n",
      "h.2.ln_1: torch.Size([1, 4, 768])\n",
      "h.2.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.2.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.2.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.2.attn: tuple\n",
      "h.2.ln_2: torch.Size([1, 4, 768])\n",
      "h.2.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.2.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.2.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.2.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.2.mlp: torch.Size([1, 4, 768])\n",
      "h.2: tuple\n",
      "h.3.ln_1: torch.Size([1, 4, 768])\n",
      "h.3.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.3.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.3.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.3.attn: tuple\n",
      "h.3.ln_2: torch.Size([1, 4, 768])\n",
      "h.3.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.3.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.3.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.3.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.3.mlp: torch.Size([1, 4, 768])\n",
      "h.3: tuple\n",
      "h.4.ln_1: torch.Size([1, 4, 768])\n",
      "h.4.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.4.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.4.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.4.attn: tuple\n",
      "h.4.ln_2: torch.Size([1, 4, 768])\n",
      "h.4.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.4.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.4.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.4.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.4.mlp: torch.Size([1, 4, 768])\n",
      "h.4: tuple\n",
      "h.5.ln_1: torch.Size([1, 4, 768])\n",
      "h.5.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.5.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.5.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.5.attn: tuple\n",
      "h.5.ln_2: torch.Size([1, 4, 768])\n",
      "h.5.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.5.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.5.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.5.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.5.mlp: torch.Size([1, 4, 768])\n",
      "h.5: tuple\n",
      "h.6.ln_1: torch.Size([1, 4, 768])\n",
      "h.6.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.6.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.6.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.6.attn: tuple\n",
      "h.6.ln_2: torch.Size([1, 4, 768])\n",
      "h.6.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.6.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.6.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.6.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.6.mlp: torch.Size([1, 4, 768])\n",
      "h.6: tuple\n",
      "h.7.ln_1: torch.Size([1, 4, 768])\n",
      "h.7.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.7.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.7.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.7.attn: tuple\n",
      "h.7.ln_2: torch.Size([1, 4, 768])\n",
      "h.7.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.7.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.7.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.7.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.7.mlp: torch.Size([1, 4, 768])\n",
      "h.7: tuple\n",
      "h.8.ln_1: torch.Size([1, 4, 768])\n",
      "h.8.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.8.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.8.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.8.attn: tuple\n",
      "h.8.ln_2: torch.Size([1, 4, 768])\n",
      "h.8.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.8.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.8.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.8.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.8.mlp: torch.Size([1, 4, 768])\n",
      "h.8: tuple\n",
      "h.9.ln_1: torch.Size([1, 4, 768])\n",
      "h.9.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.9.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.9.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.9.attn: tuple\n",
      "h.9.ln_2: torch.Size([1, 4, 768])\n",
      "h.9.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.9.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.9.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.9.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.9.mlp: torch.Size([1, 4, 768])\n",
      "h.9: tuple\n",
      "h.10.ln_1: torch.Size([1, 4, 768])\n",
      "h.10.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.10.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.10.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.10.attn: tuple\n",
      "h.10.ln_2: torch.Size([1, 4, 768])\n",
      "h.10.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.10.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.10.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.10.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.10.mlp: torch.Size([1, 4, 768])\n",
      "h.10: tuple\n",
      "h.11.ln_1: torch.Size([1, 4, 768])\n",
      "h.11.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.11.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.11.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.11.attn: tuple\n",
      "h.11.ln_2: torch.Size([1, 4, 768])\n",
      "h.11.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.11.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.11.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.11.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.11.mlp: torch.Size([1, 4, 768])\n",
      "h.11: tuple\n",
      "ln_f: torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model\n",
    "\n",
    "# Load GPT-2 small\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# Dictionary to store all activations\n",
    "activations = {}\n",
    "\n",
    "# Generic hook for all modules\n",
    "def register_all_hooks(model):\n",
    "    \"\"\"\n",
    "    Registers a forward hook for every submodule in the model.\n",
    "    Stores the output of each module in the activations dict.\n",
    "    \"\"\"\n",
    "    def make_hook(name):\n",
    "        def hook(module, inputs, output):\n",
    "            # Only store tensors (or tuples of tensors)\n",
    "            if torch.is_tensor(output):\n",
    "                activations[name] = output.detach().cpu()\n",
    "            elif isinstance(output, (tuple, list)):\n",
    "                # handle modules that return tuples\n",
    "                activations[name] = tuple(\n",
    "                    o.detach().cpu() if torch.is_tensor(o) else o\n",
    "                    for o in output\n",
    "                )\n",
    "        return hook\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        module.register_forward_hook(make_hook(name))\n",
    "\n",
    "# Register hooks on all modules\n",
    "register_all_hooks(model)\n",
    "\n",
    "# Example input\n",
    "input_ids = torch.tensor([[464, 318, 257, 1332]])  # \"Once upon a time\"\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(input_ids)\n",
    "\n",
    "# Check all stored activations\n",
    "for k in activations:\n",
    "    print(f\"{k}: {activations[k].shape if torch.is_tensor(activations[k]) else 'tuple'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "66f5f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model\n",
    "\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def register_all_hooks(model):\n",
    "    def make_hook(name):\n",
    "        def hook(module, inputs, output):\n",
    "            if torch.is_tensor(output):\n",
    "                activations[name] = output.detach().cpu()\n",
    "            elif isinstance(output, (tuple, list)):\n",
    "                # handle modules that return tuples (e.g., attention)\n",
    "                activations[name] = tuple(\n",
    "                    o.detach().cpu() if torch.is_tensor(o) else o\n",
    "                    for o in output\n",
    "                )\n",
    "        return hook\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        module.register_forward_hook(make_hook(name))\n",
    "\n",
    "register_all_hooks(model)\n",
    "\n",
    "# Example run\n",
    "input_ids = torch.tensor([[464, 318, 257, 1332]])  # \"Once upon a time\"\n",
    "with torch.no_grad():\n",
    "    model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "3f975acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte\n",
      "wpe\n",
      "drop\n",
      "h.0.ln_1\n",
      "h.0.attn.c_attn\n",
      "h.0.attn.c_proj\n",
      "h.0.attn.resid_dropout\n",
      "h.0.attn\n",
      "h.0.ln_2\n",
      "h.0.mlp.c_fc\n",
      "h.0.mlp.act\n",
      "h.0.mlp.c_proj\n",
      "h.0.mlp.dropout\n",
      "h.0.mlp\n",
      "h.0\n",
      "h.1.ln_1\n",
      "h.1.attn.c_attn\n",
      "h.1.attn.c_proj\n",
      "h.1.attn.resid_dropout\n",
      "h.1.attn\n",
      "h.1.ln_2\n",
      "h.1.mlp.c_fc\n",
      "h.1.mlp.act\n",
      "h.1.mlp.c_proj\n",
      "h.1.mlp.dropout\n",
      "h.1.mlp\n",
      "h.1\n",
      "h.2.ln_1\n",
      "h.2.attn.c_attn\n",
      "h.2.attn.c_proj\n",
      "h.2.attn.resid_dropout\n",
      "h.2.attn\n",
      "h.2.ln_2\n",
      "h.2.mlp.c_fc\n",
      "h.2.mlp.act\n",
      "h.2.mlp.c_proj\n",
      "h.2.mlp.dropout\n",
      "h.2.mlp\n",
      "h.2\n",
      "h.3.ln_1\n",
      "h.3.attn.c_attn\n",
      "h.3.attn.c_proj\n",
      "h.3.attn.resid_dropout\n",
      "h.3.attn\n",
      "h.3.ln_2\n",
      "h.3.mlp.c_fc\n",
      "h.3.mlp.act\n",
      "h.3.mlp.c_proj\n",
      "h.3.mlp.dropout\n",
      "h.3.mlp\n",
      "h.3\n",
      "h.4.ln_1\n",
      "h.4.attn.c_attn\n",
      "h.4.attn.c_proj\n",
      "h.4.attn.resid_dropout\n",
      "h.4.attn\n",
      "h.4.ln_2\n",
      "h.4.mlp.c_fc\n",
      "h.4.mlp.act\n",
      "h.4.mlp.c_proj\n",
      "h.4.mlp.dropout\n",
      "h.4.mlp\n",
      "h.4\n",
      "h.5.ln_1\n",
      "h.5.attn.c_attn\n",
      "h.5.attn.c_proj\n",
      "h.5.attn.resid_dropout\n",
      "h.5.attn\n",
      "h.5.ln_2\n",
      "h.5.mlp.c_fc\n",
      "h.5.mlp.act\n",
      "h.5.mlp.c_proj\n",
      "h.5.mlp.dropout\n",
      "h.5.mlp\n",
      "h.5\n",
      "h.6.ln_1\n",
      "h.6.attn.c_attn\n",
      "h.6.attn.c_proj\n",
      "h.6.attn.resid_dropout\n",
      "h.6.attn\n",
      "h.6.ln_2\n",
      "h.6.mlp.c_fc\n",
      "h.6.mlp.act\n",
      "h.6.mlp.c_proj\n",
      "h.6.mlp.dropout\n",
      "h.6.mlp\n",
      "h.6\n",
      "h.7.ln_1\n",
      "h.7.attn.c_attn\n",
      "h.7.attn.c_proj\n",
      "h.7.attn.resid_dropout\n",
      "h.7.attn\n",
      "h.7.ln_2\n",
      "h.7.mlp.c_fc\n",
      "h.7.mlp.act\n",
      "h.7.mlp.c_proj\n",
      "h.7.mlp.dropout\n",
      "h.7.mlp\n",
      "h.7\n",
      "h.8.ln_1\n",
      "h.8.attn.c_attn\n",
      "h.8.attn.c_proj\n",
      "h.8.attn.resid_dropout\n",
      "h.8.attn\n",
      "h.8.ln_2\n",
      "h.8.mlp.c_fc\n",
      "h.8.mlp.act\n",
      "h.8.mlp.c_proj\n",
      "h.8.mlp.dropout\n",
      "h.8.mlp\n",
      "h.8\n",
      "h.9.ln_1\n",
      "h.9.attn.c_attn\n",
      "h.9.attn.c_proj\n",
      "h.9.attn.resid_dropout\n",
      "h.9.attn\n",
      "h.9.ln_2\n",
      "h.9.mlp.c_fc\n",
      "h.9.mlp.act\n",
      "h.9.mlp.c_proj\n",
      "h.9.mlp.dropout\n",
      "h.9.mlp\n",
      "h.9\n",
      "h.10.ln_1\n",
      "h.10.attn.c_attn\n",
      "h.10.attn.c_proj\n",
      "h.10.attn.resid_dropout\n",
      "h.10.attn\n",
      "h.10.ln_2\n",
      "h.10.mlp.c_fc\n",
      "h.10.mlp.act\n",
      "h.10.mlp.c_proj\n",
      "h.10.mlp.dropout\n",
      "h.10.mlp\n",
      "h.10\n",
      "h.11.ln_1\n",
      "h.11.attn.c_attn\n",
      "h.11.attn.c_proj\n",
      "h.11.attn.resid_dropout\n",
      "h.11.attn\n",
      "h.11.ln_2\n",
      "h.11.mlp.c_fc\n",
      "h.11.mlp.act\n",
      "h.11.mlp.c_proj\n",
      "h.11.mlp.dropout\n",
      "h.11.mlp\n",
      "h.11\n",
      "ln_f\n"
     ]
    }
   ],
   "source": [
    "for key in activations.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "df874053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[\"h.0.ln_1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "8cf141a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.0.attn.bias\n",
      "h.0.attn.c_attn.bias\n",
      "h.0.attn.c_attn.weight\n",
      "h.0.attn.c_proj.bias\n",
      "h.0.attn.c_proj.weight\n",
      "h.0.ln_1.bias\n",
      "h.0.ln_1.weight\n",
      "h.0.ln_2.bias\n",
      "h.0.ln_2.weight\n",
      "h.0.mlp.c_fc.bias\n",
      "h.0.mlp.c_fc.weight\n",
      "h.0.mlp.c_proj.bias\n",
      "h.0.mlp.c_proj.weight\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/Users/uonliaquat/Downloads/model.safetensors\"\n",
    "\n",
    "with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    h0_keys = [k for k in f.keys() if k.startswith(\"h.0\")]\n",
    "\n",
    "for key in h0_keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "id": "aaefd18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768]), torch.Size([768]))"
      ]
     },
     "execution_count": 1276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/uonliaquat/Downloads/model.safetensors\"\n",
    "\n",
    "with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    # ln_weight = f.get_tensor(\"h.0.ln_1.weight\")\n",
    "    # ln_bias = f.get_tensor(\"h.0.ln_1.bias\")\n",
    "    h0_ln1_weight = f.get_tensor(\"h.0.ln_1.weight\")\n",
    "    h0_ln1_bias = f.get_tensor(\"h.0.ln_1.bias\")\n",
    "\n",
    "\n",
    "h0_ln1_weight.shape, h0_ln1_bias.shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdb99f",
   "metadata": {},
   "source": [
    "## Read C Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1391,
   "id": "84265b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "['gpt.embeddings.0', 'gpt.h.0.attn.0.attention_scores', 'gpt.h.0.attn.0.attention_scores_scaled', 'gpt.h.0.attn.0.attention_weights', 'gpt.h.0.attn.0.context_vecs', 'gpt.h.0.attn.0.key_transposed', 'gpt.h.0.attn.0.output', 'gpt.h.0.attn.1.attention_scores', 'gpt.h.0.attn.1.attention_scores_scaled', 'gpt.h.0.attn.1.attention_weights']\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"/Users/uonliaquat/workspace/zerograd/c_model.safetensors\"\n",
    "\n",
    "gpt_c = {}\n",
    "\n",
    "with safe_open(filename, framework=\"pt\", device=\"cpu\") as f:\n",
    "    for key in f.keys():\n",
    "        gpt_c[key] = f.get_tensor(key)\n",
    "\n",
    "# Inspect\n",
    "print(len(gpt_c))\n",
    "print(list(gpt_c.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415effdc",
   "metadata": {},
   "source": [
    "## Python GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1439,
   "id": "1558cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model\n",
    "\n",
    "\n",
    "class GPT2_Full_Debug(nn.Module):\n",
    "    def __init__(self, model_name=\"gpt2\", device=\"cpu\", dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        gpt2 = GPT2Model.from_pretrained(model_name)\n",
    "        gpt2.eval()\n",
    "\n",
    "        # ============================================================\n",
    "        # Constants\n",
    "        # ============================================================\n",
    "        self.n_heads = gpt2.config.n_head      # 12\n",
    "        self.hidden_size = gpt2.config.n_embd  # 768\n",
    "        self.head_dim = self.hidden_size // self.n_heads\n",
    "        self.vocab_size = gpt2.config.vocab_size\n",
    "\n",
    "        # ============================================================\n",
    "        # Embeddings\n",
    "        # ============================================================\n",
    "        self.wte = nn.Embedding.from_pretrained(gpt2.wte.weight.detach().to(dtype), freeze=True)\n",
    "        self.wpe = nn.Embedding.from_pretrained(gpt2.wpe.weight.detach().to(dtype), freeze=True)\n",
    "\n",
    "        # ============================================================\n",
    "        # First Transformer Block\n",
    "        # ============================================================\n",
    "        block0 = gpt2.h[0]\n",
    "\n",
    "        # ---- LN1 ----\n",
    "        self.ln_1 = nn.LayerNorm(self.hidden_size, eps=block0.ln_1.eps, elementwise_affine=True)\n",
    "        self.ln_1.weight.data.copy_(block0.ln_1.weight)\n",
    "        self.ln_1.bias.data.copy_(block0.ln_1.bias)\n",
    "\n",
    "        # ---- Attention QKV ----\n",
    "        self.c_attn = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=True)\n",
    "        self.c_attn.weight.data.copy_(block0.attn.c_attn.weight.T)\n",
    "        self.c_attn.bias.data.copy_(block0.attn.c_attn.bias)\n",
    "\n",
    "        # ---- Attention output proj ----\n",
    "        self.c_proj_attn = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.c_proj_attn.weight.data.copy_(block0.attn.c_proj.weight.T)\n",
    "        self.c_proj_attn.bias.data.copy_(block0.attn.c_proj.bias)\n",
    "\n",
    "        # ---- LN2 ----\n",
    "        self.ln_2 = nn.LayerNorm(self.hidden_size, eps=block0.ln_2.eps, elementwise_affine=True)\n",
    "        self.ln_2.weight.data.copy_(block0.ln_2.weight)\n",
    "        self.ln_2.bias.data.copy_(block0.ln_2.bias)\n",
    "\n",
    "        # ---- MLP ----\n",
    "        self.c_fc = nn.Linear(self.hidden_size, 4 * self.hidden_size, bias=True)\n",
    "        self.c_proj_mlp = nn.Linear(4 * self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.c_fc.weight.data.copy_(block0.mlp.c_fc.weight.T)\n",
    "        self.c_fc.bias.data.copy_(block0.mlp.c_fc.bias)\n",
    "        self.c_proj_mlp.weight.data.copy_(block0.mlp.c_proj.weight.T)\n",
    "        self.c_proj_mlp.bias.data.copy_(block0.mlp.c_proj.bias)\n",
    "\n",
    "        # ============================================================\n",
    "        # Final GPT-2 LayerNorm\n",
    "        # ============================================================\n",
    "        self.ln_f = nn.LayerNorm(self.hidden_size, eps=gpt2.ln_f.eps, elementwise_affine=True)\n",
    "        self.ln_f.weight.data.copy_(gpt2.ln_f.weight)\n",
    "        self.ln_f.bias.data.copy_(gpt2.ln_f.bias)\n",
    "\n",
    "        # ============================================================\n",
    "        # LM Head (tied weights)\n",
    "        # ============================================================\n",
    "        self.lm_head = nn.Linear(self.hidden_size, self.vocab_size, bias=False)\n",
    "        self.lm_head.weight.data.copy_(self.wte.weight)  # weight tying\n",
    "\n",
    "        self.to(device=device, dtype=dtype)\n",
    "\n",
    "    # ============================================================\n",
    "    # Forward with FULL DEBUG TRACE\n",
    "    # ============================================================\n",
    "    def forward(self, input_ids):\n",
    "        bsz, seq_len = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        out = {}\n",
    "\n",
    "        # ============================================================\n",
    "        # Embeddings\n",
    "        # ============================================================\n",
    "        pos_ids = torch.arange(seq_len, device=device).unsqueeze(0)\n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(pos_ids)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        out[\"token_emb\"] = tok_emb\n",
    "        out[\"pos_emb\"] = pos_emb\n",
    "        out[\"embeddings\"] = x\n",
    "\n",
    "        # ============================================================\n",
    "        # LN 1\n",
    "        # ============================================================\n",
    "        x_ln1 = self.ln_1(x)\n",
    "        out[\"ln_1\"] = x_ln1\n",
    "\n",
    "        # ============================================================\n",
    "        # QKV Projection\n",
    "        # ============================================================\n",
    "        qkv = self.c_attn(x_ln1)\n",
    "        q, k, v = qkv.split(self.hidden_size, dim=2)\n",
    "\n",
    "        out[\"qkv\"] = qkv\n",
    "        out[\"q\"] = q\n",
    "        out[\"k\"] = k\n",
    "        out[\"v\"] = v\n",
    "\n",
    "        # ============================================================\n",
    "        # Split Heads\n",
    "        # ============================================================\n",
    "        def split_heads(x):\n",
    "            return x.view(bsz, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        qh = split_heads(q)\n",
    "        kh = split_heads(k)\n",
    "        vh = split_heads(v)\n",
    "\n",
    "        out[\"q_heads\"] = qh\n",
    "        out[\"k_heads\"] = kh\n",
    "        out[\"v_heads\"] = vh\n",
    "\n",
    "        # ============================================================\n",
    "        # K^T\n",
    "        # ============================================================\n",
    "        kh_t = kh.transpose(-2, -1)\n",
    "        out[\"k_transpose\"] = kh_t\n",
    "\n",
    "        # ============================================================\n",
    "        # QK^T\n",
    "        # ============================================================\n",
    "        qk = torch.matmul(qh, kh_t)\n",
    "        out[\"qk\"] = qk\n",
    "\n",
    "        # ============================================================\n",
    "        # Scale\n",
    "        # ============================================================\n",
    "        qk_scaled = qk * (1.0 / math.sqrt(self.head_dim))\n",
    "        out[\"qk_scaled\"] = qk_scaled\n",
    "\n",
    "        # ============================================================\n",
    "        # Causal Mask\n",
    "        # ============================================================\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).view(1, 1, seq_len, seq_len)\n",
    "        qk_masked = qk_scaled.masked_fill(causal_mask == 0, float(\"-inf\"))\n",
    "        out[\"qk_masked\"] = qk_masked\n",
    "\n",
    "        # ============================================================\n",
    "        # Softmax\n",
    "        # ============================================================\n",
    "        attn_probs = torch.softmax(qk_masked, dim=-1)\n",
    "        out[\"attn_probs\"] = attn_probs\n",
    "\n",
    "        # ============================================================\n",
    "        # Attention Output per head\n",
    "        # ============================================================\n",
    "        attn_ctx_heads = torch.matmul(attn_probs, vh)\n",
    "        out[\"attn_ctx_heads\"] = attn_ctx_heads\n",
    "\n",
    "        # ============================================================\n",
    "        # Merge Heads\n",
    "        # ============================================================\n",
    "        attn_ctx = attn_ctx_heads.transpose(1, 2).contiguous()\n",
    "        attn_ctx = attn_ctx.view(bsz, seq_len, self.hidden_size)\n",
    "        out[\"attn_ctx\"] = attn_ctx\n",
    "\n",
    "        # ============================================================\n",
    "        # Attention Projection + Residual\n",
    "        # ============================================================\n",
    "        attn_out = self.c_proj_attn(attn_ctx)\n",
    "        out[\"attn_out\"] = attn_out\n",
    "\n",
    "        x_resid1 = x + attn_out\n",
    "        out[\"resid_1\"] = x_resid1\n",
    "\n",
    "        # ============================================================\n",
    "        # LN 2\n",
    "        # ============================================================\n",
    "        x_ln2 = self.ln_2(x_resid1)\n",
    "        out[\"ln_2\"] = x_ln2\n",
    "\n",
    "        # ============================================================\n",
    "        # MLP\n",
    "        # ============================================================\n",
    "        mlp_fc = self.c_fc(x_ln2)\n",
    "        mlp_gelu = torch.nn.functional.gelu(mlp_fc)\n",
    "        mlp_out = self.c_proj_mlp(mlp_gelu)\n",
    "\n",
    "        out[\"mlp_fc\"] = mlp_fc\n",
    "        out[\"mlp_gelu\"] = mlp_gelu\n",
    "        out[\"mlp_out\"] = mlp_out\n",
    "\n",
    "        # ============================================================\n",
    "        # Block Residual\n",
    "        # ============================================================\n",
    "        x_block = x_resid1 + mlp_out\n",
    "        out[\"block_output\"] = x_block\n",
    "\n",
    "        # ============================================================\n",
    "        # FINAL GPT-2 LayerNorm\n",
    "        # ============================================================\n",
    "        x_final_ln = self.ln_f(x_block)\n",
    "        out[\"ln_final\"] = x_final_ln\n",
    "\n",
    "        # ============================================================\n",
    "        # LM HEAD\n",
    "        # ============================================================\n",
    "        logits = self.lm_head(x_final_ln)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        out[\"logits\"] = logits\n",
    "        out[\"probs\"] = probs\n",
    "\n",
    "        out[\"lm_head.weight\"] = self.lm_head.weight\n",
    "\n",
    "        last_token_probs = probs[:, -1, :]  # [bsz, vocab_size]\n",
    "\n",
    "        # Option 1: Greedy (argmax)\n",
    "        next_token_id = torch.argmax(last_token_probs, dim=-1)  # [bsz]\n",
    "\n",
    "        out[\"next_token_id\"] = next_token_id\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1440,
   "id": "6afb1601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_emb': tensor([[[-0.05,  0.02,  0.08,  ...,  0.27,  0.00,  0.09],\n",
       "          [-0.03,  0.00,  0.01,  ..., -0.12, -0.05,  0.07],\n",
       "          [-0.05,  0.01,  0.05,  ...,  0.04,  0.07, -0.04],\n",
       "          [ 0.15,  0.02,  0.03,  ...,  0.01, -0.16, -0.12]]],\n",
       "        dtype=torch.float32),\n",
       " 'pos_emb': tensor([[[    -0.02,     -0.20,      0.00,  ...,     -0.04,      0.03,\n",
       "                0.05],\n",
       "          [     0.02,     -0.05,     -0.09,  ...,      0.03,      0.01,\n",
       "               -0.00],\n",
       "          [     0.00,     -0.08,      0.05,  ...,      0.02,      0.02,\n",
       "               -0.02],\n",
       "          [    -0.00,     -0.07,      0.11,  ...,      0.01,      0.02,\n",
       "               -0.01]]], dtype=torch.float32),\n",
       " 'embeddings': tensor([[[-0.07, -0.18,  0.08,  ...,  0.23,  0.03,  0.15],\n",
       "          [-0.01, -0.05, -0.09,  ..., -0.09, -0.04,  0.07],\n",
       "          [-0.05, -0.08,  0.10,  ...,  0.06,  0.09, -0.06],\n",
       "          [ 0.15, -0.06,  0.14,  ...,  0.02, -0.14, -0.13]]],\n",
       "        dtype=torch.float32),\n",
       " 'ln_1': tensor([[[-0.04, -0.06, -0.03,  ...,  0.11,  0.00,  0.06],\n",
       "          [-0.01, -0.02, -0.12,  ..., -0.08, -0.05,  0.05],\n",
       "          [-0.05, -0.05,  0.02,  ...,  0.05,  0.08, -0.07],\n",
       "          [ 0.17, -0.02,  0.05,  ...,  0.02, -0.15, -0.14]]],\n",
       "        dtype=torch.float32),\n",
       " 'qkv': tensor([[[ 0.75, -0.56, -0.33,  ...,  0.07, -0.22, -0.07],\n",
       "          [ 0.00,  1.15, -0.88,  ...,  0.21,  0.49,  0.07],\n",
       "          [ 0.79, -0.36,  0.61,  ...,  0.16,  0.04,  0.03],\n",
       "          [ 0.34,  0.74, -0.50,  ...,  0.16,  0.06,  0.18]]],\n",
       "        dtype=torch.float32),\n",
       " 'q': tensor([[[ 0.75, -0.56, -0.33,  ..., -1.21, -0.01, -0.67],\n",
       "          [ 0.00,  1.15, -0.88,  ...,  0.43, -0.80, -1.11],\n",
       "          [ 0.79, -0.36,  0.61,  ..., -0.34, -1.58, -1.16],\n",
       "          [ 0.34,  0.74, -0.50,  ..., -0.73, -1.44, -0.58]]],\n",
       "        dtype=torch.float32),\n",
       " 'k': tensor([[[-1.22,  1.50,  0.62,  ..., -0.13,  0.31,  2.02],\n",
       "          [-1.83,  2.11,  2.35,  ...,  0.77,  1.30,  1.35],\n",
       "          [-2.31,  2.71,  1.51,  ...,  0.22,  0.67,  1.28],\n",
       "          [-3.00,  2.84,  2.82,  ..., -0.18, -0.27,  1.26]]],\n",
       "        dtype=torch.float32),\n",
       " 'v': tensor([[[-0.01,  0.13, -0.04,  ...,  0.07, -0.22, -0.07],\n",
       "          [ 0.09,  0.24, -0.18,  ...,  0.21,  0.49,  0.07],\n",
       "          [-0.05,  0.01, -0.03,  ...,  0.16,  0.04,  0.03],\n",
       "          [ 0.07,  0.05, -0.07,  ...,  0.16,  0.06,  0.18]]],\n",
       "        dtype=torch.float32),\n",
       " 'q_heads': tensor([[[[ 0.75, -0.56, -0.33,  ..., -0.43,  0.52,  0.03],\n",
       "           [ 0.00,  1.15, -0.88,  ..., -0.10, -0.08,  0.07],\n",
       "           [ 0.79, -0.36,  0.61,  ..., -1.38,  1.21,  0.52],\n",
       "           [ 0.34,  0.74, -0.50,  ..., -0.97,  0.69,  0.36]],\n",
       " \n",
       "          [[-1.59,  0.84, -1.23,  ..., -0.33,  1.21, -0.85],\n",
       "           [-2.17, -0.30, -0.76,  ...,  1.86,  1.02, -0.14],\n",
       "           [-0.89,  0.28, -1.99,  ..., -0.83,  0.24, -0.17],\n",
       "           [-0.31,  1.71, -2.04,  ..., -1.23,  0.21, -1.61]],\n",
       " \n",
       "          [[-0.37,  0.35,  0.81,  ...,  1.16, -1.17, -0.12],\n",
       "           [ 0.06,  1.32,  0.68,  ...,  1.10, -0.44,  0.45],\n",
       "           [ 0.05,  0.74,  0.63,  ...,  1.74, -0.22,  0.19],\n",
       "           [ 0.18,  0.25,  1.40,  ...,  1.48, -0.75,  1.01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.46,  0.46, -0.09,  ..., -1.03,  0.45,  0.06],\n",
       "           [ 0.30, -0.06, -0.20,  ..., -1.50,  0.28,  0.26],\n",
       "           [ 0.29, -0.22, -0.41,  ..., -0.98,  0.54,  0.08],\n",
       "           [ 0.07,  0.07,  0.11,  ..., -0.43,  0.18,  0.06]],\n",
       " \n",
       "          [[ 0.22,  1.15, -0.04,  ...,  0.31,  0.73, -1.17],\n",
       "           [-0.05,  1.35, -0.10,  ...,  0.12,  0.25, -1.38],\n",
       "           [ 0.05,  1.85,  1.00,  ..., -0.37, -0.27, -1.97],\n",
       "           [-0.10,  1.55, -0.38,  ..., -0.05,  0.15, -2.28]],\n",
       " \n",
       "          [[-0.03, -0.76,  0.96,  ..., -1.21, -0.01, -0.67],\n",
       "           [-0.21, -0.35,  0.61,  ...,  0.43, -0.80, -1.11],\n",
       "           [ 0.16, -0.70,  0.56,  ..., -0.34, -1.58, -1.16],\n",
       "           [ 0.51, -0.70,  0.22,  ..., -0.73, -1.44, -0.58]]]],\n",
       "        dtype=torch.float32),\n",
       " 'k_heads': tensor([[[[-1.22,  1.50,  0.62,  ..., -1.19, -0.88,  1.14],\n",
       "           [-1.83,  2.11,  2.35,  ..., -0.48, -1.09,  1.54],\n",
       "           [-2.31,  2.71,  1.51,  ..., -0.58, -1.93,  2.26],\n",
       "           [-3.00,  2.84,  2.82,  ..., -0.23, -2.44,  1.95]],\n",
       " \n",
       "          [[-0.38,  0.88, -0.35,  ...,  0.22,  1.76, -0.29],\n",
       "           [-1.31, -0.75, -0.82,  ...,  2.32,  3.29, -0.11],\n",
       "           [-0.71, -1.61, -2.97,  ..., -1.69,  4.45,  0.19],\n",
       "           [-0.44,  0.22, -2.13,  ..., -2.13,  3.65, -1.53]],\n",
       " \n",
       "          [[ 0.28,  0.08,  0.48,  ..., -1.36, -1.66,  0.86],\n",
       "           [ 1.01, -0.52,  0.02,  ..., -2.27, -0.29,  2.12],\n",
       "           [ 0.32, -0.03,  0.33,  ..., -3.13,  0.24,  1.33],\n",
       "           [ 0.51,  1.24,  0.76,  ..., -3.37,  0.78,  1.40]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.59, -0.26, -0.36,  ...,  0.22,  0.68,  0.79],\n",
       "           [ 0.45, -0.16, -0.17,  ...,  0.80,  0.16, -0.17],\n",
       "           [-0.09, -0.10, -0.01,  ...,  1.24,  0.62,  0.47],\n",
       "           [-0.09,  0.11, -0.52,  ...,  1.46,  0.26,  0.70]],\n",
       " \n",
       "          [[ 1.29,  1.62, -0.62,  ...,  0.05,  1.43, -1.49],\n",
       "           [ 0.83,  0.41, -0.95,  ..., -0.79,  1.45, -0.53],\n",
       "           [ 0.99,  0.83, -0.17,  ..., -1.28,  0.98, -1.00],\n",
       "           [ 0.72,  0.73, -1.23,  ..., -1.14,  1.02, -1.11]],\n",
       " \n",
       "          [[ 0.47, -0.08,  0.43,  ..., -0.13,  0.31,  2.02],\n",
       "           [-0.01,  0.35,  0.81,  ...,  0.77,  1.30,  1.35],\n",
       "           [-0.11,  1.06,  0.03,  ...,  0.22,  0.67,  1.28],\n",
       "           [-0.22, -0.31, -0.40,  ..., -0.18, -0.27,  1.26]]]],\n",
       "        dtype=torch.float32),\n",
       " 'v_heads': tensor([[[[-0.01,  0.13, -0.04,  ...,  0.02, -0.10,  0.14],\n",
       "           [ 0.09,  0.24, -0.18,  ...,  0.13, -0.10,  0.17],\n",
       "           [-0.05,  0.01, -0.03,  ...,  0.12, -0.04, -0.08],\n",
       "           [ 0.07,  0.05, -0.07,  ..., -0.15, -0.19,  0.26]],\n",
       " \n",
       "          [[ 0.41,  0.27, -0.17,  ..., -0.58, -0.25,  0.15],\n",
       "           [ 0.45,  0.13, -0.25,  ...,  0.11,  0.73,  0.00],\n",
       "           [ 0.77, -0.13,  0.35,  ..., -0.12,  0.33,  0.13],\n",
       "           [ 0.57,  0.00,  0.02,  ...,  0.23, -0.05, -0.08]],\n",
       " \n",
       "          [[ 0.06, -0.12,  0.15,  ...,  0.02,  0.01, -0.14],\n",
       "           [ 0.07, -0.08,  0.52,  ...,  0.13,  0.03, -0.24],\n",
       "           [-0.42, -0.04, -0.52,  ..., -0.13, -0.10,  0.20],\n",
       "           [-0.18,  0.12,  0.02,  ...,  0.22, -0.06,  0.02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.09, -0.17,  0.07,  ..., -0.16,  0.01, -0.04],\n",
       "           [-0.36, -0.05,  0.11,  ...,  0.07, -0.04,  0.15],\n",
       "           [-0.14, -0.20, -0.11,  ...,  0.18, -0.20,  0.40],\n",
       "           [-0.08,  0.10,  0.20,  ...,  0.02,  0.14, -0.12]],\n",
       " \n",
       "          [[ 0.14, -0.16, -0.23,  ...,  0.15,  0.08, -0.21],\n",
       "           [-0.27, -0.23,  0.29,  ...,  0.22,  0.04,  0.12],\n",
       "           [-0.21,  0.07, -0.13,  ...,  0.28,  0.04, -0.11],\n",
       "           [ 0.12, -0.31, -0.12,  ..., -0.49, -0.26, -0.04]],\n",
       " \n",
       "          [[-0.06, -0.49,  0.10,  ...,  0.07, -0.22, -0.07],\n",
       "           [ 0.17, -0.02, -0.16,  ...,  0.21,  0.49,  0.07],\n",
       "           [-0.09, -0.02, -0.07,  ...,  0.16,  0.04,  0.03],\n",
       "           [ 0.03,  0.09, -0.18,  ...,  0.16,  0.06,  0.18]]]],\n",
       "        dtype=torch.float32),\n",
       " 'k_transpose': tensor([[[[-1.22, -1.83, -2.31, -3.00],\n",
       "           [ 1.50,  2.11,  2.71,  2.84],\n",
       "           [ 0.62,  2.35,  1.51,  2.82],\n",
       "           ...,\n",
       "           [-1.19, -0.48, -0.58, -0.23],\n",
       "           [-0.88, -1.09, -1.93, -2.44],\n",
       "           [ 1.14,  1.54,  2.26,  1.95]],\n",
       " \n",
       "          [[-0.38, -1.31, -0.71, -0.44],\n",
       "           [ 0.88, -0.75, -1.61,  0.22],\n",
       "           [-0.35, -0.82, -2.97, -2.13],\n",
       "           ...,\n",
       "           [ 0.22,  2.32, -1.69, -2.13],\n",
       "           [ 1.76,  3.29,  4.45,  3.65],\n",
       "           [-0.29, -0.11,  0.19, -1.53]],\n",
       " \n",
       "          [[ 0.28,  1.01,  0.32,  0.51],\n",
       "           [ 0.08, -0.52, -0.03,  1.24],\n",
       "           [ 0.48,  0.02,  0.33,  0.76],\n",
       "           ...,\n",
       "           [-1.36, -2.27, -3.13, -3.37],\n",
       "           [-1.66, -0.29,  0.24,  0.78],\n",
       "           [ 0.86,  2.12,  1.33,  1.40]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.59,  0.45, -0.09, -0.09],\n",
       "           [-0.26, -0.16, -0.10,  0.11],\n",
       "           [-0.36, -0.17, -0.01, -0.52],\n",
       "           ...,\n",
       "           [ 0.22,  0.80,  1.24,  1.46],\n",
       "           [ 0.68,  0.16,  0.62,  0.26],\n",
       "           [ 0.79, -0.17,  0.47,  0.70]],\n",
       " \n",
       "          [[ 1.29,  0.83,  0.99,  0.72],\n",
       "           [ 1.62,  0.41,  0.83,  0.73],\n",
       "           [-0.62, -0.95, -0.17, -1.23],\n",
       "           ...,\n",
       "           [ 0.05, -0.79, -1.28, -1.14],\n",
       "           [ 1.43,  1.45,  0.98,  1.02],\n",
       "           [-1.49, -0.53, -1.00, -1.11]],\n",
       " \n",
       "          [[ 0.47, -0.01, -0.11, -0.22],\n",
       "           [-0.08,  0.35,  1.06, -0.31],\n",
       "           [ 0.43,  0.81,  0.03, -0.40],\n",
       "           ...,\n",
       "           [-0.13,  0.77,  0.22, -0.18],\n",
       "           [ 0.31,  1.30,  0.67, -0.27],\n",
       "           [ 2.02,  1.35,  1.28,  1.26]]]], dtype=torch.float32),\n",
       " 'qk': tensor([[[[     4.84,     -9.45,    -13.50,    -14.47],\n",
       "           [     0.71,    -21.12,    -18.86,    -23.53],\n",
       "           [    -0.03,    -14.85,    -21.36,    -22.29],\n",
       "           [     3.65,     -7.99,    -10.51,    -19.32]],\n",
       " \n",
       "          [[    35.85,     36.88,     25.46,     28.24],\n",
       "           [    26.63,     89.05,     26.93,     19.31],\n",
       "           [     9.40,     11.92,     53.38,     -4.47],\n",
       "           [    28.65,     32.42,     24.18,     69.59]],\n",
       " \n",
       "          [[    -0.95,    -12.14,    -15.06,    -15.59],\n",
       "           [    -0.14,    -17.62,    -17.13,    -17.57],\n",
       "           [    -6.25,    -11.81,    -22.83,    -21.54],\n",
       "           [    -5.38,     -9.68,    -15.06,    -21.54]],\n",
       " \n",
       "          [[    -6.37,    -50.31,    -74.99,    -59.98],\n",
       "           [    -4.75,      3.30,    -29.12,    -20.64],\n",
       "           [   -10.05,    -10.88,     -0.80,     -8.23],\n",
       "           [     3.35,    -11.11,     -2.66,     25.74]],\n",
       " \n",
       "          [[    15.43,      3.73,     -9.09,    -10.01],\n",
       "           [     2.91,    -12.91,    -31.20,    -20.59],\n",
       "           [     2.62,     -9.91,    -11.94,    -12.45],\n",
       "           [     6.77,     -8.14,    -12.09,     -2.88]],\n",
       " \n",
       "          [[   111.73,     31.07,     12.39,     20.07],\n",
       "           [    59.82,     74.61,     21.49,     15.13],\n",
       "           [    28.18,     14.10,     42.19,     19.25],\n",
       "           [    57.62,     16.10,     19.51,     78.44]],\n",
       " \n",
       "          [[     5.48,      2.13,    -13.23,    -11.38],\n",
       "           [    10.52,     -2.44,    -15.96,    -18.13],\n",
       "           [     3.62,     -9.26,    -24.59,    -11.19],\n",
       "           [     4.67,     -4.46,    -15.69,     -8.90]],\n",
       " \n",
       "          [[   -11.64,    -22.62,    -27.56,    -32.37],\n",
       "           [   -16.87,    -43.99,    -35.78,    -41.86],\n",
       "           [   -34.06,    -38.02,    -42.08,    -40.97],\n",
       "           [   -29.50,    -36.40,    -31.81,    -38.97]],\n",
       " \n",
       "          [[    -4.51,    -10.69,     -6.79,    -16.55],\n",
       "           [    -3.08,    -14.72,     -9.68,    -20.10],\n",
       "           [   -18.79,    -21.46,     -4.63,    -25.56],\n",
       "           [   -11.13,    -12.47,     -7.83,    -20.45]],\n",
       " \n",
       "          [[     1.78,     -7.88,     -9.87,    -13.10],\n",
       "           [    -5.51,    -17.51,    -15.27,    -18.39],\n",
       "           [    -2.75,    -11.51,    -10.17,    -13.49],\n",
       "           [    -6.17,    -14.25,    -12.63,    -17.54]],\n",
       " \n",
       "          [[    21.77,     12.19,     10.86,      8.12],\n",
       "           [    31.03,     25.47,     16.53,      8.17],\n",
       "           [    29.84,     18.52,     24.43,     10.49],\n",
       "           [    27.45,     17.77,     17.90,     23.97]],\n",
       " \n",
       "          [[    -5.72,     -8.46,    -11.61,    -10.49],\n",
       "           [    -1.18,     -7.65,    -12.78,    -11.83],\n",
       "           [    -8.49,    -15.85,    -15.34,    -18.68],\n",
       "           [    -5.17,    -15.26,    -20.29,    -15.48]]]], dtype=torch.float32),\n",
       " 'qk_scaled': tensor([[[[     0.60,     -1.18,     -1.69,     -1.81],\n",
       "           [     0.09,     -2.64,     -2.36,     -2.94],\n",
       "           [    -0.00,     -1.86,     -2.67,     -2.79],\n",
       "           [     0.46,     -1.00,     -1.31,     -2.41]],\n",
       " \n",
       "          [[     4.48,      4.61,      3.18,      3.53],\n",
       "           [     3.33,     11.13,      3.37,      2.41],\n",
       "           [     1.18,      1.49,      6.67,     -0.56],\n",
       "           [     3.58,      4.05,      3.02,      8.70]],\n",
       " \n",
       "          [[    -0.12,     -1.52,     -1.88,     -1.95],\n",
       "           [    -0.02,     -2.20,     -2.14,     -2.20],\n",
       "           [    -0.78,     -1.48,     -2.85,     -2.69],\n",
       "           [    -0.67,     -1.21,     -1.88,     -2.69]],\n",
       " \n",
       "          [[    -0.80,     -6.29,     -9.37,     -7.50],\n",
       "           [    -0.59,      0.41,     -3.64,     -2.58],\n",
       "           [    -1.26,     -1.36,     -0.10,     -1.03],\n",
       "           [     0.42,     -1.39,     -0.33,      3.22]],\n",
       " \n",
       "          [[     1.93,      0.47,     -1.14,     -1.25],\n",
       "           [     0.36,     -1.61,     -3.90,     -2.57],\n",
       "           [     0.33,     -1.24,     -1.49,     -1.56],\n",
       "           [     0.85,     -1.02,     -1.51,     -0.36]],\n",
       " \n",
       "          [[    13.97,      3.88,      1.55,      2.51],\n",
       "           [     7.48,      9.33,      2.69,      1.89],\n",
       "           [     3.52,      1.76,      5.27,      2.41],\n",
       "           [     7.20,      2.01,      2.44,      9.81]],\n",
       " \n",
       "          [[     0.68,      0.27,     -1.65,     -1.42],\n",
       "           [     1.31,     -0.30,     -2.00,     -2.27],\n",
       "           [     0.45,     -1.16,     -3.07,     -1.40],\n",
       "           [     0.58,     -0.56,     -1.96,     -1.11]],\n",
       " \n",
       "          [[    -1.46,     -2.83,     -3.44,     -4.05],\n",
       "           [    -2.11,     -5.50,     -4.47,     -5.23],\n",
       "           [    -4.26,     -4.75,     -5.26,     -5.12],\n",
       "           [    -3.69,     -4.55,     -3.98,     -4.87]],\n",
       " \n",
       "          [[    -0.56,     -1.34,     -0.85,     -2.07],\n",
       "           [    -0.38,     -1.84,     -1.21,     -2.51],\n",
       "           [    -2.35,     -2.68,     -0.58,     -3.20],\n",
       "           [    -1.39,     -1.56,     -0.98,     -2.56]],\n",
       " \n",
       "          [[     0.22,     -0.98,     -1.23,     -1.64],\n",
       "           [    -0.69,     -2.19,     -1.91,     -2.30],\n",
       "           [    -0.34,     -1.44,     -1.27,     -1.69],\n",
       "           [    -0.77,     -1.78,     -1.58,     -2.19]],\n",
       " \n",
       "          [[     2.72,      1.52,      1.36,      1.01],\n",
       "           [     3.88,      3.18,      2.07,      1.02],\n",
       "           [     3.73,      2.31,      3.05,      1.31],\n",
       "           [     3.43,      2.22,      2.24,      3.00]],\n",
       " \n",
       "          [[    -0.72,     -1.06,     -1.45,     -1.31],\n",
       "           [    -0.15,     -0.96,     -1.60,     -1.48],\n",
       "           [    -1.06,     -1.98,     -1.92,     -2.34],\n",
       "           [    -0.65,     -1.91,     -2.54,     -1.93]]]], dtype=torch.float32),\n",
       " 'qk_masked': tensor([[[[     0.60,      -inf,      -inf,      -inf],\n",
       "           [     0.09,     -2.64,      -inf,      -inf],\n",
       "           [    -0.00,     -1.86,     -2.67,      -inf],\n",
       "           [     0.46,     -1.00,     -1.31,     -2.41]],\n",
       " \n",
       "          [[     4.48,      -inf,      -inf,      -inf],\n",
       "           [     3.33,     11.13,      -inf,      -inf],\n",
       "           [     1.18,      1.49,      6.67,      -inf],\n",
       "           [     3.58,      4.05,      3.02,      8.70]],\n",
       " \n",
       "          [[    -0.12,      -inf,      -inf,      -inf],\n",
       "           [    -0.02,     -2.20,      -inf,      -inf],\n",
       "           [    -0.78,     -1.48,     -2.85,      -inf],\n",
       "           [    -0.67,     -1.21,     -1.88,     -2.69]],\n",
       " \n",
       "          [[    -0.80,      -inf,      -inf,      -inf],\n",
       "           [    -0.59,      0.41,      -inf,      -inf],\n",
       "           [    -1.26,     -1.36,     -0.10,      -inf],\n",
       "           [     0.42,     -1.39,     -0.33,      3.22]],\n",
       " \n",
       "          [[     1.93,      -inf,      -inf,      -inf],\n",
       "           [     0.36,     -1.61,      -inf,      -inf],\n",
       "           [     0.33,     -1.24,     -1.49,      -inf],\n",
       "           [     0.85,     -1.02,     -1.51,     -0.36]],\n",
       " \n",
       "          [[    13.97,      -inf,      -inf,      -inf],\n",
       "           [     7.48,      9.33,      -inf,      -inf],\n",
       "           [     3.52,      1.76,      5.27,      -inf],\n",
       "           [     7.20,      2.01,      2.44,      9.81]],\n",
       " \n",
       "          [[     0.68,      -inf,      -inf,      -inf],\n",
       "           [     1.31,     -0.30,      -inf,      -inf],\n",
       "           [     0.45,     -1.16,     -3.07,      -inf],\n",
       "           [     0.58,     -0.56,     -1.96,     -1.11]],\n",
       " \n",
       "          [[    -1.46,      -inf,      -inf,      -inf],\n",
       "           [    -2.11,     -5.50,      -inf,      -inf],\n",
       "           [    -4.26,     -4.75,     -5.26,      -inf],\n",
       "           [    -3.69,     -4.55,     -3.98,     -4.87]],\n",
       " \n",
       "          [[    -0.56,      -inf,      -inf,      -inf],\n",
       "           [    -0.38,     -1.84,      -inf,      -inf],\n",
       "           [    -2.35,     -2.68,     -0.58,      -inf],\n",
       "           [    -1.39,     -1.56,     -0.98,     -2.56]],\n",
       " \n",
       "          [[     0.22,      -inf,      -inf,      -inf],\n",
       "           [    -0.69,     -2.19,      -inf,      -inf],\n",
       "           [    -0.34,     -1.44,     -1.27,      -inf],\n",
       "           [    -0.77,     -1.78,     -1.58,     -2.19]],\n",
       " \n",
       "          [[     2.72,      -inf,      -inf,      -inf],\n",
       "           [     3.88,      3.18,      -inf,      -inf],\n",
       "           [     3.73,      2.31,      3.05,      -inf],\n",
       "           [     3.43,      2.22,      2.24,      3.00]],\n",
       " \n",
       "          [[    -0.72,      -inf,      -inf,      -inf],\n",
       "           [    -0.15,     -0.96,      -inf,      -inf],\n",
       "           [    -1.06,     -1.98,     -1.92,      -inf],\n",
       "           [    -0.65,     -1.91,     -2.54,     -1.93]]]], dtype=torch.float32),\n",
       " 'attn_probs': tensor([[[[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.94,     0.06,     0.00,     0.00],\n",
       "           [    0.82,     0.13,     0.06,     0.00],\n",
       "           [    0.68,     0.16,     0.12,     0.04]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.00,     1.00,     0.00,     0.00],\n",
       "           [    0.00,     0.01,     0.99,     0.00],\n",
       "           [    0.01,     0.01,     0.00,     0.98]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.90,     0.10,     0.00,     0.00],\n",
       "           [    0.62,     0.31,     0.08,     0.00],\n",
       "           [    0.50,     0.29,     0.15,     0.07]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.27,     0.73,     0.00,     0.00],\n",
       "           [    0.20,     0.18,     0.63,     0.00],\n",
       "           [    0.06,     0.01,     0.03,     0.91]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.88,     0.12,     0.00,     0.00],\n",
       "           [    0.73,     0.15,     0.12,     0.00],\n",
       "           [    0.65,     0.10,     0.06,     0.19]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.14,     0.86,     0.00,     0.00],\n",
       "           [    0.14,     0.02,     0.83,     0.00],\n",
       "           [    0.07,     0.00,     0.00,     0.93]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.83,     0.17,     0.00,     0.00],\n",
       "           [    0.81,     0.16,     0.02,     0.00],\n",
       "           [    0.63,     0.20,     0.05,     0.12]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.97,     0.03,     0.00,     0.00],\n",
       "           [    0.51,     0.31,     0.19,     0.00],\n",
       "           [    0.40,     0.17,     0.30,     0.12]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.81,     0.19,     0.00,     0.00],\n",
       "           [    0.13,     0.09,     0.77,     0.00],\n",
       "           [    0.27,     0.23,     0.41,     0.09]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.82,     0.18,     0.00,     0.00],\n",
       "           [    0.58,     0.19,     0.23,     0.00],\n",
       "           [    0.49,     0.18,     0.22,     0.12]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.67,     0.33,     0.00,     0.00],\n",
       "           [    0.57,     0.14,     0.29,     0.00],\n",
       "           [    0.44,     0.13,     0.13,     0.29]],\n",
       " \n",
       "          [[    1.00,     0.00,     0.00,     0.00],\n",
       "           [    0.69,     0.31,     0.00,     0.00],\n",
       "           [    0.55,     0.22,     0.23,     0.00],\n",
       "           [    0.58,     0.17,     0.09,     0.16]]]], dtype=torch.float32),\n",
       " 'attn_ctx_heads': tensor([[[[-0.01,  0.13, -0.04,  ...,  0.02, -0.10,  0.14],\n",
       "           [-0.01,  0.14, -0.05,  ...,  0.03, -0.10,  0.14],\n",
       "           [-0.00,  0.14, -0.05,  ...,  0.04, -0.10,  0.13],\n",
       "           [ 0.00,  0.13, -0.06,  ...,  0.05, -0.10,  0.13]],\n",
       " \n",
       "          [[ 0.41,  0.27, -0.17,  ..., -0.58, -0.25,  0.15],\n",
       "           [ 0.45,  0.13, -0.25,  ...,  0.11,  0.73,  0.00],\n",
       "           [ 0.77, -0.13,  0.34,  ..., -0.12,  0.33,  0.13],\n",
       "           [ 0.57,  0.00,  0.02,  ...,  0.23, -0.04, -0.08]],\n",
       " \n",
       "          [[ 0.06, -0.12,  0.15,  ...,  0.02,  0.01, -0.14],\n",
       "           [ 0.06, -0.12,  0.19,  ...,  0.04,  0.01, -0.15],\n",
       "           [ 0.02, -0.10,  0.21,  ...,  0.04,  0.01, -0.14],\n",
       "           [-0.03, -0.08,  0.15,  ...,  0.04, -0.00, -0.11]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.09, -0.17,  0.07,  ..., -0.16,  0.01, -0.04],\n",
       "           [-0.14, -0.15,  0.08,  ..., -0.12,  0.00, -0.01],\n",
       "           [-0.16, -0.16,  0.04,  ..., -0.04, -0.05,  0.10],\n",
       "           [-0.15, -0.12,  0.05,  ..., -0.03, -0.03,  0.08]],\n",
       " \n",
       "          [[ 0.14, -0.16, -0.23,  ...,  0.15,  0.08, -0.21],\n",
       "           [ 0.00, -0.18, -0.06,  ...,  0.18,  0.07, -0.10],\n",
       "           [-0.02, -0.10, -0.13,  ...,  0.20,  0.06, -0.13],\n",
       "           [ 0.03, -0.18, -0.12,  ..., -0.00, -0.03, -0.10]],\n",
       " \n",
       "          [[-0.06, -0.49,  0.10,  ...,  0.07, -0.22, -0.07],\n",
       "           [ 0.01, -0.35,  0.02,  ...,  0.11,  0.00, -0.03],\n",
       "           [-0.02, -0.28,  0.00,  ...,  0.12, -0.00, -0.02],\n",
       "           [-0.01, -0.28, -0.00,  ...,  0.12, -0.03,  0.00]]]],\n",
       "        dtype=torch.float32),\n",
       " 'attn_ctx': tensor([[[-0.01,  0.13, -0.04,  ...,  0.07, -0.22, -0.07],\n",
       "          [-0.01,  0.14, -0.05,  ...,  0.11,  0.00, -0.03],\n",
       "          [-0.00,  0.14, -0.05,  ...,  0.12, -0.00, -0.02],\n",
       "          [ 0.00,  0.13, -0.06,  ...,  0.12, -0.03,  0.00]]],\n",
       "        dtype=torch.float32),\n",
       " 'attn_out': tensor([[[-0.09, -0.01, -0.20,  ...,  0.02,  0.01, -0.00],\n",
       "          [-0.54, -0.02, -0.17,  ..., -0.04,  0.05, -0.01],\n",
       "          [-0.82,  0.10, -0.21,  ..., -0.06,  0.05, -0.04],\n",
       "          [-0.63,  0.01, -0.31,  ..., -0.01,  0.01, -0.04]]],\n",
       "        dtype=torch.float32),\n",
       " 'resid_1': tensor([[[-0.16, -0.19, -0.12,  ...,  0.24,  0.04,  0.15],\n",
       "          [-0.55, -0.07, -0.26,  ..., -0.12,  0.01,  0.07],\n",
       "          [-0.86,  0.02, -0.11,  ...,  0.00,  0.15, -0.10],\n",
       "          [-0.48, -0.04, -0.17,  ...,  0.01, -0.13, -0.17]]],\n",
       "        dtype=torch.float32),\n",
       " 'ln_2': tensor([[[ 0.02, -0.00, -0.02,  ...,  0.27,  0.05,  0.14],\n",
       "          [-0.02,  0.02, -0.04,  ..., -0.16,  0.00,  0.04],\n",
       "          [-0.06,  0.03, -0.02,  ..., -0.05,  0.14, -0.13],\n",
       "          [-0.02,  0.02, -0.03,  ..., -0.02, -0.16, -0.18]]],\n",
       "        dtype=torch.float32),\n",
       " 'mlp_fc': tensor([[[-0.12, -0.94, -0.66,  ..., -3.15, -1.40,  0.87],\n",
       "          [ 0.11, -0.58, -0.61,  ..., -1.82, -0.52, -0.48],\n",
       "          [ 0.15, -2.05, -1.56,  ..., -0.99, -1.42,  0.18],\n",
       "          [ 0.15, -0.88, -0.57,  ..., -1.36, -1.44, -0.81]]],\n",
       "        dtype=torch.float32),\n",
       " 'mlp_gelu': tensor([[[-0.05, -0.16, -0.17,  ..., -0.00, -0.11,  0.71],\n",
       "          [ 0.06, -0.16, -0.17,  ..., -0.06, -0.16, -0.15],\n",
       "          [ 0.08, -0.04, -0.09,  ..., -0.16, -0.11,  0.10],\n",
       "          [ 0.08, -0.17, -0.16,  ..., -0.12, -0.11, -0.17]]],\n",
       "        dtype=torch.float32),\n",
       " 'mlp_out': tensor([[[ 1.16,  0.69,  0.46,  ..., -2.28,  1.43, -0.93],\n",
       "          [-0.16, -0.30, -0.14,  ..., -0.40,  0.52, -0.14],\n",
       "          [-0.67,  0.36, -0.07,  ..., -0.27, -0.05, -0.35],\n",
       "          [ 0.70, -0.63,  0.36,  ...,  0.49, -0.47,  0.77]]],\n",
       "        dtype=torch.float32),\n",
       " 'block_output': tensor([[[ 1.00,  0.50,  0.34,  ..., -2.03,  1.47, -0.78],\n",
       "          [-0.70, -0.38, -0.40,  ..., -0.52,  0.52, -0.07],\n",
       "          [-1.53,  0.38, -0.19,  ..., -0.27,  0.10, -0.46],\n",
       "          [ 0.21, -0.67,  0.19,  ...,  0.50, -0.60,  0.60]]],\n",
       "        dtype=torch.float32),\n",
       " 'ln_final': tensor([[[ 0.29,  0.17,  0.05,  ..., -0.53,  0.32, -0.20],\n",
       "          [-0.54, -0.25, -0.49,  ..., -0.33,  0.28, -0.02],\n",
       "          [-1.12,  0.26, -0.29,  ..., -0.19, -0.00, -0.28],\n",
       "          [ 0.14, -0.47,  0.10,  ...,  0.27, -0.38,  0.40]]],\n",
       "        dtype=torch.float32),\n",
       " 'logits': tensor([[[ -0.74,  -1.82,  -5.20,  ..., -10.75,  -7.67,  -2.77],\n",
       "          [ -9.92, -12.56, -16.40,  ..., -19.87, -12.17, -12.26],\n",
       "          [-15.63, -11.38, -16.86,  ..., -22.26, -17.58, -14.94],\n",
       "          [ -8.44,  -8.45, -14.33,  ..., -16.61, -18.35, -10.95]]],\n",
       "        dtype=torch.float32),\n",
       " 'probs': tensor([[[    0.00,     0.00,     0.00,  ...,     0.00,     0.00,     0.00],\n",
       "          [    0.00,     0.00,     0.00,  ...,     0.00,     0.00,     0.00],\n",
       "          [    0.00,     0.00,     0.00,  ...,     0.00,     0.00,     0.00],\n",
       "          [    0.00,     0.00,     0.00,  ...,     0.00,     0.00,     0.00]]],\n",
       "        dtype=torch.float32),\n",
       " 'lm_head.weight': Parameter containing:\n",
       " tensor([[-0.11, -0.04,  0.03,  ..., -0.14,  0.02,  0.05],\n",
       "         [ 0.04, -0.05,  0.05,  ...,  0.09,  0.00,  0.04],\n",
       "         [-0.13,  0.05,  0.18,  ...,  0.09, -0.13, -0.09],\n",
       "         ...,\n",
       "         [-0.04, -0.05,  0.01,  ...,  0.10,  0.10, -0.07],\n",
       "         [ 0.19,  0.02,  0.05,  ..., -0.10,  0.08, -0.02],\n",
       "         [ 0.05, -0.03,  0.05,  ...,  0.01,  0.16,  0.12]], dtype=torch.float32,\n",
       "        requires_grad=True),\n",
       " 'next_token_id': tensor([640])}"
      ]
     },
     "execution_count": 1440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create embedding module\n",
    "gpt2_p = GPT2_Full_Debug().eval()\n",
    "\n",
    "# Example input\n",
    "input_ids = torch.tensor([[7454, 2402, 257, 640]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = gpt2_p(input_ids)\n",
    "\n",
    "# print(out[\"embeddings\"].shape)  # (1, 4, 768)\n",
    "# print(out[\"ln_1\"].shape)        # (1, 4, 768)\n",
    "# print(out[\"c_attn\"].shape)      # (1, 4, 2304)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1441,
   "id": "70793d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([640])"
      ]
     },
     "execution_count": 1441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"next_token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "id": "dc37fa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 50257]), torch.Size([50257, 768]))"
      ]
     },
     "execution_count": 1400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_c['head.weight'].shape, out[\"lm_head.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "id": "408134ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float32, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 1401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(gpt_c['head.weight'] - out[\"lm_head.weight\"].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1404,
   "id": "f66796af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float32)"
      ]
     },
     "execution_count": 1404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out[\"embeddings\"] - gpt_c['gpt.embeddings.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1405,
   "id": "4772a235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out[\"ln_1\"] - gpt_c['gpt.h.0.ln_0.output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1406,
   "id": "724d9fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out[\"qkv\"] - gpt_c['gpt.h.0.c_attn.output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "id": "a4b47f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['q'] - gpt_c[\"gpt.h.0.q\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1408,
   "id": "30d068f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['k'] - gpt_c[\"gpt.h.0.k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "id": "17fe8d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['v'] - gpt_c[\"gpt.h.0.v\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "id": "9706000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "1 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "2 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "3 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "4 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "5 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "6 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "7 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "8 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "9 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "10 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "11 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for h in range(12):\n",
    "    head = out['k_heads'][:, h, :, :]\n",
    "    print(h, head.shape, gpt_c[f'gpt.h.0.k_head.{h}'].shape)\n",
    "    print(torch.max(head - gpt_c[f'gpt.h.0.k_head.{h}']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "id": "adab28e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "1 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "2 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "3 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "4 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "5 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "6 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "7 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "8 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "9 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "10 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "11 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for h in range(12):\n",
    "    head = out['q_heads'][:, h, :, :]\n",
    "    print(h, head.shape, gpt_c[f'gpt.h.0.q_head.{h}'].shape)\n",
    "    print(torch.max(head - gpt_c[f'gpt.h.0.q_head.{h}']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "id": "55c861fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "1 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "2 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "3 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "4 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "5 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "6 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "7 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "8 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "9 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "10 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "11 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for h in range(12):\n",
    "    head = out['v_heads'][:, h, :, :]\n",
    "    print(h, head.shape, gpt_c[f'gpt.h.0.v_head.{h}'].shape)\n",
    "    print(torch.max(head - gpt_c[f'gpt.h.0.v_head.{h}']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb350c",
   "metadata": {},
   "source": [
    "# K^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "id": "ac0b720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "1 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "2 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "3 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "4 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "5 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "6 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "7 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "8 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "9 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "10 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "11 torch.Size([1, 64, 4]) torch.Size([1, 64, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for h in range(12):\n",
    "    k_transpose = out['k_transpose'][:, h, :, :]\n",
    "    print(h, k_transpose.shape, gpt_c[f'gpt.h.0.attn.{h}.key_transposed'].shape)\n",
    "    print(torch.max(k_transpose - gpt_c[f'gpt.h.0.attn.{h}.key_transposed']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7bef27",
   "metadata": {},
   "source": [
    "# Q.K^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "id": "6f5ae781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "1 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "2 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "3 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "4 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "5 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "6 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "7 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "8 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "9 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "10 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "11 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for h in range(12):\n",
    "    qk = out['qk'][:, h, :, :]\n",
    "    print(h, qk.shape, gpt_c[f'gpt.h.0.attn.{h}.attention_scores'].shape)\n",
    "    print(torch.max(qk - gpt_c[f'gpt.h.0.attn.{h}.attention_scores']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de291d",
   "metadata": {},
   "source": [
    "## (Q.K^T)/sqrt(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "id": "e7940f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "1 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "2 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "3 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "4 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "5 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "6 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(0., dtype=torch.float32)\n",
      "7 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "8 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "9 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "10 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "11 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for h in range(12):\n",
    "    qk_masked = out['qk_masked'][:, h, :, :]\n",
    "    print(h, qk_masked.shape, gpt_c[f'gpt.h.0.attn.{h}.attention_scores_scaled'].shape)\n",
    "    print(torch.max(qk_masked - gpt_c[f'gpt.h.0.attn.{h}.attention_scores_scaled']))\n",
    "    # print(qk_scaled)\n",
    "    # print(gpt_c[f'gpt.h.0.attn.{h}.attention_scores_scaled'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ca9f7",
   "metadata": {},
   "source": [
    "## softmaxx((Q.K^T)/sqrt(embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "id": "dcfebbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "1 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "2 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "3 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "4 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "5 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "6 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "7 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "8 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "9 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "10 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "11 torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for h in range(12):\n",
    "    attn_probs = out['attn_probs'][:, h, :, :]\n",
    "    print(h, attn_probs.shape, gpt_c[f'gpt.h.0.attn.{h}.attention_weights'].shape)\n",
    "    print(torch.max(attn_probs - gpt_c[f'gpt.h.0.attn.{h}.attention_weights']))\n",
    "    # print(qk_scaled)\n",
    "    # print(gpt_c[f'gpt.h.0.attn.{h}.attention_scores_scaled'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11292c67",
   "metadata": {},
   "source": [
    "## softmax((Q.K^T)/sqrt(embd)).V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1417,
   "id": "f4eeb0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "1 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "2 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "3 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "4 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "5 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "6 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "7 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "8 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "9 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "10 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n",
      "11 torch.Size([1, 4, 64]) torch.Size([1, 4, 64])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for h in range(12):\n",
    "    attn_ctx_heads = out['attn_ctx_heads'][:, h, :, :]\n",
    "    print(h, attn_ctx_heads.shape, gpt_c[f'gpt.h.0.attn.{h}.context_vecs'].shape)\n",
    "    print(torch.max(attn_ctx_heads - gpt_c[f'gpt.h.0.attn.{h}.context_vecs']))\n",
    "    # print(qk_scaled)\n",
    "    # print(gpt_c[f'gpt.h.0.attn.{h}.attention_scores_scaled'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1418,
   "id": "44e4eeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "attn_ctx = out['attn_ctx']\n",
    "print(h, attn_ctx.shape, gpt_c[f'gpt.h.0.context_vec'].shape)\n",
    "print(torch.max(attn_ctx - gpt_c[f'gpt.h.0.context_vec']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1419,
   "id": "899f1ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 torch.Size([1, 4, 768]) torch.Size([1, 4, 768])\n",
      "tensor(    0.00, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "attn_out = out['attn_out']\n",
    "print(h, attn_out.shape, gpt_c[f'gpt.h.0.c_proj.output'].shape)\n",
    "print(torch.max(attn_out - gpt_c[f'gpt.h.0.c_proj.output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "id": "973edf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['resid_1'] - gpt_c[\"gpt.h.0.res_out.0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1421,
   "id": "eb3b4402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['resid_1'] - gpt_c[\"gpt.h.0.res_out.0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "id": "5adeeaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['ln_2'] - gpt_c[\"gpt.h.0.ln_1.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1423,
   "id": "4bfa286a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['mlp_fc'] - gpt_c[\"gpt.h.0.mlp.c_fc.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1424,
   "id": "0c983ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['mlp_gelu'] - gpt_c[\"gpt.h.0.mlp.gelu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1425,
   "id": "fc6adf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['mlp_out'] - gpt_c[\"gpt.h.0.mlp.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1426,
   "id": "d86e90de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['block_output'] - gpt_c[\"gpt.h.0.res_out.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1427,
   "id": "2e2ddac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['ln_final'] - gpt_c[\"gpt.ln_f.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1432,
   "id": "38fd15f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['logits'] - gpt_c[\"gpt.head.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1438,
   "id": "f4b20792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(    -0.00, dtype=torch.float32)"
      ]
     },
     "execution_count": 1438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(out['probs'] - gpt_c[\"gpt.output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1355,
   "id": "7d2fafbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.03, -0.03, -0.03,  ...,  0.00,  0.00,  0.00],\n",
       "         [ 0.05,  0.05,  0.05,  ...,  0.00,  0.00,  0.00],\n",
       "         [ 0.10,  0.10,  0.10,  ...,  0.00,  0.00,  0.00],\n",
       "         [-0.01, -0.01, -0.01,  ...,  0.00,  0.00,  0.00]]],\n",
       "       dtype=torch.float32)"
      ]
     },
     "execution_count": 1355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_c[\"gpt.out_proj.output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1376,
   "id": "e671f374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token_emb',\n",
       " 'pos_emb',\n",
       " 'embeddings',\n",
       " 'ln_1',\n",
       " 'qkv',\n",
       " 'q',\n",
       " 'k',\n",
       " 'v',\n",
       " 'q_heads',\n",
       " 'k_heads',\n",
       " 'v_heads',\n",
       " 'k_transpose',\n",
       " 'qk',\n",
       " 'qk_scaled',\n",
       " 'qk_masked',\n",
       " 'attn_probs',\n",
       " 'attn_ctx_heads',\n",
       " 'attn_ctx',\n",
       " 'attn_out',\n",
       " 'resid_1',\n",
       " 'ln_2',\n",
       " 'mlp_fc',\n",
       " 'mlp_gelu',\n",
       " 'mlp_out',\n",
       " 'block_output',\n",
       " 'ln_final',\n",
       " 'logits',\n",
       " 'probs',\n",
       " 'lm_head.weight']"
      ]
     },
     "execution_count": 1376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(out.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1431,
   "id": "43e31dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.74,  -1.82,  -5.20,  ..., -10.75,  -7.67,  -2.77],\n",
       "         [ -9.92, -12.56, -16.40,  ..., -19.87, -12.17, -12.26],\n",
       "         [-15.63, -11.38, -16.86,  ..., -22.26, -17.58, -14.94],\n",
       "         [ -8.44,  -8.45, -14.33,  ..., -16.61, -18.35, -10.95]]],\n",
       "       dtype=torch.float32)"
      ]
     },
     "execution_count": 1431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_c[\"gpt.head.output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1430,
   "id": "14d3b3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt.embeddings.0',\n",
       " 'gpt.h.0.attn.0.attention_scores',\n",
       " 'gpt.h.0.attn.0.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.0.attention_weights',\n",
       " 'gpt.h.0.attn.0.context_vecs',\n",
       " 'gpt.h.0.attn.0.key_transposed',\n",
       " 'gpt.h.0.attn.0.output',\n",
       " 'gpt.h.0.attn.1.attention_scores',\n",
       " 'gpt.h.0.attn.1.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.1.attention_weights',\n",
       " 'gpt.h.0.attn.1.context_vecs',\n",
       " 'gpt.h.0.attn.1.key_transposed',\n",
       " 'gpt.h.0.attn.1.output',\n",
       " 'gpt.h.0.attn.10.attention_scores',\n",
       " 'gpt.h.0.attn.10.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.10.attention_weights',\n",
       " 'gpt.h.0.attn.10.context_vecs',\n",
       " 'gpt.h.0.attn.10.key_transposed',\n",
       " 'gpt.h.0.attn.10.output',\n",
       " 'gpt.h.0.attn.11.attention_scores',\n",
       " 'gpt.h.0.attn.11.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.11.attention_weights',\n",
       " 'gpt.h.0.attn.11.context_vecs',\n",
       " 'gpt.h.0.attn.11.key_transposed',\n",
       " 'gpt.h.0.attn.11.output',\n",
       " 'gpt.h.0.attn.2.attention_scores',\n",
       " 'gpt.h.0.attn.2.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.2.attention_weights',\n",
       " 'gpt.h.0.attn.2.context_vecs',\n",
       " 'gpt.h.0.attn.2.key_transposed',\n",
       " 'gpt.h.0.attn.2.output',\n",
       " 'gpt.h.0.attn.3.attention_scores',\n",
       " 'gpt.h.0.attn.3.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.3.attention_weights',\n",
       " 'gpt.h.0.attn.3.context_vecs',\n",
       " 'gpt.h.0.attn.3.key_transposed',\n",
       " 'gpt.h.0.attn.3.output',\n",
       " 'gpt.h.0.attn.4.attention_scores',\n",
       " 'gpt.h.0.attn.4.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.4.attention_weights',\n",
       " 'gpt.h.0.attn.4.context_vecs',\n",
       " 'gpt.h.0.attn.4.key_transposed',\n",
       " 'gpt.h.0.attn.4.output',\n",
       " 'gpt.h.0.attn.5.attention_scores',\n",
       " 'gpt.h.0.attn.5.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.5.attention_weights',\n",
       " 'gpt.h.0.attn.5.context_vecs',\n",
       " 'gpt.h.0.attn.5.key_transposed',\n",
       " 'gpt.h.0.attn.5.output',\n",
       " 'gpt.h.0.attn.6.attention_scores',\n",
       " 'gpt.h.0.attn.6.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.6.attention_weights',\n",
       " 'gpt.h.0.attn.6.context_vecs',\n",
       " 'gpt.h.0.attn.6.key_transposed',\n",
       " 'gpt.h.0.attn.6.output',\n",
       " 'gpt.h.0.attn.7.attention_scores',\n",
       " 'gpt.h.0.attn.7.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.7.attention_weights',\n",
       " 'gpt.h.0.attn.7.context_vecs',\n",
       " 'gpt.h.0.attn.7.key_transposed',\n",
       " 'gpt.h.0.attn.7.output',\n",
       " 'gpt.h.0.attn.8.attention_scores',\n",
       " 'gpt.h.0.attn.8.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.8.attention_weights',\n",
       " 'gpt.h.0.attn.8.context_vecs',\n",
       " 'gpt.h.0.attn.8.key_transposed',\n",
       " 'gpt.h.0.attn.8.output',\n",
       " 'gpt.h.0.attn.9.attention_scores',\n",
       " 'gpt.h.0.attn.9.attention_scores_scaled',\n",
       " 'gpt.h.0.attn.9.attention_weights',\n",
       " 'gpt.h.0.attn.9.context_vecs',\n",
       " 'gpt.h.0.attn.9.key_transposed',\n",
       " 'gpt.h.0.attn.9.output',\n",
       " 'gpt.h.0.c_attn.a',\n",
       " 'gpt.h.0.c_attn.output',\n",
       " 'gpt.h.0.c_proj.a',\n",
       " 'gpt.h.0.c_proj.output',\n",
       " 'gpt.h.0.context_vec',\n",
       " 'gpt.h.0.k',\n",
       " 'gpt.h.0.k_head.0',\n",
       " 'gpt.h.0.k_head.1',\n",
       " 'gpt.h.0.k_head.10',\n",
       " 'gpt.h.0.k_head.11',\n",
       " 'gpt.h.0.k_head.2',\n",
       " 'gpt.h.0.k_head.3',\n",
       " 'gpt.h.0.k_head.4',\n",
       " 'gpt.h.0.k_head.5',\n",
       " 'gpt.h.0.k_head.6',\n",
       " 'gpt.h.0.k_head.7',\n",
       " 'gpt.h.0.k_head.8',\n",
       " 'gpt.h.0.k_head.9',\n",
       " 'gpt.h.0.ln_0.mean_var',\n",
       " 'gpt.h.0.ln_0.output',\n",
       " 'gpt.h.0.ln_0.x_norm',\n",
       " 'gpt.h.0.ln_0.x_norm_scaled',\n",
       " 'gpt.h.0.ln_0.x_norm_shifted',\n",
       " 'gpt.h.0.ln_1.mean_var',\n",
       " 'gpt.h.0.ln_1.output',\n",
       " 'gpt.h.0.ln_1.x_norm',\n",
       " 'gpt.h.0.ln_1.x_norm_scaled',\n",
       " 'gpt.h.0.ln_1.x_norm_shifted',\n",
       " 'gpt.h.0.mlp.c_fc.a',\n",
       " 'gpt.h.0.mlp.c_fc.output',\n",
       " 'gpt.h.0.mlp.c_proj.a',\n",
       " 'gpt.h.0.mlp.c_proj.output',\n",
       " 'gpt.h.0.mlp.gelu',\n",
       " 'gpt.h.0.mlp.output',\n",
       " 'gpt.h.0.output',\n",
       " 'gpt.h.0.q',\n",
       " 'gpt.h.0.q_head.0',\n",
       " 'gpt.h.0.q_head.1',\n",
       " 'gpt.h.0.q_head.10',\n",
       " 'gpt.h.0.q_head.11',\n",
       " 'gpt.h.0.q_head.2',\n",
       " 'gpt.h.0.q_head.3',\n",
       " 'gpt.h.0.q_head.4',\n",
       " 'gpt.h.0.q_head.5',\n",
       " 'gpt.h.0.q_head.6',\n",
       " 'gpt.h.0.q_head.7',\n",
       " 'gpt.h.0.q_head.8',\n",
       " 'gpt.h.0.q_head.9',\n",
       " 'gpt.h.0.res_out.0',\n",
       " 'gpt.h.0.res_out.1',\n",
       " 'gpt.h.0.v',\n",
       " 'gpt.h.0.v_head.0',\n",
       " 'gpt.h.0.v_head.1',\n",
       " 'gpt.h.0.v_head.10',\n",
       " 'gpt.h.0.v_head.11',\n",
       " 'gpt.h.0.v_head.2',\n",
       " 'gpt.h.0.v_head.3',\n",
       " 'gpt.h.0.v_head.4',\n",
       " 'gpt.h.0.v_head.5',\n",
       " 'gpt.h.0.v_head.6',\n",
       " 'gpt.h.0.v_head.7',\n",
       " 'gpt.h.0.v_head.8',\n",
       " 'gpt.h.0.v_head.9',\n",
       " 'gpt.head.a',\n",
       " 'gpt.head.output',\n",
       " 'gpt.ln_f.mean_var',\n",
       " 'gpt.ln_f.output',\n",
       " 'gpt.ln_f.x_norm',\n",
       " 'gpt.ln_f.x_norm_scaled',\n",
       " 'gpt.ln_f.x_norm_shifted',\n",
       " 'gpt.next_token_prob_dist',\n",
       " 'gpt.output',\n",
       " 'gpt.wpe.output',\n",
       " 'gpt.wte.output',\n",
       " 'head.weight']"
      ]
     },
     "execution_count": 1430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gpt_c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1442,
   "id": "f654c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 52\n",
      "Token IDs:\n",
      "[818, 2274, 812, 11, 11666, 4430, 468, 1716, 281, 1593, 2891, 287, 867, 11798, 11, 5742, 4837, 16602, 1366, 11, 43511, 8861, 11, 290, 2987, 2551, 1642, 13, 10850, 4673, 4981, 389, 8776, 319, 1588, 40522, 290, 460, 7716, 2420, 11, 7564, 7572, 11, 290, 4331, 10906, 1912, 319, 2180, 6096, 13]\n",
      "\n",
      "Decoded text:\n",
      "In recent years, artificial intelligence has become an important tool in many industries, helping researchers analyze data, automate tasks, and improve decision making. Machine learning models are trained on large datasets and can generate text, recognize patterns, and predict outcomes based on previous examples.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "paragraph = (\n",
    "    \"In recent years, artificial intelligence has become an important tool \"\n",
    "    \"in many industries, helping researchers analyze data, automate tasks, \"\n",
    "    \"and improve decision making. Machine learning models are trained on \"\n",
    "    \"large datasets and can generate text, recognize patterns, and predict \"\n",
    "    \"outcomes based on previous examples.\"\n",
    ")\n",
    "\n",
    "# Encode\n",
    "token_ids = tokenizer.encode(paragraph)\n",
    "\n",
    "print(\"Number of tokens:\", len(token_ids))\n",
    "print(\"Token IDs:\")\n",
    "print(token_ids)\n",
    "\n",
    "# Optional: decode back to verify\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded text:\")\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1443,
   "id": "a753ca95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In recent years, artificial intelligence has become an important tool in many industries, helping researchers analyze data, automate tasks, and improve decision making. Machine learning models are trained on large datasets and can generate text, recognize patterns, and predict outcomes based on previous examples.'"
      ]
     },
     "execution_count": 1443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1445,
   "id": "d62d636a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 1445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "818, 2274, 812, 11, 11666, 4430, 468, 1716, 281, 1593, 2891, 287, 867, 11798, 11, 5742, 4837, 16602, 1366, 11, 43511, 8861, 11, 290, 2987, 2551, 1642, 13, 10850, 4673, 4981, 389, 8776, 319, 1588, 40522, 290, 460, 7716, 2420, 11, 7564, 7572, 11, 290, 4331, 10906, 1912, 319, 2180, 6096, 13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
