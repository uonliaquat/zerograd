{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb770f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7586e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensors(filename):\n",
    "    tensors_dict = {}\n",
    "    \n",
    "    # Tracking current tensor state\n",
    "    current_name = \"default_tensor\"\n",
    "    current_metadata = {}\n",
    "    current_data = []\n",
    "    \n",
    "    meta_keys = ['size', 'ndim', 'shape', 'stride', 'elem_size', 'requires_grad']\n",
    "\n",
    "    def finalize_tensor(name, meta, data):\n",
    "        \"\"\"Helper to reshape data and store in the dictionary.\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "        \n",
    "        target_shape = [int(s) for s in meta.get('shape', [len(data)])]\n",
    "        # Using float64 (double) as your images show high precision decimals\n",
    "        # if name == current_name:\n",
    "        #     tensors_dict = torch.tensor(data, dtype=torch.float32).reshape(target_shape)\n",
    "        # else:\n",
    "        tensors_dict[name] = torch.tensor(data, dtype=torch.float64).reshape(target_shape)\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split by comma and remove empty strings/whitespace\n",
    "            parts = [p.strip() for p in line.split(',') if p.strip()]\n",
    "            \n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            label = parts[0].replace(':', '')\n",
    "\n",
    "            # 1. Check if it's a known metadata key\n",
    "            if label in meta_keys:\n",
    "                vals = [float(v) for v in parts[1:]]\n",
    "                current_metadata[label] = vals[0] if len(vals) == 1 else vals\n",
    "            \n",
    "            else:\n",
    "                try:\n",
    "                    # 2. Try to parse as numeric data\n",
    "                    row_data = [float(p) for p in parts]\n",
    "                    current_data.extend(row_data)\n",
    "                except ValueError:\n",
    "                    # 3. If it's a string but NOT metadata, it's a new tensor name\n",
    "                    # Save the previous tensor first\n",
    "                    if current_data:\n",
    "                        finalize_tensor(current_name, current_metadata, current_data)\n",
    "                    \n",
    "                    # Reset for the new tensor\n",
    "                    current_name = label\n",
    "                    current_metadata = {}\n",
    "                    current_data = []\n",
    "\n",
    "    # Finalize the last tensor in the file\n",
    "    finalize_tensor(current_name, current_metadata, current_data)\n",
    "    if \"default_tensor\" in tensors_dict.keys():\n",
    "        tensors_dict = tensors_dict[\"default_tensor\"]\n",
    "    return tensors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3c7a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_model(path):\n",
    "    gpt_model = {}\n",
    "    folder_path = Path(path)\n",
    "    for file in folder_path.iterdir():\n",
    "        if file.is_file():  # Ensure it's a file and not a subfolder\n",
    "            filename = Path(file.name).stem\n",
    "            # print(f\"Filename: {filename}\")\n",
    "            gpt_model[filename] = load_tensors(file)\n",
    "            # print(f\"Full Path: {file}\")\n",
    "    return gpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fa39cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Weights': tensor([[ 0.85,  0.90, -0.04,  ..., -0.58, -0.18,  0.63],\n",
       "         [ 0.28, -0.13, -0.90,  ..., -0.52,  0.49,  0.62],\n",
       "         [ 0.62,  0.66, -0.76,  ...,  0.83,  0.33, -0.73],\n",
       "         ...,\n",
       "         [ 0.94,  0.39,  0.47,  ...,  0.83,  0.61, -0.22],\n",
       "         [-0.61,  0.08, -0.45,  ...,  0.30, -0.30, -0.89],\n",
       "         [-0.06,  0.91, -0.10,  ..., -0.90,  0.63, -0.89]]),\n",
       " 'Output': tensor([[[    -2.76,     -0.48,      5.78,      0.43,      5.98,      6.62,\n",
       "               -6.77,      1.45,      6.02,     -8.07,      5.96,     14.18,\n",
       "               -1.58,      3.50,      8.39,      1.56,      2.96,    -13.13,\n",
       "               -3.33,      4.71,      5.05,      7.32,      7.52,      9.71,\n",
       "                4.27,    -10.34,      2.62,      6.44,    -10.13,     -6.80,\n",
       "               -1.95,    -16.84],\n",
       "          [    -0.42,     -5.68,     -5.70,     10.29,     12.53,      4.34,\n",
       "                4.55,      1.88,      2.43,     -8.14,     -1.08,      5.87,\n",
       "               10.10,     -5.06,     13.84,     -8.20,      9.89,     -2.25,\n",
       "                2.49,     15.63,      1.92,     -4.66,     -4.83,      1.01,\n",
       "               -8.26,    -26.62,      0.27,     -5.13,    -15.58,     20.13,\n",
       "               -2.94,    -10.78],\n",
       "          [     5.83,      0.99,      5.05,    -12.93,     -2.88,     -2.98,\n",
       "                8.35,    -17.04,     14.30,     -3.02,      6.21,     -3.88,\n",
       "               -4.28,      1.96,      3.64,     -4.51,     15.86,    -17.62,\n",
       "               -8.18,     -3.88,     -1.62,     -0.01,      3.31,      3.31,\n",
       "               -3.51,     -6.35,      4.97,     -9.78,    -11.61,     11.94,\n",
       "                8.57,      0.89],\n",
       "          [    -0.41,     -5.68,     -5.71,     10.28,     12.54,      4.34,\n",
       "                4.55,      1.89,      2.43,     -8.15,     -1.09,      5.88,\n",
       "               10.11,     -5.05,     13.85,     -8.20,      9.90,     -2.26,\n",
       "                2.49,     15.63,      1.92,     -4.67,     -4.82,      1.01,\n",
       "               -8.26,    -26.61,      0.27,     -5.14,    -15.58,     20.14,\n",
       "               -2.94,    -10.80],\n",
       "          [     0.35,     -5.77,      2.68,     11.69,      4.98,      5.03,\n",
       "                4.83,    -14.95,      2.84,    -11.67,     -5.56,      3.60,\n",
       "               -1.14,    -16.67,     11.59,    -11.69,      4.67,      5.24,\n",
       "               -3.30,     16.64,     11.00,     -2.87,     -0.18,     -1.56,\n",
       "               -4.93,    -28.03,     -4.91,     -6.87,     -3.94,      1.93,\n",
       "               -9.40,     -1.32],\n",
       "          [    -7.27,     13.47,     -5.78,      1.36,      7.17,     18.35,\n",
       "               -1.35,      0.50,      6.18,     -5.75,     -0.48,      2.46,\n",
       "                6.36,      2.86,      1.33,      7.77,     15.07,     -8.86,\n",
       "              -15.53,     -1.63,     13.30,     -6.99,      0.17,      4.41,\n",
       "                3.27,     -5.59,     16.74,      0.69,    -11.52,      6.17,\n",
       "               -2.09,    -12.23]],\n",
       " \n",
       "         [[     4.09,     -1.32,     -8.94,      9.43,      3.54,     -0.70,\n",
       "              -10.01,      6.25,    -16.73,     -9.70,    -11.20,     12.82,\n",
       "               -0.65,     -7.20,     17.35,    -11.93,      8.54,      7.34,\n",
       "                4.14,      9.48,      2.44,     -8.48,     -1.80,      3.47,\n",
       "               -9.80,    -23.57,     -3.82,     -1.91,     -6.12,    -11.36,\n",
       "              -10.66,     -2.72],\n",
       "          [    -2.70,     -6.07,     -7.34,      6.48,     -4.34,     -3.55,\n",
       "               -6.90,     10.96,     -6.31,     -6.55,     -8.63,      5.38,\n",
       "               -0.61,     -2.66,      1.98,      3.04,      0.82,      0.97,\n",
       "                3.89,     -6.33,     -4.52,      5.39,     -2.56,     -1.90,\n",
       "                1.13,    -10.30,     -6.70,      4.53,     -2.14,    -12.02,\n",
       "               -8.37,     -0.15],\n",
       "          [    -7.71,     -2.25,     -3.94,      0.97,     -0.73,      3.33,\n",
       "                5.29,     -2.04,     14.67,     -1.05,      4.77,      0.00,\n",
       "               -5.43,      1.34,     -4.24,     15.54,     12.26,    -16.74,\n",
       "               -0.35,     -8.26,     -3.37,     15.06,     -7.82,     -3.84,\n",
       "                1.44,     -5.63,      4.67,     -5.33,     -9.26,      7.49,\n",
       "                3.26,    -14.54],\n",
       "          [     3.49,      1.55,     -3.94,     -3.53,     -1.94,      8.62,\n",
       "               -4.48,     -5.53,     -2.52,     -1.13,     -9.07,      2.76,\n",
       "                3.20,      7.78,      6.43,     -8.93,      9.69,     -1.46,\n",
       "               -4.68,     -0.99,      0.60,    -16.26,      1.30,     -2.45,\n",
       "                2.00,      5.05,     -2.49,      0.74,      0.25,      1.22,\n",
       "               -7.19,     14.36],\n",
       "          [     1.77,    -12.76,     -4.74,     25.63,      2.61,     -1.23,\n",
       "               -9.40,      0.05,    -17.62,     -6.04,     -5.45,      8.92,\n",
       "                6.23,     -9.50,      2.95,    -19.99,      6.77,     14.73,\n",
       "               -5.23,     12.97,     -4.11,     -0.56,     -4.97,      8.44,\n",
       "              -19.38,    -29.11,    -10.07,     12.48,    -10.30,     -2.07,\n",
       "               -9.29,      6.83],\n",
       "          [     2.83,      6.32,      8.39,     -6.35,      2.54,      1.85,\n",
       "                0.32,    -11.69,     19.77,     -9.22,     13.46,      4.13,\n",
       "               -5.79,      4.77,      7.33,     -5.20,     12.59,    -14.88,\n",
       "              -13.57,      4.13,      1.32,     -1.44,      3.61,      0.93,\n",
       "                0.16,    -10.18,      7.36,     -2.01,     -9.38,     10.59,\n",
       "                5.86,     -0.94]]])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model_c = load_gpt_model(\"/Users/uonliaquat/workspace/zerograd/models\")\n",
    "gpt_model_c[\"transformer_layer_0__self_attention_layer_heads_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f8fdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = '../models'\n",
    "# gpt_model_c = {\n",
    "#     \"input_tokens\":  load_tensors(f'{base_path}/input_tokens.csv')['default_tensor'].long(),\n",
    "#     \"token_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.token_embed_layer.csv'),\n",
    "#     \"pos_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.pos_embed_layer.csv'),\n",
    "#     \"position_indicies\":  load_tensors(f'{base_path}/gpt_model.workspace.position_indicies.csv')['default_tensor'].long(),\n",
    "#     \"input_embeddings\": load_tensors(f'{base_path}/gpt_model.workspace.input_embeddings.csv')['default_tensor']\n",
    "# }\n",
    "# gpt_model_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b98d9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_within_tolerance(a, b, atol):\n",
    "    if a.shape != b.shape:\n",
    "        print(\"Shape mismatch:\", a.shape, b.shape)\n",
    "        return False\n",
    "\n",
    "    max_diff = round((a - b).abs().max().item(), 2)\n",
    "    print(\"max |diff|:\", max_diff)\n",
    "\n",
    "    return max_diff <= atol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12b649",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efcd23ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_layer_0__self_attention_layer_attention_scores_0\n",
      "gpt_model.pos_embed_layer\n",
      "transformer_layer_0__self_attention_layer_attention_scores_1\n",
      "input_tokens\n",
      "transformer_layer_0__self_attention_layer_context_vecs_0\n",
      "transformer_layer_0__self_attention_layer_w_key\n",
      "transformer_layer_0__self_attention_layer_context_vecs_1\n",
      "transformer_layer_0__self_attention_layer_w_query\n",
      "gpt_model.token_embed_layer\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_0\n",
      "gpt_model.workspace.position_indices\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_1\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_0\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_1\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_0\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_1\n",
      "transformer_layer_0__self_attention_layer_w_value\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_0\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_1\n",
      "transformer_layer_0__self_attention_layer_attention_weights_1\n",
      "gpt_model.workspace.input_embeddings\n",
      "transformer_layer_0__self_attention_layer_attention_weights_0\n",
      "transformer_layer_0__self_attention_layer_values_chunks_0\n",
      "transformer_layer_0__self_attention_layer_concat_heads\n",
      "transformer_layer_0__self_attention_layer_heads_proj\n",
      "transformer_layer_0__feed_forward_network_layer2\n",
      "transformer_layer_0__self_attention_layer_values_chunks_1\n",
      "transformer_layer_0__feed_forward_network_layer1\n"
     ]
    }
   ],
   "source": [
    "for key in gpt_model_c.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc403fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key(target_dict, substring):\n",
    "    \"\"\"\n",
    "    Returns the first key that contains the substring.\n",
    "    Returns None if no match is found.\n",
    "    \"\"\"\n",
    "    return next((k for k in target_dict if substring in k), None)\n",
    "\n",
    "# Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4953a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_transposed_global = None\n",
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, self_attention_layer_c, n_heads, atol):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.atol = atol\n",
    "        self.self_attention_layer_c = self_attention_layer_c\n",
    "        \n",
    "        W_query_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'w_query')]['Weights']\n",
    "        W_key_weights =     self_attention_layer_c[find_key(self_attention_layer_c, 'w_key')]['Weights']\n",
    "        W_value_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'w_value')]['Weights']\n",
    "        head_proj_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'heads_proj')]['Weights']\n",
    "\n",
    "        # print(\"W_query_weights\\n \", W_query_weights)\n",
    "        W_query_weights = W_query_weights.t()\n",
    "        W_key_weights = W_key_weights.t()\n",
    "        W_value_weights = W_value_weights.t()\n",
    "        head_proj_weights = head_proj_weights.t()\n",
    "\n",
    "        self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "        self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "        self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "        self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "        self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "        self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "\n",
    "        self.heads_proj = nn.Linear(head_proj_weights.shape[0], head_proj_weights.shape[1], bias=False)\n",
    "        self.heads_proj.weight = nn.Parameter(head_proj_weights)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "        # print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "        # print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "        # print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "        # print(f\"heads_proj\\nShape: {self.heads_proj.weight.shape}\\n{self.heads_proj.weight}\")\n",
    "        print(\"===========================================\")\n",
    "\n",
    "        query = self.W_query(x)\n",
    "        key = self.W_key(x)\n",
    "        value = self.W_value(x)\n",
    "        self.layer_name = 'self_attention_layer'\n",
    "        query_matched = tensors_within_tolerance(query, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_query')]['Output'], self.atol)\n",
    "        key_matched = tensors_within_tolerance(key,  self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_key')]['Output'], self.atol)\n",
    "        value_matched = tensors_within_tolerance(value,  self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_value')]['Output'], self.atol)\n",
    "\n",
    "        print(f\"query_matched:   {query_matched}\")\n",
    "        print(f\"key_matched:     {key_matched}\")\n",
    "        print(f\"value_matched:   {value_matched}\")\n",
    "        # print(\"Python\", query)\n",
    "        # print(\"C\", self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_query')]['Output'])\n",
    "\n",
    "        queries_chnuks  = torch.chunk(query, self.n_heads, -1)\n",
    "        keys_chnuks     = torch.chunk(key , self.n_heads, -1)\n",
    "        values_chnuks   = torch.chunk(value, self.n_heads, -1)\n",
    "\n",
    "        for head in range(0, self.n_heads):\n",
    "            #print(self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_queries_chunks_{head}')])\n",
    "            #print(self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_queries_chunks_{head}')]['Output'].shape)\n",
    "            query_chnuks_matched    = tensors_within_tolerance(queries_chnuks[head],  self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_queries_chunks_{head}')], self.atol)\n",
    "            key_chnuks_matched      = tensors_within_tolerance(keys_chnuks[head],     self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_keys_chunks_{head}')], self.atol)\n",
    "            value_chnuks_matched    = tensors_within_tolerance(values_chnuks[head],   self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_values_chunks_{head}')], self.atol)\n",
    "            print(f\"query_chnuks_matched:   {query_chnuks_matched}\")\n",
    "            print(f\"key_chnuks_matched:     {key_chnuks_matched}\")\n",
    "            print(f\"value_chnuks_matched:   {value_chnuks_matched}\")\n",
    "\n",
    "        context_vecs = []\n",
    "        for head in range(0, self.n_heads):\n",
    "            #print(self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_context_vecs_{head}')])\n",
    "            \n",
    "            print(f\"=================================== HEAD {head} ===================================\\n\")\n",
    "            key_transposed = keys_chnuks[head].transpose(1, 2)\n",
    "            key_transposed_matched = tensors_within_tolerance(key_transposed, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_keys_transposed_{head}')], self.atol)\n",
    "            print(f\"key_transposed_matched:   {key_transposed_matched}\")\n",
    "\n",
    "            attention_scores = queries_chnuks[head] @ key_transposed\n",
    "            attention_scores_matched = tensors_within_tolerance(attention_scores, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_attention_scores_{head}')], self.atol)\n",
    "            print(f\"attention_scores_matched:   {attention_scores_matched}\")\n",
    "\n",
    "\n",
    "            attention_scores_scaled = attention_scores * 1/math.sqrt(keys_chnuks[head].shape[1])\n",
    "            attention_scores_scaled_matched = tensors_within_tolerance(attention_scores_scaled, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_attention_scores_scaled_{head}')], self.atol)\n",
    "            print(f\"attention_scores_scaled_matched:   {attention_scores_scaled_matched}\")\n",
    "\n",
    "            attention_weights = F.softmax(attention_scores_scaled, dim=-1)\n",
    "            attention_weights_matched = tensors_within_tolerance(attention_weights, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_attention_weights_{head}')], self.atol)\n",
    "            print(f\"attention_weights_matched:   {attention_weights_matched}\")\n",
    "\n",
    "            context_vec = attention_weights @ values_chnuks[head]\n",
    "            context_vec_matched = tensors_within_tolerance(context_vec, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_context_vecs_{head}')], self.atol)\n",
    "            print(f\"context_vec_matched:   {context_vec_matched}\")\n",
    "\n",
    "            context_vecs.append(context_vec)\n",
    "        #     print(f\"=================================================================================\\n\")\n",
    "\n",
    "        concat_heads = torch.cat(context_vecs, dim=-1)\n",
    "        concat_heads_matched = tensors_within_tolerance(concat_heads, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_concat_heads')], self.atol)\n",
    "        print(f\"concat_heads_matched:   {concat_heads_matched}\")\n",
    "\n",
    "        projected_context_vecs = self.heads_proj(concat_heads)\n",
    "        projected_context_vecs_matched = tensors_within_tolerance(projected_context_vecs, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_heads_proj')]['Output'], self.atol)\n",
    "        print(f\"projected_context_vecs_matched:   {projected_context_vecs_matched}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b41c3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, feed_forward_network_c):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(2, 5)\n",
    "        self.output = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff63f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, transformer_layer_c, n_heads, atol):\n",
    "        super().__init__() \n",
    "        self.n_heads = n_heads\n",
    "        self.atol = atol\n",
    "        self_attention_layer_c = {k: v for k, v in transformer_layer_c.items() if \"self_attention_layer\" in k}\n",
    "        feed_forward_network_c = {k: v for k, v in transformer_layer_c.items() if \"feed_forward_network\" in k}\n",
    "        self.self_attention_multi_head  = SelfAttentionMultiHead(self_attention_layer_c, n_heads, atol)\n",
    "        #self.feed_forward_network       = FeedForwardNetwork(feed_forward_network_c)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention_multi_head(x)\n",
    "        #x = self.feed_forward_network(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b426a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, transformer_block_c, n_layers, n_heads, atol):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.atol = atol\n",
    "        self.transformer_layers = {}\n",
    "        for layer_no in range(0, n_layers):\n",
    "            transformer_layer_c = {k: v for k, v in transformer_block_c.items() if k.startswith(f\"transformer_layer_{layer_no}\")}\n",
    "            self.transformer_layers[layer_no] = Transformer(transformer_layer_c, n_heads, atol)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer_no in range(0, self.n_layers):\n",
    "            x = self.transformer_layers[layer_no](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d47f7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, gpt_model_c, n_layers, n_heads, atol):\n",
    "        super().__init__()   \n",
    "        self.atol = atol\n",
    "           \n",
    "        num_token_embeddings            = gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape[0]\n",
    "        token_embedding_dim             = gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape[1]\n",
    "        num_pos_embeddings              = gpt_model_c['gpt_model.pos_embed_layer']['Weights'].shape[0]\n",
    "        pos_embedding_dim               = gpt_model_c['gpt_model.pos_embed_layer']['Weights'].shape[1]\n",
    "\n",
    "        print(f\"num_token_embeddings: {num_token_embeddings}\")\n",
    "        print(f\"token_embedding_dim: {token_embedding_dim}\")\n",
    "        print(f\"num_pos_embeddings: {num_pos_embeddings}\")\n",
    "        print(f\"pos_embedding_dim: {pos_embedding_dim}\")\n",
    "\n",
    "        self.token_embeddings_layer     = nn.Embedding(num_embeddings=num_token_embeddings, embedding_dim=token_embedding_dim)\n",
    "        self.pos_embeddings_layer       = nn.Embedding(num_embeddings=num_pos_embeddings, embedding_dim=pos_embedding_dim)\n",
    "\n",
    "        self.token_embeddings_layer.weight.data.copy_(gpt_model_c['gpt_model.token_embed_layer']['Weights'])\n",
    "        self.pos_embeddings_layer.weight.data.copy_(gpt_model_c['gpt_model.pos_embed_layer']['Weights'])\n",
    "        \n",
    " \n",
    "        assert(gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape == self.token_embeddings_layer.weight.shape)\n",
    "        assert(gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape == self.pos_embeddings_layer.weight.shape)\n",
    "\n",
    "        \n",
    "        self.token_embeddings_c         = gpt_model_c['gpt_model.token_embed_layer']['Output']\n",
    "        self.pos_embeddings_c           = gpt_model_c['gpt_model.pos_embed_layer']['Output']\n",
    "        self.input_embeddings_c         = gpt_model_c['gpt_model.workspace.input_embeddings']\n",
    "        self.position_indicies_c        = gpt_model_c['gpt_model.workspace.position_indices'].long()\n",
    "\n",
    "        transformer_block_c = {k: v for k, v in gpt_model_c.items() if \"transformer_layer\" in k}\n",
    "        self.transformer_block = TransformerBlock(transformer_block_c, n_layers, n_heads, self.atol)\n",
    "\n",
    "    def forward(self, input_tokens_c):\n",
    "        #print(f\"x.shape:    {input_tokens_c.shape}\")\n",
    "        token_embeddings    = self.token_embeddings_layer(input_tokens_c)[0]\n",
    "        print(self.position_indicies_c)\n",
    "        pos_embeddings      = self.pos_embeddings_layer(self.position_indicies_c)[0]\n",
    "        input_embeddings    = token_embeddings + pos_embeddings\n",
    "\n",
    "\n",
    "        token_embeddings_matched    = tensors_within_tolerance(token_embeddings,    self.token_embeddings_c,    self.atol)\n",
    "        pos_embeddings_matched      = tensors_within_tolerance(pos_embeddings,      self.pos_embeddings_c,      self.atol)\n",
    "        input_embeddings_matched    = tensors_within_tolerance(input_embeddings,    self.input_embeddings_c,    self.atol)\n",
    "\n",
    "        print(f\"token_embeddings_matched:   {token_embeddings_matched}\")\n",
    "        print(f\"pos_embeddings_matched:     {pos_embeddings_matched}\")\n",
    "        print(f\"input_embeddings_matched:   {input_embeddings_matched}\")\n",
    "\n",
    "\n",
    "        # print(\"***********Token Embeddings***********\\n\")\n",
    "        # print(self.token_embeddings_c[0], \"\\n\")\n",
    "        # print(token_embeddings[0], \"\\n\\n\")\n",
    "\n",
    "        # print(\"***********Pose Embeddings***********\\n\")\n",
    "        # print(self.pos_embeddings_c[0], \"\\n\")\n",
    "        # print(pos_embeddings[0], \"\\n\\n\")\n",
    "\n",
    "\n",
    "        # print(\"***********Input Embeddings***********\\n\")\n",
    "        # print(self.input_embeddings_c[0], \"\\n\")\n",
    "        # print(input_embeddings[0], \"\\n\\n\")\n",
    "\n",
    "        # assert(\n",
    "        #     token_embeddings_matched and \n",
    "        #     pos_embeddings_matched and \n",
    "        #     input_embeddings_matched\n",
    "        # )\n",
    "        \n",
    "        contextual_embddings = self.transformer_block(input_embeddings)\n",
    "\n",
    "\n",
    "        # print(token_embeddings - self.token_embed_layer_output_c)\n",
    "        return contextual_embddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2bd660d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_token_embeddings: 6\n",
      "token_embedding_dim: 32\n",
      "num_pos_embeddings: 6\n",
      "pos_embedding_dim: 32\n",
      "tensor([[[0, 1, 2, 3, 4, 5],\n",
      "         [0, 1, 2, 3, 4, 5]]])\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "token_embeddings_matched:   True\n",
      "pos_embeddings_matched:     True\n",
      "input_embeddings_matched:   True\n",
      "===========================================\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "query_matched:   True\n",
      "key_matched:     True\n",
      "value_matched:   True\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "query_chnuks_matched:   True\n",
      "key_chnuks_matched:     True\n",
      "value_chnuks_matched:   True\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "query_chnuks_matched:   True\n",
      "key_chnuks_matched:     True\n",
      "value_chnuks_matched:   True\n",
      "=================================== HEAD 0 ===================================\n",
      "\n",
      "max |diff|: 0.0\n",
      "key_transposed_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_scaled_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_weights_matched:   True\n",
      "max |diff|: 0.0\n",
      "context_vec_matched:   True\n",
      "=================================== HEAD 1 ===================================\n",
      "\n",
      "max |diff|: 0.0\n",
      "key_transposed_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_scaled_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_weights_matched:   True\n",
      "max |diff|: 0.0\n",
      "context_vec_matched:   True\n",
      "max |diff|: 0.0\n",
      "concat_heads_matched:   True\n",
      "max |diff|: 0.0\n",
      "projected_context_vecs_matched:   True\n"
     ]
    }
   ],
   "source": [
    "atol = 0.00000001\n",
    "gpt = GPT(gpt_model_c=gpt_model_c, n_layers=1, n_heads=2, atol=atol)\n",
    "input_embeddings = gpt(gpt_model_c['input_tokens'].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65ef5fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_layer_0__self_attention_layer_context_vecs_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_0\n",
      "gpt_model.pos_embed_layer\n",
      "transformer_layer_0__self_attention_layer_attention_scores_1\n",
      "transformer_layer_0__self_attention_layer_context_vecs_2\n",
      "input_tokens\n",
      "transformer_layer_0__self_attention_layer_context_vecs_0\n",
      "transformer_layer_0__self_attention_layer_w_key\n",
      "transformer_layer_0__self_attention_layer_attention_scores_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_2\n",
      "transformer_layer_0__self_attention_layer_context_vecs_1\n",
      "transformer_layer_0__self_attention_layer_context_vecs_5\n",
      "transformer_layer_0__self_attention_layer_values_chunks_9\n",
      "transformer_layer_0__self_attention_layer_attention_scores_6\n",
      "transformer_layer_0__self_attention_layer_attention_scores_7\n",
      "transformer_layer_0__self_attention_layer_values_chunks_8\n",
      "transformer_layer_0__self_attention_layer_context_vecs_4\n",
      "transformer_layer_0__self_attention_layer_context_vecs_6\n",
      "transformer_layer_0__self_attention_layer_attention_scores_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_4\n",
      "transformer_layer_0__self_attention_layer_w_query\n",
      "transformer_layer_0__self_attention_layer_context_vecs_7\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_6\n",
      "gpt_model.token_embed_layer\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_4\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_7\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_4\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_6\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_9\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_5\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_7\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_6\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_8\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_4\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_7\n",
      "transformer_layer_0__self_attention_layer_attention_weights_8\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_0\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_2\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_1\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_2\n",
      "transformer_layer_0__self_attention_layer_attention_weights_9\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_0\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_3\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_1\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_0\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_2\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_1\n",
      "transformer_layer_0__self_attention_layer_attention_weights_7\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_3\n",
      "transformer_layer_0__self_attention_layer_w_value\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_2\n",
      "transformer_layer_0__self_attention_layer_attention_weights_6\n",
      "transformer_layer_0__self_attention_layer_attention_weights_4\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_0\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_1\n",
      "transformer_layer_0__self_attention_layer_attention_weights_5\n",
      "transformer_layer_0__self_attention_layer_attention_weights_1\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_9\n",
      "gpt_model.workspace.input_embeddings\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_4\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_8\n",
      "transformer_layer_0__self_attention_layer_attention_weights_0\n",
      "transformer_layer_0__self_attention_layer_attention_weights_2\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_9\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_6\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_8\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_9\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_7\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_8\n",
      "transformer_layer_0__self_attention_layer_attention_weights_3\n",
      "transformer_layer_0__self_attention_layer_values_chunks_6\n",
      "transformer_layer_0__self_attention_layer_attention_scores_9\n",
      "transformer_layer_0__self_attention_layer_attention_scores_8\n",
      "transformer_layer_0__self_attention_layer_values_chunks_7\n",
      "transformer_layer_0__self_attention_layer_context_vecs_9\n",
      "transformer_layer_0__self_attention_layer_values_chunks_5\n",
      "transformer_layer_0__self_attention_layer_values_chunks_4\n",
      "transformer_layer_0__self_attention_layer_context_vecs_8\n",
      "gpt_model.workspace.position_indicies\n",
      "transformer_layer_0__self_attention_layer_values_chunks_0\n",
      "transformer_layer_0__self_attention_layer_concat_heads\n",
      "transformer_layer_0__self_attention_layer_heads_proj\n",
      "transformer_layer_0__feed_forward_network_layer2\n",
      "transformer_layer_0__self_attention_layer_values_chunks_1\n",
      "transformer_layer_0__self_attention_layer_values_chunks_3\n",
      "transformer_layer_0__feed_forward_network_layer1\n",
      "transformer_layer_0__self_attention_layer_values_chunks_2\n"
     ]
    }
   ],
   "source": [
    "for key in gpt_model_c.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "3efab6b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[595], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkey_transposed_global\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "key_transposed_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8a38027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SelfAttention:\n",
    "#     def __init__(self, self_attention_layer_c):\n",
    "#         W_query_weights =   self_attention_layer_c['W_Query']\n",
    "#         W_key_weights =     self_attention_layer_c['W_Key']\n",
    "#         W_value_weights =   self_attention_layer_c['W_Value']\n",
    "\n",
    "#         W_query_weights = W_query_weights.t()\n",
    "#         W_key_weights = W_key_weights.t()\n",
    "#         W_value_weights = W_value_weights.t()\n",
    "\n",
    "#         self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "#         self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "#         self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "#         self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "#         self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "#         self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "#         print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "#         print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "#         print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "#         print(\"===========================================\")\n",
    "\n",
    "#         query = self.W_query(x)\n",
    "#         key = self.W_key(x)\n",
    "#         value = self.W_value(x)\n",
    "        \n",
    "\n",
    "#         # print(f\"Query\\nShape: {query.shape}\\n{query}\")\n",
    "#         # print(f\"Key\\nShape:   {key.shape}\\n{key}\")\n",
    "#         # print(f\"Value\\nShape: {value.shape}\\n{value}\")\n",
    "#         # print(\"===========================================\")\n",
    "\n",
    "#         key_transposed = key.t()\n",
    "#         print(f\"Key transposed\\nShape: {key_transposed.shape}\\n{key_transposed}\")\n",
    "\n",
    "#         attention_scores = query @ key_transposed\n",
    "#         print(f\"Attention Scores\\nShape: {attention_scores.shape}\\n{attention_scores}\")\n",
    "\n",
    "#         attention_scores_scaled = attention_scores * 1/math.sqrt(key.shape[1])\n",
    "#         print(f\"Attention Scores Scaled\\nShape: {attention_scores_scaled.shape}\\n{attention_scores_scaled}\")\n",
    "\n",
    "#         attention_weights = F.softmax(attention_scores_scaled, dim=1)\n",
    "#         print(f\"Attention Weights\\nShape: {attention_weights.shape}\\n{attention_weights}\")\n",
    "\n",
    "#         context_vecs = attention_weights @ value\n",
    "#         print(f\"Context Vecs\\nShape: {context_vecs.shape}\\n{context_vecs}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "56a1f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_attention = SelfAttention(self_attention_layer_c)\n",
    "\n",
    "# self_attention.forward(input_embeddings_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eac5d4",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0b8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ae2e1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_attention = SelfAttentionMultiHead(self_attention_layer_c, 2)\n",
    "\n",
    "# self_attention.forward(input_embeddings_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefd18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
