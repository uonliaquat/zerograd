{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "eb770f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12b649",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "8cf141a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.0.attn.bias\n",
      "h.0.attn.c_attn.bias\n",
      "h.0.attn.c_attn.weight\n",
      "h.0.attn.c_proj.bias\n",
      "h.0.attn.c_proj.weight\n",
      "h.0.ln_1.bias\n",
      "h.0.ln_1.weight\n",
      "h.0.ln_2.bias\n",
      "h.0.ln_2.weight\n",
      "h.0.mlp.c_fc.bias\n",
      "h.0.mlp.c_fc.weight\n",
      "h.0.mlp.c_proj.bias\n",
      "h.0.mlp.c_proj.weight\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/Users/uonliaquat/Downloads/model.safetensors\"\n",
    "\n",
    "with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    h0_keys = [k for k in f.keys() if k.startswith(\"h.0\")]\n",
    "\n",
    "for key in h0_keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdb99f",
   "metadata": {},
   "source": [
    "## Read C Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "84265b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['pos_emb', 'token_emb']\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"/Users/uonliaquat/workspace/zerograd/c_model.safetensors\"\n",
    "\n",
    "output_c = {}\n",
    "\n",
    "with safe_open(filename, framework=\"pt\", device=\"cpu\") as f:\n",
    "    for key in f.keys():\n",
    "        output_c[key] = f.get_tensor(key)\n",
    "\n",
    "# Inspect\n",
    "print(len(output_c))\n",
    "print(list(output_c.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "9039e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, normalized_shape, weight, bias, eps=1e-5):\n",
    "#         super().__init__()\n",
    "#         self.normalized_shape = normalized_shape\n",
    "#         self.eps = eps\n",
    "\n",
    "#         # MUST be Parameters\n",
    "#         self.weight = nn.Parameter(weight.clone())\n",
    "#         self.bias = nn.Parameter(bias.clone())\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Compute mean & variance over last dim\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "#         x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "#         # Explicit reshape (matches PyTorch internals)\n",
    "#         return x_hat * self.weight.view(1, 1, -1) + self.bias.view(1, 1, -1)\n",
    "    \n",
    "\n",
    "# ln_ref = torch.nn.LayerNorm(768)\n",
    "# ln_custom = LayerNorm(\n",
    "#     768,\n",
    "#     ln_ref.weight.data,\n",
    "#     ln_ref.bias.data,\n",
    "#     ln_ref.eps\n",
    "# )\n",
    "\n",
    "# x = torch.randn(2, 5, 768)\n",
    "\n",
    "# print(torch.allclose(ln_ref(x), ln_custom(x), atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415effdc",
   "metadata": {},
   "source": [
    "## Python GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "1558cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape,\n",
    "        eps: float = 1e-5,\n",
    "        elementwise_affine: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = tuple(normalized_shape)\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "\n",
    "        if elementwise_affine:\n",
    "            self.weight = nn.Parameter(\n",
    "                torch.ones(self.normalized_shape, **factory_kwargs)\n",
    "            )\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.zeros(self.normalized_shape, **factory_kwargs)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter(\"weight\", None)\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x, out, h, l) -> torch.Tensor:\n",
    "        # Normalize over the last len(normalized_shape) dimensions\n",
    "        dims = tuple(range(-len(self.normalized_shape), 0))\n",
    "\n",
    "        mean = x.mean(dim=dims, keepdim=True)\n",
    "        #out[f'h.{h}.ln_1.mean'] = mean\n",
    "        var = x.var(dim=dims, keepdim=True, unbiased=False)\n",
    "        #out[f'h.{h}.ln_1.var'] = var\n",
    "        out[f'h.{h}.ln_{l}.mean_var'] = torch.cat([mean, var], dim=-1)\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        out[f'h.{h}.ln_{l}.x_norm'] = x_hat\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            x_hat = x_hat * self.weight\n",
    "            out[f'h.{h}.ln_{l}.x_norm_scaled'] = x_hat\n",
    "            x_hat = x_hat + self.bias\n",
    "            out[f'h.{h}.ln_{l}.x_norm_shifted'] = x_hat\n",
    "        return x_hat\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return (\n",
    "            f\"normalized_shape={self.normalized_shape}, \"\n",
    "            f\"eps={self.eps}, \"\n",
    "            f\"elementwise_affine={self.elementwise_affine}\"\n",
    "        )\n",
    "    \n",
    "\n",
    "class GPT2_Full_Debug(nn.Module):\n",
    "    def __init__(self, model_size=\"gpt2\", safetensors_path=None, device=\"cpu\", dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "\n",
    "        config = GPT2Config.from_pretrained(model_size)\n",
    "        self.model = GPT2Model(config).to(device=device, dtype=dtype)\n",
    "        state_dict = load_file(safetensors_path, device=device)\n",
    "        self.model.load_state_dict(state_dict, strict=False)\n",
    "        self.model.eval()\n",
    "        self.n_heads = config.n_head\n",
    "        self.hidden_size = config.n_embd\n",
    "        self.head_dim = self.hidden_size // self.n_heads\n",
    "        self.n_layers = config.n_layer\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "\n",
    "\n",
    "        # gpt2 = GPT2Model.from_pretrained(model_name)\n",
    "        # gpt2.eval()\n",
    "\n",
    "        # config = GPT2Config.from_pretrained(\"gpt2-xl\")\n",
    "        # model = GPT2Model(config)\n",
    "        # state_dict = load_file(\"/Users/uonliaquat/Downloads/gpt2-xl.safetensors\", device=\"cpu\")\n",
    "        # model.load_state_dict(state_dict)\n",
    "        # model.eval()\n",
    "\n",
    "        self.n_heads = config.n_head\n",
    "        self.hidden_size = config.n_embd\n",
    "        self.head_dim = self.hidden_size // self.n_heads\n",
    "        self.n_layers = config.n_layer\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        # Embeddings\n",
    "        self.wte = nn.Embedding.from_pretrained(self.model.wte.weight.detach().to(dtype), freeze=True)\n",
    "        self.wpe = nn.Embedding.from_pretrained(self.model.wpe.weight.detach().to(dtype), freeze=True)\n",
    "        print(self.model.wte.weight.detach().to(dtype))\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i, block in enumerate(self.model.h):\n",
    "            b = nn.ModuleDict({\n",
    "                \"ln_1\": LayerNorm(self.hidden_size, eps=block.ln_1.eps),\n",
    "                \"c_attn\": nn.Linear(self.hidden_size, 3 * self.hidden_size),\n",
    "                \"c_proj_attn\": nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                \"ln_2\": LayerNorm(self.hidden_size, eps=block.ln_2.eps),\n",
    "                \"c_fc\": nn.Linear(self.hidden_size, 4 * self.hidden_size),\n",
    "                \"c_proj_mlp\": nn.Linear(4 * self.hidden_size, self.hidden_size),\n",
    "            })\n",
    "            # Copy weights\n",
    "            b[\"ln_1\"].weight.data.copy_(block.ln_1.weight)\n",
    "            b[\"ln_1\"].bias.data.copy_(block.ln_1.bias)\n",
    "\n",
    "            b[\"c_attn\"].weight.data.copy_(block.attn.c_attn.weight.T)\n",
    "            b[\"c_attn\"].bias.data.copy_(block.attn.c_attn.bias)\n",
    "\n",
    "            b[\"c_proj_attn\"].weight.data.copy_(block.attn.c_proj.weight.T)\n",
    "            b[\"c_proj_attn\"].bias.data.copy_(block.attn.c_proj.bias)\n",
    "\n",
    "            b[\"ln_2\"].weight.data.copy_(block.ln_2.weight)\n",
    "            b[\"ln_2\"].bias.data.copy_(block.ln_2.bias)\n",
    "\n",
    "            b[\"c_fc\"].weight.data.copy_(block.mlp.c_fc.weight.T)\n",
    "            b[\"c_fc\"].bias.data.copy_(block.mlp.c_fc.bias)\n",
    "\n",
    "            b[\"c_proj_mlp\"].weight.data.copy_(block.mlp.c_proj.weight.T)\n",
    "            b[\"c_proj_mlp\"].bias.data.copy_(block.mlp.c_proj.bias)\n",
    "\n",
    "            self.blocks.append(b)\n",
    "\n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(self.hidden_size, eps=self.model.ln_f.eps)\n",
    "        self.ln_f.weight.data.copy_(self.model.ln_f.weight)\n",
    "        self.ln_f.bias.data.copy_(self.model.ln_f.bias)\n",
    "\n",
    "        # LM head (weight tied)\n",
    "        self.lm_head = nn.Linear(self.hidden_size, self.vocab_size, bias=False)\n",
    "        self.lm_head.weight.data.copy_(self.wte.weight)\n",
    "\n",
    "        self.to(device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        bsz, seq_len = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        out = {}\n",
    "\n",
    "        # Embeddings\n",
    "        pos_ids = torch.arange(seq_len, device=device).unsqueeze(0)\n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(pos_ids)\n",
    "        x = tok_emb + pos_emb\n",
    "        out[\"token_emb\"] = tok_emb\n",
    "        out[\"pos_emb\"] = pos_emb\n",
    "\n",
    "        # Iterate over blocks\n",
    "        for layer_idx, b in enumerate(self.blocks):\n",
    "            #layer_out = {}\n",
    "            out[f\"h.{layer_idx}.input_embedding\"] = x\n",
    "\n",
    "            # LN1\n",
    "            x_ln1 = b[\"ln_1\"](x, out, layer_idx, 1)\n",
    "            out[f\"h.{layer_idx}.ln_1\"] = x_ln1\n",
    "\n",
    "            # QKV\n",
    "            qkv = b[\"c_attn\"](x_ln1)\n",
    "            q, k, v = qkv.split(self.hidden_size, dim=2)\n",
    "            out[f\"h.{layer_idx}.qkv\"] = qkv\n",
    "            out[f\"h.{layer_idx}.attn.q\"] = q\n",
    "            out[f\"h.{layer_idx}.attn.k\"] = k\n",
    "            out[f\"h.{layer_idx}.attn.v\"] = v\n",
    "\n",
    "            # Split heads\n",
    "            def split_heads(x):\n",
    "                return x.view(bsz, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "            qh = split_heads(q)[0]\n",
    "            kh = split_heads(k)[0]\n",
    "            vh = split_heads(v)[0]\n",
    "\n",
    "            qh_list = torch.split(qh, 1, dim=0)  # split into chunks of size 1\n",
    "            kh_list = torch.split(kh, 1, dim=0)  # split into chunks of size 1\n",
    "            vh_list = torch.split(vh, 1, dim=0)  # split into chunks of size 1 \n",
    "            \n",
    "            for i, (q, k, v) in enumerate(zip(qh_list, kh_list, vh_list)):\n",
    "\n",
    "                out[f\"h.{layer_idx}.attn.{i}.q_head\"] = q\n",
    "                out[f\"h.{layer_idx}.attn.{i}.k_head\"] = k\n",
    "                out[f\"h.{layer_idx}.attn.{i}.v_head\"] = v\n",
    "\n",
    "            # Attention\n",
    "            kh_t = kh.transpose(-2, -1)\n",
    "            qk = torch.matmul(qh, kh_t)\n",
    "            qk_scaled = qk / math.sqrt(self.head_dim)\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).view(1, 1, seq_len, seq_len)\n",
    "            qk_masked = qk_scaled.masked_fill(causal_mask == 0, float(\"-inf\"))\n",
    "            attn_probs = torch.softmax(qk_masked, dim=-1)\n",
    "            attn_ctx_heads = torch.matmul(attn_probs, vh)\n",
    "            attn_ctx = attn_ctx_heads.transpose(1, 2).contiguous().view(bsz, seq_len, self.hidden_size)\n",
    "            attn_out = b[\"c_proj_attn\"](attn_ctx)\n",
    "\n",
    "            out[f\"h.{layer_idx}.attn.c_proj\"] = attn_out\n",
    "\n",
    "            #resid 1\n",
    "            x = x + attn_out\n",
    "            out[f\"h.{layer_idx}.resid.1\"] = x\n",
    "\n",
    "            # LN2 + MLP\n",
    "            x_ln2 = b[\"ln_2\"](x, out, layer_idx, 2)\n",
    "            mlp_out = b[\"c_proj_mlp\"](torch.nn.functional.gelu(b[\"c_fc\"](x_ln2)))\n",
    "            out[f\"h.{layer_idx}.mlp\"] = mlp_out\n",
    "\n",
    "            # resid 2\n",
    "            x = x + mlp_out\n",
    "            out[f\"h.{layer_idx}.resid.2\"] = x\n",
    "\n",
    "            # out[f\"h.{layer_idx}.ln_2\"] = x_ln2\n",
    "            # out[f\"h.{layer_idx}.mlp_fc_out\"] = mlp_out\n",
    "            # out[f\"h.{layer_idx}.block_output\"] = x\n",
    "\n",
    "            #out[f\"layer_{layer_idx}\"] = layer_out\n",
    "\n",
    "        # Final layer norm\n",
    "        x_final_ln = self.ln_f(x)\n",
    "        out[\"ln_f\"] = x_final_ln\n",
    "\n",
    "        # LM head\n",
    "        logits = self.lm_head(x_final_ln)\n",
    "        out[\"head\"] = logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        # out[\"logits\"] = logits\n",
    "        out[\"probs\"] = probs\n",
    "\n",
    "        # Greedy next token\n",
    "        next_token_prob_dist = probs[:, -1, :]\n",
    "        out['next_token_prob_dist'] = next_token_prob_dist\n",
    "        next_token_id = torch.argmax(next_token_prob_dist, dim=-1)\n",
    "        out[\"next_token_id\"] = next_token_id\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "e75ab983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.02, -0.02,  0.02,  ..., -0.01,  0.01, -0.04],\n",
      "        [-0.02,  0.02,  0.01,  ..., -0.02,  0.03, -0.02],\n",
      "        [ 0.06,  0.05,  0.05,  ...,  0.00,  0.04, -0.01],\n",
      "        ...,\n",
      "        [ 0.07, -0.08, -0.01,  ...,  0.04, -0.00, -0.04],\n",
      "        [-0.01, -0.04, -0.00,  ...,  0.04,  0.01, -0.02],\n",
      "        [ 0.00, -0.00, -0.08,  ..., -0.01,  0.06,  0.12]], dtype=torch.float32)\n",
      "tensor([[8001, 9542, 4430,  287, 1160, 2075,  318]])\n",
      "tensor([[9542, 4430,  287, 1160, 2075,  318, 1016]])\n",
      "tensor([[4430,  287, 1160, 2075,  318, 1016,  284]])\n",
      "tensor([[ 287, 1160, 2075,  318, 1016,  284,  307]])\n",
      "tensor([[1160, 2075,  318, 1016,  284,  307,  257]])\n",
      "Artificial intelligence in 2026 is going to be a big\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "paragraph = (\"Artificial intelligence in 2026 is\")\n",
    "\n",
    "model_size = 'gpt2-xl'\n",
    "model_path = \"/Users/uonliaquat/Downloads/gpt2-xl.safetensors\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_size)\n",
    "input_ids = tokenizer.encode(paragraph, return_tensors=\"pt\").to(device)\n",
    "#print(\"token_ids: \", input_ids)\n",
    "\n",
    "\n",
    "context_len = 7\n",
    "if input_ids.shape[1] > context_len:\n",
    "    input_ids = input_ids[:, -context_len:]  # keep last 52 tokens\n",
    "\n",
    "cur_len = input_ids.shape[1]\n",
    "\n",
    "model = GPT2_Full_Debug(\n",
    "    safetensors_path=model_path,\n",
    "    model_size=model_size,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "num_generate = 5  # number of new tokens\n",
    "generated_ids = input_ids[0].tolist()  # start with initial context\n",
    "\n",
    "# Store intermediate outputs for all steps\n",
    "all_steps_outputs = []\n",
    "\n",
    "# -------------------------------\n",
    "# Generation loop\n",
    "# -------------------------------\n",
    "for step in range(num_generate):\n",
    "    cur_input = torch.tensor([generated_ids[-context_len:]], device=device)\n",
    "    print(cur_input)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(cur_input)\n",
    "\n",
    "    # Save intermediate outputs for this step\n",
    "    all_steps_outputs.append(outputs)\n",
    "\n",
    "    # Get next token (greedy)\n",
    "    next_token_id = outputs['next_token_id'].item()\n",
    "\n",
    "    # Append to generated list\n",
    "    generated_ids.append(next_token_id)\n",
    "\n",
    "# -------------------------------\n",
    "# Decode full generated sequence\n",
    "# -------------------------------\n",
    "full_text = tokenizer.decode(generated_ids)\n",
    "#print(\"\\n=== Full generated text (initial + 50 new tokens) ===\\n\")\n",
    "print(full_text)\n",
    "\n",
    "# -------------------------------\n",
    "# Optional: print next token for first 5 steps\n",
    "# -------------------------------\n",
    "# print(\"\\n=== First 5 generated token IDs and strings ===\")\n",
    "# for step_out in all_steps_outputs:\n",
    "#     tid = step_out['next_token_id'].item()\n",
    "#     print(tid, tokenizer.decode([tid]))\n",
    "\n",
    "output_p = all_steps_outputs[0]\n",
    "# list(output_p.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "0b36fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'h.0.input_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[717], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(output_p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_emb\u001b[39m\u001b[38;5;124m'\u001b[39m],  output_c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_emb\u001b[39m\u001b[38;5;124m'\u001b[39m], atol\u001b[38;5;241m=\u001b[39matol))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(output_p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_emb\u001b[39m\u001b[38;5;124m'\u001b[39m],  output_c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_emb\u001b[39m\u001b[38;5;124m'\u001b[39m], atol\u001b[38;5;241m=\u001b[39matol))\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(output_p[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.input_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43moutput_c\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mh\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.input_embedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, atol\u001b[38;5;241m=\u001b[39matol))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(output_p[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln_1.mean_var\u001b[39m\u001b[38;5;124m'\u001b[39m], output_c[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln_1.mean_var\u001b[39m\u001b[38;5;124m'\u001b[39m], atol\u001b[38;5;241m=\u001b[39matol))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mallclose(output_p[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln_1.x_norm\u001b[39m\u001b[38;5;124m'\u001b[39m], output_c[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ln_1.x_norm\u001b[39m\u001b[38;5;124m'\u001b[39m], atol\u001b[38;5;241m=\u001b[39matol))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'h.0.input_embedding'"
     ]
    }
   ],
   "source": [
    "atol = 1e-4\n",
    "h = 0\n",
    "for h in range(0, 12):\n",
    "    print(torch.allclose(output_p['token_emb'],  output_c['token_emb'], atol=atol))\n",
    "    print(torch.allclose(output_p['pos_emb'],  output_c['pos_emb'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.input_embedding'], output_c[f'h.{h}.input_embedding'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1.mean_var'], output_c[f'h.{h}.ln_1.mean_var'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1.x_norm'], output_c[f'h.{h}.ln_1.x_norm'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1.x_norm_shifted'], output_c[f'h.{h}.ln_1.x_norm_shifted'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1.x_norm_scaled'], output_c[f'h.{h}.ln_1.x_norm_scaled'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1'], output_c[f'h.{h}.ln_1'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.attn.q'], output_c[f'h.{h}.attn.q'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.attn.k'], output_c[f'h.{h}.attn.k'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.attn.v'], output_c[f'h.{h}.attn.v'], atol=atol))\n",
    "\n",
    "\n",
    "    for i in range(0, 12):\n",
    "        print(torch.allclose(output_p[f\"h.{h}.attn.{i}.q_head\"], output_c[f\"h.{h}.attn.{i}.q_head\"], atol=atol))\n",
    "        print(torch.allclose(output_p[f\"h.{h}.attn.{i}.k_head\"], output_c[f\"h.{h}.attn.{i}.k_head\"], atol=atol))\n",
    "        print(torch.allclose(output_p[f\"h.{h}.attn.{i}.v_head\"], output_c[f\"h.{h}.attn.{i}.v_head\"], atol=atol))\n",
    "\n",
    "\n",
    "    #c_proj\n",
    "    print(torch.allclose(output_p[f\"h.{0}.attn.c_proj\"], output_c[f\"h.{0}.attn.c_proj\"], atol=atol))\n",
    "\n",
    "    #residual connection 1\n",
    "    print(torch.allclose(output_p[f\"h.{0}.resid.1\"], output_c[f\"h.{0}.resid.1\"], atol=atol))\n",
    "\n",
    "    # ln_2\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_2.mean_var'], output_c[f'h.{h}.ln_2.mean_var'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_2.x_norm'], output_c[f'h.{h}.ln_2.x_norm'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_2.x_norm_shifted'], output_c[f'h.{h}.ln_2.x_norm_shifted'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_2.x_norm_scaled'], output_c[f'h.{h}.ln_2.x_norm_scaled'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.mlp'], output_c[f'h.{h}.mlp'], atol=atol))\n",
    "\n",
    "    #residual connection 2\n",
    "    print(torch.allclose(output_p[f\"h.{0}.resid.2\"], output_c[f\"h.{0}.resid.2\"], atol=atol))\n",
    "\n",
    "\n",
    "print(torch.allclose(output_p[f\"ln_f\"], output_c[f\"ln_f\"], atol=atol))\n",
    "print(torch.allclose(output_p[f\"head\"], output_c[f\"head\"], atol=atol))\n",
    "print(torch.allclose(output_p[f\"probs\"], output_c[f\"probs\"], atol=atol))\n",
    "print(torch.allclose(output_p[f\"next_token_prob_dist\"], output_c[f\"next_token_prob_dist\"], atol=atol))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "3c1963fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.01, -0.03, -0.08,  ...,  0.01, -0.09,  0.00],\n",
       "         [-0.01,  0.08, -0.05,  ...,  0.05, -0.04,  0.01],\n",
       "         [ 0.04, -0.05,  0.01,  ...,  0.02, -0.00,  0.06],\n",
       "         ...,\n",
       "         [ 0.03, -0.05,  0.03,  ..., -0.01, -0.02,  0.03],\n",
       "         [ 0.04, -0.02, -0.06,  ...,  0.00, -0.08,  0.05],\n",
       "         [-0.03, -0.00, -0.00,  ..., -0.02, -0.02, -0.01]]],\n",
       "       dtype=torch.float32)"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_p['token_emb']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae780c4",
   "metadata": {},
   "source": [
    "## Gnerating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "288a78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.01,  0.00,  0.02,  ..., -0.03,  0.10,  0.02],\n",
       "         [-0.02, -0.03, -0.02,  ...,  0.03,  0.07,  0.02],\n",
       "         [ 0.02, -0.02,  0.03,  ..., -0.05,  0.06, -0.06],\n",
       "         ...,\n",
       "         [-0.02,  0.04, -0.01,  ...,  0.02, -0.04,  0.01],\n",
       "         [-0.00, -0.01, -0.00,  ..., -0.04, -0.00, -0.05],\n",
       "         [ 0.03, -0.04,  0.03,  ..., -0.01,  0.04, -0.01]]],\n",
       "       dtype=torch.float32)"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_c['token_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(list(range(tokenizer.vocab_size)))\n",
    "\n",
    "# Choose a separator (must not appear in any token)\n",
    "SEP = \"|\"  # safe ASCII separator\n",
    "\n",
    "def clean_token(tok):\n",
    "    tok = tok.replace('Ġ', ' ')   # space marker\n",
    "    tok = tok.replace('Ċ', '<NL>')  # newline marker as visible string\n",
    "    # Optional: replace any other non-ASCII sequences\n",
    "    tok = ''.join(c if ord(c) < 128 else '<U>' for c in tok)\n",
    "    return tok\n",
    "\n",
    "with open(\"gpt2_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(SEP.join(clean_token(tok) for tok in tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a114a",
   "metadata": {},
   "source": [
    "## GPT Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "fa80cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_context: 7\n",
      "tensor([[8001, 9542, 4430,  287, 1160, 2075,  318]])\n",
      "Input IDs shape: torch.Size([1, 7])\n",
      "\n",
      "\n",
      " Artificial intelligence in 2026 is a new field of research in which the size of a feature set is increased or decreased with each passing year. This is because of the way in which the brain processes information.\n",
      "\n",
      "The researchers used a computer program called DeepMind to create a neural\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# GPT-2 has no pad token by default, use eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pad_token_id = tokenizer.pad_token_id  # now this is 50256\n",
    "\n",
    "prompt = \"Artificial intelligence in 2026 is\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "max_context = len(input_ids[0])\n",
    "print(f\"max_context: {max_context}\")\n",
    "print(input_ids)\n",
    "\n",
    "if input_ids.shape[1] > max_context:\n",
    "    input_ids = input_ids[:, -max_context:]\n",
    "elif input_ids.shape[1] < max_context:\n",
    "    pad_length = max_context - input_ids.shape[1]\n",
    "    pad_ids = torch.full((1, pad_length), pad_token_id, device=device)\n",
    "    input_ids = torch.cat([pad_ids, input_ids], dim=1)\n",
    "\n",
    "print(\"Input IDs shape:\", input_ids.shape)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=max_context + 50,  # generate 50 new tokens after context\n",
    "    do_sample=False              # greedy\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print('\\n\\n', generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "753734c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([318])"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a1f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
