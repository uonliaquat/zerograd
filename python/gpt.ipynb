{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "eb770f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7586e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensors(filename):\n",
    "    tensors_dict = {}\n",
    "    \n",
    "    # Tracking current tensor state\n",
    "    current_name = \"default_tensor\"\n",
    "    current_metadata = {}\n",
    "    current_data = []\n",
    "    \n",
    "    meta_keys = ['size', 'ndim', 'shape', 'stride', 'elem_size', 'requires_grad']\n",
    "\n",
    "    def finalize_tensor(name, meta, data):\n",
    "        \"\"\"Helper to reshape data and store in the dictionary.\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "        \n",
    "        target_shape = [int(s) for s in meta.get('shape', [len(data)])]\n",
    "        # Using float64 (double) as your images show high precision decimals\n",
    "        tensors_dict[name] = torch.tensor(data, dtype=torch.float32).reshape(target_shape)\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split by comma and remove empty strings/whitespace\n",
    "            parts = [p.strip() for p in line.split(',') if p.strip()]\n",
    "            \n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            label = parts[0].replace(':', '')\n",
    "\n",
    "            # 1. Check if it's a known metadata key\n",
    "            if label in meta_keys:\n",
    "                vals = [float(v) for v in parts[1:]]\n",
    "                current_metadata[label] = vals[0] if len(vals) == 1 else vals\n",
    "            \n",
    "            else:\n",
    "                try:\n",
    "                    # 2. Try to parse as numeric data\n",
    "                    row_data = [float(p) for p in parts]\n",
    "                    current_data.extend(row_data)\n",
    "                except ValueError:\n",
    "                    # 3. If it's a string but NOT metadata, it's a new tensor name\n",
    "                    # Save the previous tensor first\n",
    "                    if current_data:\n",
    "                        finalize_tensor(current_name, current_metadata, current_data)\n",
    "                    \n",
    "                    # Reset for the new tensor\n",
    "                    current_name = label\n",
    "                    current_metadata = {}\n",
    "                    current_data = []\n",
    "\n",
    "    # Finalize the last tensor in the file\n",
    "    finalize_tensor(current_name, current_metadata, current_data)\n",
    "    \n",
    "    return tensors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beeeb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = '../tensors'\n",
    "# input_tokens = load_tensors(f'{base_path}/input_tokens.csv')['default_tensor'].long()\n",
    "# token_embeddings_layer = load_tensors(f'{base_path}/gpt_model.token_embed_layer.csv')\n",
    "# pos_embeddings_layer = load_tensors(f'{base_path}/gpt_model.pos_embed_layer.csv')\n",
    "# pos_embeddings_layer = load_tensors(f'{base_path}/gpt_model.workspace.position_indicies.csv')\n",
    "# input_embeddings = load_tensors(f'{base_path}/gpt_model.input_embeddings.csv')['default_tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3f8fdb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': tensor([[[1, 2, 0, 2, 5, 3],\n",
       "          [0, 0, 4, 3, 2, 1]]]),\n",
       " 'token_embeddings_layer': {'Weights': tensor([[-1.00, -0.74,  0.51, -0.08,  0.07, -0.56],\n",
       "          [-0.91,  0.36,  0.36,  0.87, -0.23,  0.04],\n",
       "          [ 0.66, -0.93, -0.89,  0.06,  0.34, -0.98],\n",
       "          [-0.23, -0.87, -0.17,  0.37,  0.18,  0.86],\n",
       "          [ 0.69,  0.05, -0.82,  0.31, -0.17,  0.40],\n",
       "          [ 0.82,  0.52, -0.48, -0.91,  0.47, -0.34]]),\n",
       "  'Output': tensor([[[-0.91,  0.36,  0.36,  0.87, -0.23,  0.04],\n",
       "           [ 0.66, -0.93, -0.89,  0.06,  0.34, -0.98],\n",
       "           [-1.00, -0.74,  0.51, -0.08,  0.07, -0.56],\n",
       "           [ 0.66, -0.93, -0.89,  0.06,  0.34, -0.98],\n",
       "           [ 0.82,  0.52, -0.48, -0.91,  0.47, -0.34],\n",
       "           [-0.23, -0.87, -0.17,  0.37,  0.18,  0.86]],\n",
       "  \n",
       "          [[-1.00, -0.74,  0.51, -0.08,  0.07, -0.56],\n",
       "           [-1.00, -0.74,  0.51, -0.08,  0.07, -0.56],\n",
       "           [ 0.69,  0.05, -0.82,  0.31, -0.17,  0.40],\n",
       "           [-0.23, -0.87, -0.17,  0.37,  0.18,  0.86],\n",
       "           [ 0.66, -0.93, -0.89,  0.06,  0.34, -0.98],\n",
       "           [-0.91,  0.36,  0.36,  0.87, -0.23,  0.04]]])},\n",
       " 'pos_embeddings_layer': {'Weights': tensor([[ 0.27,  0.51,  0.98, -0.27, -0.51,  0.97],\n",
       "          [ 0.45,  0.51,  0.30, -0.85,  0.26,  0.77],\n",
       "          [-0.45, -0.13,  0.53, -0.04, -0.52, -0.45],\n",
       "          [-0.28, -0.67, -0.03,  0.80,  0.82, -0.88],\n",
       "          [ 0.81,  0.01,  0.03, -0.36,  0.97, -0.01],\n",
       "          [-0.47, -0.82,  0.90, -0.85,  0.00, -0.23]]),\n",
       "  'Output': tensor([[[ 0.27,  0.51,  0.98, -0.27, -0.51,  0.97],\n",
       "           [ 0.45,  0.51,  0.30, -0.85,  0.26,  0.77],\n",
       "           [-0.45, -0.13,  0.53, -0.04, -0.52, -0.45],\n",
       "           [-0.28, -0.67, -0.03,  0.80,  0.82, -0.88],\n",
       "           [ 0.81,  0.01,  0.03, -0.36,  0.97, -0.01],\n",
       "           [-0.47, -0.82,  0.90, -0.85,  0.00, -0.23]],\n",
       "  \n",
       "          [[ 0.27,  0.51,  0.98, -0.27, -0.51,  0.97],\n",
       "           [ 0.45,  0.51,  0.30, -0.85,  0.26,  0.77],\n",
       "           [-0.45, -0.13,  0.53, -0.04, -0.52, -0.45],\n",
       "           [-0.28, -0.67, -0.03,  0.80,  0.82, -0.88],\n",
       "           [ 0.81,  0.01,  0.03, -0.36,  0.97, -0.01],\n",
       "           [-0.47, -0.82,  0.90, -0.85,  0.00, -0.23]]])},\n",
       " 'position_indicies': tensor([[[0, 1, 2, 3, 4, 5],\n",
       "          [0, 1, 2, 3, 4, 5]]]),\n",
       " 'input_embeddings': tensor([[[-0.64,  0.87,  1.34,  0.60, -0.74,  1.00],\n",
       "          [ 1.11, -0.42, -0.59, -0.80,  0.61, -0.22],\n",
       "          [-1.45, -0.86,  1.04, -0.13, -0.46, -1.01],\n",
       "          [ 0.38, -1.60, -0.92,  0.85,  1.16, -1.86],\n",
       "          [ 1.63,  0.53, -0.44, -1.27,  1.45, -0.36],\n",
       "          [-0.70, -1.68,  0.73, -0.48,  0.18,  0.63]],\n",
       " \n",
       "         [[-0.73, -0.22,  1.49, -0.35, -0.44,  0.40],\n",
       "          [-0.55, -0.23,  0.81, -0.94,  0.33,  0.21],\n",
       "          [ 0.24, -0.07, -0.28,  0.26, -0.69, -0.05],\n",
       "          [-0.51, -1.53, -0.19,  1.17,  1.00, -0.02],\n",
       "          [ 1.47, -0.92, -0.86, -0.30,  1.32, -1.00],\n",
       "          [-1.37, -0.46,  1.25,  0.02, -0.23, -0.19]]])}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = '../tensors'\n",
    "gpt_model_c = {\n",
    "    \"input_tokens\":  load_tensors(f'{base_path}/input_tokens.csv')['default_tensor'].long(),\n",
    "    \"token_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.token_embed_layer.csv'),\n",
    "    \"pos_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.pos_embed_layer.csv'),\n",
    "    \"position_indicies\":  load_tensors(f'{base_path}/gpt_model.workspace.position_indicies.csv')['default_tensor'].long(),\n",
    "    \"input_embeddings\": load_tensors(f'{base_path}/gpt_model.input_embeddings.csv')['default_tensor']\n",
    "}\n",
    "gpt_model_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "34901992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5]]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model_c['position_indicies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b98d9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_within_tolerance(a, b, atol):\n",
    "    if a.shape != b.shape:\n",
    "        print(\"Shape mismatch:\", a.shape, b.shape)\n",
    "        return False\n",
    "\n",
    "    max_diff = round((a - b).abs().max().item(), 2)\n",
    "    print(\"max |diff|:\", max_diff)\n",
    "\n",
    "    return max_diff <= atol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12b649",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d47f7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, gpt_model_c):\n",
    "        super().__init__()      \n",
    "        num_token_embeddings            = gpt_model_c['token_embeddings_layer']['Weights'].shape[0]\n",
    "        token_embedding_dim             = gpt_model_c['token_embeddings_layer']['Weights'].shape[1]\n",
    "        num_pos_embeddings              = gpt_model_c['pos_embeddings_layer']['Weights'].shape[0]\n",
    "        pos_embedding_dim               = gpt_model_c['pos_embeddings_layer']['Weights'].shape[1]\n",
    "\n",
    "        self.token_embeddings_layer     = nn.Embedding(num_embeddings=num_token_embeddings, embedding_dim=token_embedding_dim)\n",
    "        self.pos_embeddings_layer       = nn.Embedding(num_embeddings=num_pos_embeddings, embedding_dim=pos_embedding_dim)\n",
    "\n",
    "        self.token_embeddings_layer.weight.data.copy_(gpt_model_c['token_embeddings_layer']['Weights'])\n",
    "        self.pos_embeddings_layer.weight.data.copy_(gpt_model_c['pos_embeddings_layer']['Weights'])\n",
    "        \n",
    "        assert(gpt_model_c['token_embeddings_layer']['Weights'].shape == self.token_embeddings_layer.weight.shape)\n",
    "        assert(gpt_model_c['token_embeddings_layer']['Weights'].shape == self.pos_embeddings_layer.weight.shape)\n",
    "\n",
    "        \n",
    "        self.token_embeddings_c         = gpt_model_c['token_embeddings_layer']['Output']\n",
    "        self.pos_embeddings_c           = gpt_model_c['pos_embeddings_layer']['Output']\n",
    "        self.input_embeddings_c         = gpt_model_c['input_embeddings']\n",
    "        self.position_indicies_c        = gpt_model_c['position_indicies']\n",
    "\n",
    "        self.atol = 0.02\n",
    "\n",
    "    def forward(self, input_tokens_c):\n",
    "        print(f\"x.shape:    {input_tokens_c.shape}\")\n",
    "        token_embeddings    = self.token_embeddings_layer(input_tokens_c)[0]\n",
    "        pos_embeddings      = self.pos_embeddings_layer(self.position_indicies_c)[0]\n",
    "        input_embeddings    = token_embeddings + pos_embeddings\n",
    "\n",
    "        print(f\"token_embeddings.shape: {token_embeddings.shape}\")\n",
    "        print(f\"pos_embeddings.shape:   {pos_embeddings.shape}\")\n",
    "        print(f\"input_embeddings.shape: {input_embeddings.shape}\")\n",
    "\n",
    "        token_embeddings_matched    = tensors_within_tolerance(token_embeddings,    self.token_embeddings_c,    self.atol)\n",
    "        pos_embeddings_matched      = tensors_within_tolerance(pos_embeddings,      self.pos_embeddings_c,      self.atol)\n",
    "        input_embeddings_matched    = tensors_within_tolerance(input_embeddings,    self.input_embeddings_c,    self.atol)\n",
    "\n",
    "        print(f\"token_embeddings_matched:   {token_embeddings_matched}\")\n",
    "        print(f\"pos_embeddings_matched:     {pos_embeddings_matched}\")\n",
    "        print(f\"input_embeddings_matched:   {input_embeddings_matched}\")\n",
    "\n",
    "        assert(\n",
    "            token_embeddings_matched and \n",
    "            pos_embeddings_matched and \n",
    "            input_embeddings_matched\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        # print(token_embeddings - self.token_embed_layer_output_c)\n",
    "        return input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2bd660d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:    torch.Size([1, 2, 6])\n",
      "token_embeddings.shape: torch.Size([2, 6, 6])\n",
      "pos_embeddings.shape:   torch.Size([2, 6, 6])\n",
      "input_embeddings.shape: torch.Size([2, 6, 6])\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.01\n",
      "token_embeddings_matched:   True\n",
      "pos_embeddings_matched:     True\n",
      "input_embeddings_matched:   True\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT(gpt_model_c=gpt_model_c)\n",
    "input_embeddings = gpt(gpt_model_c['input_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef5fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8a38027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention:\n",
    "    def __init__(self, self_attention_layer_c):\n",
    "        W_query_weights =   self_attention_layer_c['W_Query']\n",
    "        W_key_weights =     self_attention_layer_c['W_Key']\n",
    "        W_value_weights =   self_attention_layer_c['W_Value']\n",
    "\n",
    "        W_query_weights = W_query_weights.t()\n",
    "        W_key_weights = W_key_weights.t()\n",
    "        W_value_weights = W_value_weights.t()\n",
    "\n",
    "        self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "        self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "        self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "        self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "        self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "        self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "        print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "        print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "        print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "        print(\"===========================================\")\n",
    "\n",
    "        query = self.W_query(x)\n",
    "        key = self.W_key(x)\n",
    "        value = self.W_value(x)\n",
    "        \n",
    "\n",
    "        # print(f\"Query\\nShape: {query.shape}\\n{query}\")\n",
    "        # print(f\"Key\\nShape:   {key.shape}\\n{key}\")\n",
    "        # print(f\"Value\\nShape: {value.shape}\\n{value}\")\n",
    "        # print(\"===========================================\")\n",
    "\n",
    "        key_transposed = key.t()\n",
    "        print(f\"Key transposed\\nShape: {key_transposed.shape}\\n{key_transposed}\")\n",
    "\n",
    "        attention_scores = query @ key_transposed\n",
    "        print(f\"Attention Scores\\nShape: {attention_scores.shape}\\n{attention_scores}\")\n",
    "\n",
    "        attention_scores_scaled = attention_scores * 1/math.sqrt(key.shape[1])\n",
    "        print(f\"Attention Scores Scaled\\nShape: {attention_scores_scaled.shape}\\n{attention_scores_scaled}\")\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores_scaled, dim=1)\n",
    "        print(f\"Attention Weights\\nShape: {attention_weights.shape}\\n{attention_weights}\")\n",
    "\n",
    "        context_vecs = attention_weights @ value\n",
    "        print(f\"Context Vecs\\nShape: {context_vecs.shape}\\n{context_vecs}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "56a1f051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings: \n",
      "Shape: torch.Size([6, 4])\n",
      "tensor([[-1.00, -0.74,  0.51, -0.08],\n",
      "        [ 0.07, -0.56, -0.91,  0.36],\n",
      "        [ 0.36,  0.87, -0.23,  0.04],\n",
      "        [ 0.66, -0.93, -0.89,  0.06],\n",
      "        [ 0.34, -0.98, -0.23, -0.87],\n",
      "        [-0.17,  0.37,  0.18,  0.86]])\n",
      "W_Query\n",
      "Shape: torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.69, -0.17, -0.48,  0.27],\n",
      "        [ 0.05,  0.40, -0.91,  0.51],\n",
      "        [-0.82,  0.82,  0.47,  0.98],\n",
      "        [ 0.31,  0.52, -0.34, -0.27]], requires_grad=True)\n",
      "W_Key\n",
      "Shape:   torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.51,  0.30, -0.45, -0.52],\n",
      "        [ 0.97, -0.85, -0.13, -0.45],\n",
      "        [ 0.45,  0.26,  0.53, -0.28],\n",
      "        [ 0.51,  0.77, -0.04, -0.67]], requires_grad=True)\n",
      "W_Value\n",
      "Shape: torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.03,  0.81,  0.97,  0.90],\n",
      "        [ 0.80,  0.01, -0.01, -0.85],\n",
      "        [ 0.82,  0.03, -0.47,  0.00],\n",
      "        [-0.88, -0.36, -0.82, -0.23]], requires_grad=True)\n",
      "===========================================\n",
      "Key transposed\n",
      "Shape: torch.Size([4, 6])\n",
      "tensor([[ 0.10,  0.02,  0.16, -0.25,  0.09, -0.33],\n",
      "        [-0.37,  0.50, -0.38,  1.52,  1.58, -0.89],\n",
      "        [-0.35, -0.70,  0.26, -0.43,  0.02, -0.13],\n",
      "        [-1.05, -0.60,  0.84, -0.38,  0.01, -0.39]], grad_fn=<TBackward0>)\n",
      "Attention Scores\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[ 0.99, -0.19, -0.42, -0.93, -1.42,  1.31],\n",
      "        [ 0.04,  0.86, -0.39,  1.31,  1.30, -0.83],\n",
      "        [-0.98, -0.32,  0.43,  0.46,  0.98, -0.89],\n",
      "        [ 0.49,  1.43, -0.44,  1.22,  0.85, -0.59],\n",
      "        [ 1.06,  1.18, -0.32, -0.07, -0.98,  0.74],\n",
      "        [-0.48, -0.66,  0.06,  0.10,  0.68, -0.47]], grad_fn=<MmBackward0>)\n",
      "Attention Scores Scaled\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[ 0.49, -0.10, -0.21, -0.46, -0.71,  0.66],\n",
      "        [ 0.02,  0.43, -0.19,  0.66,  0.65, -0.42],\n",
      "        [-0.49, -0.16,  0.21,  0.23,  0.49, -0.45],\n",
      "        [ 0.25,  0.71, -0.22,  0.61,  0.43, -0.29],\n",
      "        [ 0.53,  0.59, -0.16, -0.04, -0.49,  0.37],\n",
      "        [-0.24, -0.33,  0.03,  0.05,  0.34, -0.24]], grad_fn=<DivBackward0>)\n",
      "Attention Weights\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[0.26, 0.14, 0.13, 0.10, 0.08, 0.30],\n",
      "        [0.13, 0.19, 0.10, 0.24, 0.24, 0.08],\n",
      "        [0.10, 0.14, 0.20, 0.20, 0.26, 0.10],\n",
      "        [0.16, 0.25, 0.10, 0.22, 0.19, 0.09],\n",
      "        [0.23, 0.24, 0.12, 0.13, 0.08, 0.20],\n",
      "        [0.14, 0.12, 0.18, 0.18, 0.24, 0.14]], grad_fn=<SoftmaxBackward0>)\n",
      "Context Vecs\n",
      "Shape: torch.Size([6, 4])\n",
      "tensor([[-0.03, -0.33, -0.10,  0.23],\n",
      "        [-0.89,  0.17,  0.29,  0.40],\n",
      "        [-0.72,  0.22,  0.30,  0.27],\n",
      "        [-0.80,  0.06,  0.25,  0.43],\n",
      "        [-0.33, -0.23,  0.02,  0.35],\n",
      "        [-0.61,  0.13,  0.21,  0.27]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "self_attention = SelfAttention(self_attention_layer_c)\n",
    "\n",
    "self_attention.forward(input_embeddings_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eac5d4",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3e0b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, self_attention_layer_c, num_heads):\n",
    "        super().__init__()\n",
    "        W_query_weights =   self_attention_layer_c['W_Query']\n",
    "        W_key_weights =     self_attention_layer_c['W_Key']\n",
    "        W_value_weights =   self_attention_layer_c['W_Value']\n",
    "        head_proj_weights =   self_attention_layer_c['heads_proj']\n",
    "\n",
    "        W_query_weights = W_query_weights.t()\n",
    "        W_key_weights = W_key_weights.t()\n",
    "        W_value_weights = W_value_weights.t()\n",
    "        head_proj_weights = head_proj_weights.t()\n",
    "\n",
    "        self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "        self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "        self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "        self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "        self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "        self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "\n",
    "        print(head_proj_weights.shape[0], head_proj_weights.shape[1])\n",
    "        self.heads_proj = nn.Linear(head_proj_weights.shape[0], head_proj_weights.shape[1], bias=False)\n",
    "        self.heads_proj.weight = nn.Parameter(head_proj_weights)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "        print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "        print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "        print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "        print(f\"heads_proj\\nShape: {self.heads_proj.weight.shape}\\n{self.heads_proj.weight}\")\n",
    "        print(\"===========================================\")\n",
    "\n",
    "        query = self.W_query(x)\n",
    "        key = self.W_key(x)\n",
    "        value = self.W_value(x)\n",
    "        \n",
    "\n",
    "        print(f\"queries\\nShape: {query.shape}\\n{query}\")\n",
    "        print(f\"keys\\nShape:   {key.shape}\\n{key}\")\n",
    "        print(f\"values\\nShape: {value.shape}\\n{value}\")\n",
    "        # print(\"===========================================\")\n",
    "\n",
    "        queries_chnuks = torch.chunk(query, self.num_heads, 1)\n",
    "        keys_chnuks = torch.chunk(key , self.num_heads, 1)\n",
    "        values_chnuks = torch.chunk(value, self.num_heads, 1)\n",
    "\n",
    "        # print(f\"queries_chnuks\\nShape: {queries_chnuks[0].shape}\\n{queries_chnuks[0]}\")\n",
    "        # print(f\"keys_chnuks\\nShape: {keys_chnuks[0].shape}\\n{keys_chnuks[0]}\")\n",
    "        # print(f\"values_chnuks\\nShape: {values_chnuks[0].shape}\\n{values_chnuks[0]}\")\n",
    "        # print(\"===========================================\")\n",
    "\n",
    "\n",
    "        context_vecs = []\n",
    "        for head in range(0, self.num_heads):\n",
    "            print(f\"=================================== HEAD {head} ===================================\\n\")\n",
    "            key_transposed = keys_chnuks[head].t()\n",
    "            print(f\"Key transposed\\nShape: {key_transposed.shape}\\n{key_transposed}\")\n",
    "\n",
    "            attention_scores = queries_chnuks[head] @ key_transposed\n",
    "            print(f\"Attention Scores\\nShape: {attention_scores.shape}\\n{attention_scores}\")\n",
    "\n",
    "            attention_scores_scaled = attention_scores * 1/math.sqrt(keys_chnuks[head].shape[1])\n",
    "            print(f\"Attention Scores Scaled\\nShape: {attention_scores_scaled.shape}\\n{attention_scores_scaled}\")\n",
    "\n",
    "            attention_weights = F.softmax(attention_scores_scaled, dim=1)\n",
    "            print(f\"Attention Weights\\nShape: {attention_weights.shape}\\n{attention_weights}\")\n",
    "            context_vec = attention_weights @ values_chnuks[head]\n",
    "            print(f\"Context Vec\\nShape: {context_vec.shape}\\n{context_vec}\")\n",
    "            context_vecs.append(context_vec)\n",
    "            print(f\"=================================================================================\\n\")\n",
    "\n",
    "        concat_heads = torch.cat(context_vecs, dim=1)\n",
    "        print(f\"concat_heads\\nShape: {concat_heads.shape}\\n{concat_heads}\")\n",
    "        print(self.heads_proj.weight)\n",
    "        projected_context_vecs = self.heads_proj(concat_heads)\n",
    "        print(f\"projected_context_vecs\\nShape: {projected_context_vecs.shape}\\n{projected_context_vecs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ae2e1297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "W_Query\n",
      "Shape: torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.69, -0.17, -0.48,  0.27],\n",
      "        [ 0.05,  0.40, -0.91,  0.51],\n",
      "        [-0.82,  0.82,  0.47,  0.98],\n",
      "        [ 0.31,  0.52, -0.34, -0.27]], requires_grad=True)\n",
      "W_Key\n",
      "Shape:   torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.51,  0.30, -0.45, -0.52],\n",
      "        [ 0.97, -0.85, -0.13, -0.45],\n",
      "        [ 0.45,  0.26,  0.53, -0.28],\n",
      "        [ 0.51,  0.77, -0.04, -0.67]], requires_grad=True)\n",
      "W_Value\n",
      "Shape: torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.03,  0.81,  0.97,  0.90],\n",
      "        [ 0.80,  0.01, -0.01, -0.85],\n",
      "        [ 0.82,  0.03, -0.47,  0.00],\n",
      "        [-0.88, -0.36, -0.82, -0.23]], requires_grad=True)\n",
      "heads_proj\n",
      "Shape: torch.Size([4, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.45,  0.88,  0.66,  0.74],\n",
      "        [ 0.83, -0.90, -0.75,  0.26],\n",
      "        [ 0.06,  0.52, -0.97,  0.47],\n",
      "        [-0.07,  0.54,  0.38,  0.45]], requires_grad=True)\n",
      "===========================================\n",
      "queries\n",
      "Shape: torch.Size([6, 4])\n",
      "tensor([[-0.83, -0.85,  0.37, -0.85],\n",
      "        [ 0.68,  0.79, -0.59, -0.06],\n",
      "        [ 0.22,  0.60,  0.35,  0.63],\n",
      "        [ 1.06,  0.50, -1.66,  0.01],\n",
      "        [ 0.28, -0.61, -2.04, -0.09],\n",
      "        [-0.03,  0.41,  1.37, -0.15]], grad_fn=<MmBackward0>)\n",
      "keys\n",
      "Shape:   torch.Size([6, 4])\n",
      "tensor([[ 0.10, -0.37, -0.35, -1.05],\n",
      "        [ 0.02,  0.50, -0.70, -0.60],\n",
      "        [ 0.16, -0.38,  0.26,  0.84],\n",
      "        [-0.25,  1.52, -0.43, -0.38],\n",
      "        [ 0.09,  1.58,  0.02,  0.01],\n",
      "        [-0.33, -0.89, -0.13, -0.39]], grad_fn=<MmBackward0>)\n",
      "values\n",
      "Shape: torch.Size([6, 4])\n",
      "tensor([[-0.15, -0.74, -1.08,  0.75],\n",
      "        [-1.01, -0.25,  0.47,  0.80],\n",
      "        [ 0.51,  0.27,  0.43, -0.45],\n",
      "        [-1.58,  0.48,  0.93,  0.47],\n",
      "        [-1.81,  1.00,  0.36,  0.44],\n",
      "        [ 1.25, -0.87, -0.21, -0.33]], grad_fn=<MmBackward0>)\n",
      "=================================== HEAD 0 ===================================\n",
      "\n",
      "Key transposed\n",
      "Shape: torch.Size([2, 6])\n",
      "tensor([[ 0.10,  0.02,  0.16, -0.25,  0.09, -0.33],\n",
      "        [-0.37,  0.50, -0.38,  1.52,  1.58, -0.89]], grad_fn=<TBackward0>)\n",
      "Attention Scores\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[ 0.23, -0.44,  0.19, -1.09, -1.42,  1.03],\n",
      "        [-0.23,  0.41, -0.19,  1.04,  1.31, -0.93],\n",
      "        [-0.20,  0.30, -0.19,  0.85,  0.96, -0.60],\n",
      "        [-0.08,  0.27, -0.02,  0.50,  0.89, -0.80],\n",
      "        [ 0.25, -0.30,  0.27, -0.99, -0.94,  0.45],\n",
      "        [-0.16,  0.21, -0.16,  0.64,  0.65, -0.36]], grad_fn=<MmBackward0>)\n",
      "Attention Scores Scaled\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[ 0.16, -0.31,  0.13, -0.77, -1.01,  0.73],\n",
      "        [-0.16,  0.29, -0.14,  0.73,  0.93, -0.66],\n",
      "        [-0.14,  0.21, -0.13,  0.60,  0.68, -0.43],\n",
      "        [-0.06,  0.19, -0.01,  0.35,  0.63, -0.56],\n",
      "        [ 0.18, -0.21,  0.19, -0.70, -0.67,  0.32],\n",
      "        [-0.11,  0.15, -0.11,  0.45,  0.46, -0.25]], grad_fn=<DivBackward0>)\n",
      "Attention Weights\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[0.20, 0.12, 0.19, 0.08, 0.06, 0.35],\n",
      "        [0.10, 0.16, 0.11, 0.25, 0.31, 0.06],\n",
      "        [0.12, 0.17, 0.12, 0.25, 0.27, 0.09],\n",
      "        [0.13, 0.17, 0.14, 0.20, 0.27, 0.08],\n",
      "        [0.21, 0.14, 0.22, 0.09, 0.09, 0.25],\n",
      "        [0.13, 0.17, 0.13, 0.23, 0.23, 0.11]], grad_fn=<SoftmaxBackward0>)\n",
      "Context Vec\n",
      "Shape: torch.Size([6, 2])\n",
      "tensor([[ 0.15, -0.33],\n",
      "        [-1.01,  0.29],\n",
      "        [-0.89,  0.21],\n",
      "        [-0.83,  0.19],\n",
      "        [-0.07, -0.22],\n",
      "        [-0.76,  0.14]], grad_fn=<MmBackward0>)\n",
      "=================================================================================\n",
      "\n",
      "=================================== HEAD 1 ===================================\n",
      "\n",
      "Key transposed\n",
      "Shape: torch.Size([2, 6])\n",
      "tensor([[-0.35, -0.70,  0.26, -0.43,  0.02, -0.13],\n",
      "        [-1.05, -0.60,  0.84, -0.38,  0.01, -0.39]], grad_fn=<TBackward0>)\n",
      "Attention Scores\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[ 0.76,  0.25, -0.61,  0.16, -0.00,  0.28],\n",
      "        [ 0.27,  0.45, -0.20,  0.28, -0.01,  0.10],\n",
      "        [-0.78, -0.62,  0.62, -0.39,  0.01, -0.29],\n",
      "        [ 0.57,  1.16, -0.42,  0.72, -0.03,  0.21],\n",
      "        [ 0.81,  1.48, -0.60,  0.92, -0.04,  0.29],\n",
      "        [-0.32, -0.86,  0.22, -0.53,  0.03, -0.11]], grad_fn=<MmBackward0>)\n",
      "Attention Scores Scaled\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[ 0.53,  0.17, -0.43,  0.12, -0.00,  0.20],\n",
      "        [ 0.19,  0.32, -0.14,  0.20, -0.01,  0.07],\n",
      "        [-0.55, -0.44,  0.44, -0.28,  0.01, -0.20],\n",
      "        [ 0.41,  0.82, -0.30,  0.51, -0.02,  0.15],\n",
      "        [ 0.57,  1.05, -0.42,  0.65, -0.03,  0.21],\n",
      "        [-0.23, -0.61,  0.16, -0.38,  0.02, -0.08]], grad_fn=<DivBackward0>)\n",
      "Attention Weights\n",
      "Shape: torch.Size([6, 6])\n",
      "tensor([[0.25, 0.17, 0.09, 0.16, 0.15, 0.18],\n",
      "        [0.18, 0.20, 0.13, 0.18, 0.15, 0.16],\n",
      "        [0.11, 0.12, 0.29, 0.14, 0.19, 0.15],\n",
      "        [0.18, 0.27, 0.09, 0.20, 0.12, 0.14],\n",
      "        [0.19, 0.30, 0.07, 0.20, 0.10, 0.13],\n",
      "        [0.16, 0.11, 0.23, 0.13, 0.20, 0.18]], grad_fn=<SoftmaxBackward0>)\n",
      "Context Vec\n",
      "Shape: torch.Size([6, 2])\n",
      "tensor([[0.02, 0.36],\n",
      "        [0.14, 0.34],\n",
      "        [0.23, 0.15],\n",
      "        [0.17, 0.41],\n",
      "        [0.17, 0.45],\n",
      "        [0.14, 0.19]], grad_fn=<MmBackward0>)\n",
      "=================================================================================\n",
      "\n",
      "concat_heads\n",
      "Shape: torch.Size([6, 4])\n",
      "tensor([[ 0.15, -0.33,  0.02,  0.36],\n",
      "        [-1.01,  0.29,  0.14,  0.34],\n",
      "        [-0.89,  0.21,  0.23,  0.15],\n",
      "        [-0.83,  0.19,  0.17,  0.41],\n",
      "        [-0.07, -0.22,  0.17,  0.45],\n",
      "        [-0.76,  0.14,  0.14,  0.19]], grad_fn=<CatBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.45,  0.88,  0.66,  0.74],\n",
      "        [ 0.83, -0.90, -0.75,  0.26],\n",
      "        [ 0.06,  0.52, -0.97,  0.47],\n",
      "        [-0.07,  0.54,  0.38,  0.45]], requires_grad=True)\n",
      "projected_context_vecs\n",
      "Shape: torch.Size([6, 4])\n",
      "tensor([[-0.07,  0.50, -0.01, -0.02],\n",
      "        [ 1.05, -1.12,  0.11,  0.43],\n",
      "        [ 0.85, -1.06, -0.10,  0.33],\n",
      "        [ 0.96, -0.88,  0.08,  0.41],\n",
      "        [ 0.28,  0.13, -0.07,  0.16],\n",
      "        [ 0.69, -0.81, -0.02,  0.27]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "self_attention = SelfAttentionMultiHead(self_attention_layer_c, 2)\n",
    "\n",
    "self_attention.forward(input_embeddings_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefd18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
