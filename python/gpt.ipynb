{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "eb770f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7586e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensors(filename):\n",
    "    tensors_dict = {}\n",
    "    \n",
    "    # Tracking current tensor state\n",
    "    current_name = \"default_tensor\"\n",
    "    current_metadata = {}\n",
    "    current_data = []\n",
    "    \n",
    "    meta_keys = ['size', 'ndim', 'shape', 'stride', 'elem_size', 'requires_grad']\n",
    "\n",
    "    def finalize_tensor(name, meta, data):\n",
    "        \"\"\"Helper to reshape data and store in the dictionary.\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "        \n",
    "        target_shape = [int(s) for s in meta.get('shape', [len(data)])]\n",
    "        # Using float64 (double) as your images show high precision decimals\n",
    "        # if name == current_name:\n",
    "        #     tensors_dict = torch.tensor(data, dtype=torch.float32).reshape(target_shape)\n",
    "        # else:\n",
    "        tensors_dict[name] = torch.tensor(data, dtype=torch.float64).reshape(target_shape)\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # Split by comma and remove empty strings/whitespace\n",
    "            parts = [p.strip() for p in line.split(',') if p.strip()]\n",
    "            \n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            label = parts[0].replace(':', '')\n",
    "\n",
    "            # 1. Check if it's a known metadata key\n",
    "            if label in meta_keys:\n",
    "                vals = [float(v) for v in parts[1:]]\n",
    "                current_metadata[label] = vals[0] if len(vals) == 1 else vals\n",
    "            \n",
    "            else:\n",
    "                try:\n",
    "                    # 2. Try to parse as numeric data\n",
    "                    row_data = [float(p) for p in parts]\n",
    "                    current_data.extend(row_data)\n",
    "                except ValueError:\n",
    "                    # 3. If it's a string but NOT metadata, it's a new tensor name\n",
    "                    # Save the previous tensor first\n",
    "                    if current_data:\n",
    "                        finalize_tensor(current_name, current_metadata, current_data)\n",
    "                    \n",
    "                    # Reset for the new tensor\n",
    "                    current_name = label\n",
    "                    current_metadata = {}\n",
    "                    current_data = []\n",
    "\n",
    "    # Finalize the last tensor in the file\n",
    "    finalize_tensor(current_name, current_metadata, current_data)\n",
    "    if \"default_tensor\" in tensors_dict.keys():\n",
    "        tensors_dict = tensors_dict[\"default_tensor\"]\n",
    "    return tensors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3c7a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_model(path):\n",
    "    gpt_model = {}\n",
    "    folder_path = Path(path)\n",
    "    for file in folder_path.iterdir():\n",
    "        if file.is_file():  # Ensure it's a file and not a subfolder\n",
    "            filename = Path(file.name).stem\n",
    "            # print(f\"Filename: {filename}\")\n",
    "            gpt_model[filename] = load_tensors(file)\n",
    "            # print(f\"Full Path: {file}\")\n",
    "    return gpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fa39cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Weights': tensor([[ 0.85,  0.90, -0.04,  ..., -0.58, -0.18,  0.63],\n",
       "         [ 0.28, -0.13, -0.90,  ..., -0.52,  0.49,  0.62],\n",
       "         [ 0.62,  0.66, -0.76,  ...,  0.83,  0.33, -0.73],\n",
       "         ...,\n",
       "         [ 0.94,  0.39,  0.47,  ...,  0.83,  0.61, -0.22],\n",
       "         [-0.61,  0.08, -0.45,  ...,  0.30, -0.30, -0.89],\n",
       "         [-0.06,  0.91, -0.10,  ..., -0.90,  0.63, -0.89]]),\n",
       " 'Output': tensor([[[    -2.76,     -0.48,      5.78,      0.43,      5.98,      6.62,\n",
       "               -6.77,      1.45,      6.02,     -8.07,      5.96,     14.18,\n",
       "               -1.58,      3.50,      8.39,      1.56,      2.96,    -13.13,\n",
       "               -3.33,      4.71,      5.05,      7.32,      7.52,      9.71,\n",
       "                4.27,    -10.34,      2.62,      6.44,    -10.13,     -6.80,\n",
       "               -1.95,    -16.84],\n",
       "          [    -0.42,     -5.68,     -5.70,     10.29,     12.53,      4.34,\n",
       "                4.55,      1.88,      2.43,     -8.14,     -1.08,      5.87,\n",
       "               10.10,     -5.06,     13.84,     -8.20,      9.89,     -2.25,\n",
       "                2.49,     15.63,      1.92,     -4.66,     -4.83,      1.01,\n",
       "               -8.26,    -26.62,      0.27,     -5.13,    -15.58,     20.13,\n",
       "               -2.94,    -10.78],\n",
       "          [     5.83,      0.99,      5.05,    -12.93,     -2.88,     -2.98,\n",
       "                8.35,    -17.04,     14.30,     -3.02,      6.21,     -3.88,\n",
       "               -4.28,      1.96,      3.64,     -4.51,     15.86,    -17.62,\n",
       "               -8.18,     -3.88,     -1.62,     -0.01,      3.31,      3.31,\n",
       "               -3.51,     -6.35,      4.97,     -9.78,    -11.61,     11.94,\n",
       "                8.57,      0.89],\n",
       "          [    -0.41,     -5.68,     -5.71,     10.28,     12.54,      4.34,\n",
       "                4.55,      1.89,      2.43,     -8.15,     -1.09,      5.88,\n",
       "               10.11,     -5.05,     13.85,     -8.20,      9.90,     -2.26,\n",
       "                2.49,     15.63,      1.92,     -4.67,     -4.82,      1.01,\n",
       "               -8.26,    -26.61,      0.27,     -5.14,    -15.58,     20.14,\n",
       "               -2.94,    -10.80],\n",
       "          [     0.35,     -5.77,      2.68,     11.69,      4.98,      5.03,\n",
       "                4.83,    -14.95,      2.84,    -11.67,     -5.56,      3.60,\n",
       "               -1.14,    -16.67,     11.59,    -11.69,      4.67,      5.24,\n",
       "               -3.30,     16.64,     11.00,     -2.87,     -0.18,     -1.56,\n",
       "               -4.93,    -28.03,     -4.91,     -6.87,     -3.94,      1.93,\n",
       "               -9.40,     -1.32],\n",
       "          [    -7.27,     13.47,     -5.78,      1.36,      7.17,     18.35,\n",
       "               -1.35,      0.50,      6.18,     -5.75,     -0.48,      2.46,\n",
       "                6.36,      2.86,      1.33,      7.77,     15.07,     -8.86,\n",
       "              -15.53,     -1.63,     13.30,     -6.99,      0.17,      4.41,\n",
       "                3.27,     -5.59,     16.74,      0.69,    -11.52,      6.17,\n",
       "               -2.09,    -12.23]],\n",
       " \n",
       "         [[     4.09,     -1.32,     -8.94,      9.43,      3.54,     -0.70,\n",
       "              -10.01,      6.25,    -16.73,     -9.70,    -11.20,     12.82,\n",
       "               -0.65,     -7.20,     17.35,    -11.93,      8.54,      7.34,\n",
       "                4.14,      9.48,      2.44,     -8.48,     -1.80,      3.47,\n",
       "               -9.80,    -23.57,     -3.82,     -1.91,     -6.12,    -11.36,\n",
       "              -10.66,     -2.72],\n",
       "          [    -2.70,     -6.07,     -7.34,      6.48,     -4.34,     -3.55,\n",
       "               -6.90,     10.96,     -6.31,     -6.55,     -8.63,      5.38,\n",
       "               -0.61,     -2.66,      1.98,      3.04,      0.82,      0.97,\n",
       "                3.89,     -6.33,     -4.52,      5.39,     -2.56,     -1.90,\n",
       "                1.13,    -10.30,     -6.70,      4.53,     -2.14,    -12.02,\n",
       "               -8.37,     -0.15],\n",
       "          [    -7.71,     -2.25,     -3.94,      0.97,     -0.73,      3.33,\n",
       "                5.29,     -2.04,     14.67,     -1.05,      4.77,      0.00,\n",
       "               -5.43,      1.34,     -4.24,     15.54,     12.26,    -16.74,\n",
       "               -0.35,     -8.26,     -3.37,     15.06,     -7.82,     -3.84,\n",
       "                1.44,     -5.63,      4.67,     -5.33,     -9.26,      7.49,\n",
       "                3.26,    -14.54],\n",
       "          [     3.49,      1.55,     -3.94,     -3.53,     -1.94,      8.62,\n",
       "               -4.48,     -5.53,     -2.52,     -1.13,     -9.07,      2.76,\n",
       "                3.20,      7.78,      6.43,     -8.93,      9.69,     -1.46,\n",
       "               -4.68,     -0.99,      0.60,    -16.26,      1.30,     -2.45,\n",
       "                2.00,      5.05,     -2.49,      0.74,      0.25,      1.22,\n",
       "               -7.19,     14.36],\n",
       "          [     1.77,    -12.76,     -4.74,     25.63,      2.61,     -1.23,\n",
       "               -9.40,      0.05,    -17.62,     -6.04,     -5.45,      8.92,\n",
       "                6.23,     -9.50,      2.95,    -19.99,      6.77,     14.73,\n",
       "               -5.23,     12.97,     -4.11,     -0.56,     -4.97,      8.44,\n",
       "              -19.38,    -29.11,    -10.07,     12.48,    -10.30,     -2.07,\n",
       "               -9.29,      6.83],\n",
       "          [     2.83,      6.32,      8.39,     -6.35,      2.54,      1.85,\n",
       "                0.32,    -11.69,     19.77,     -9.22,     13.46,      4.13,\n",
       "               -5.79,      4.77,      7.33,     -5.20,     12.59,    -14.88,\n",
       "              -13.57,      4.13,      1.32,     -1.44,      3.61,      0.93,\n",
       "                0.16,    -10.18,      7.36,     -2.01,     -9.38,     10.59,\n",
       "                5.86,     -0.94]]])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model_c = load_gpt_model(\"/Users/uonliaquat/workspace/zerograd/models\")\n",
    "gpt_model_c[\"transformer_layer_0__self_attention_layer_heads_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f8fdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = '../models'\n",
    "# gpt_model_c = {\n",
    "#     \"input_tokens\":  load_tensors(f'{base_path}/input_tokens.csv')['default_tensor'].long(),\n",
    "#     \"token_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.token_embed_layer.csv'),\n",
    "#     \"pos_embeddings_layer\": load_tensors(f'{base_path}/gpt_model.pos_embed_layer.csv'),\n",
    "#     \"position_indicies\":  load_tensors(f'{base_path}/gpt_model.workspace.position_indicies.csv')['default_tensor'].long(),\n",
    "#     \"input_embeddings\": load_tensors(f'{base_path}/gpt_model.workspace.input_embeddings.csv')['default_tensor']\n",
    "# }\n",
    "# gpt_model_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b98d9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_within_tolerance(a, b, atol):\n",
    "    if a.shape != b.shape:\n",
    "        print(\"Shape mismatch:\", a.shape, b.shape)\n",
    "        return False\n",
    "\n",
    "    max_diff = round((a - b).abs().max().item(), 2)\n",
    "    print(\"max |diff|:\", max_diff)\n",
    "\n",
    "    return max_diff <= atol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12b649",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efcd23ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_layer_0__self_attention_layer_attention_scores_0\n",
      "gpt_model.pos_embed_layer\n",
      "transformer_layer_0__self_attention_layer_attention_scores_1\n",
      "input_tokens\n",
      "transformer_layer_0__self_attention_layer_context_vecs_0\n",
      "transformer_layer_0__self_attention_layer_w_key\n",
      "transformer_layer_0__self_attention_layer_context_vecs_1\n",
      "transformer_layer_0__self_attention_layer_w_query\n",
      "gpt_model.token_embed_layer\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_0\n",
      "gpt_model.workspace.position_indices\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_1\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_0\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_1\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_0\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_1\n",
      "transformer_layer_0__self_attention_layer_w_value\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_0\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_1\n",
      "transformer_layer_0__self_attention_layer_attention_weights_1\n",
      "gpt_model.workspace.input_embeddings\n",
      "transformer_layer_0__self_attention_layer_attention_weights_0\n",
      "transformer_layer_0__self_attention_layer_values_chunks_0\n",
      "transformer_layer_0__self_attention_layer_concat_heads\n",
      "transformer_layer_0__self_attention_layer_heads_proj\n",
      "transformer_layer_0__feed_forward_network_layer2\n",
      "transformer_layer_0__self_attention_layer_values_chunks_1\n",
      "transformer_layer_0__feed_forward_network_layer1\n"
     ]
    }
   ],
   "source": [
    "for key in gpt_model_c.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc403fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key(target_dict, substring):\n",
    "    \"\"\"\n",
    "    Returns the first key that contains the substring.\n",
    "    Returns None if no match is found.\n",
    "    \"\"\"\n",
    "    return next((k for k in target_dict if substring in k), None)\n",
    "\n",
    "# Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4953a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_transposed_global = None\n",
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, self_attention_layer_c, n_heads, atol):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.atol = atol\n",
    "        self.self_attention_layer_c = self_attention_layer_c\n",
    "        \n",
    "        W_query_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'w_query')]['Weights']\n",
    "        W_key_weights =     self_attention_layer_c[find_key(self_attention_layer_c, 'w_key')]['Weights']\n",
    "        W_value_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'w_value')]['Weights']\n",
    "        head_proj_weights =   self_attention_layer_c[find_key(self_attention_layer_c, 'heads_proj')]['Weights']\n",
    "\n",
    "        # print(\"W_query_weights\\n \", W_query_weights)\n",
    "        W_query_weights = W_query_weights.t()\n",
    "        W_key_weights = W_key_weights.t()\n",
    "        W_value_weights = W_value_weights.t()\n",
    "        head_proj_weights = head_proj_weights.t()\n",
    "\n",
    "        self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "        self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "        self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "        self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "        self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "        self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "\n",
    "        self.heads_proj = nn.Linear(head_proj_weights.shape[0], head_proj_weights.shape[1], bias=False)\n",
    "        self.heads_proj.weight = nn.Parameter(head_proj_weights)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "        # print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "        # print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "        # print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "        # print(f\"heads_proj\\nShape: {self.heads_proj.weight.shape}\\n{self.heads_proj.weight}\")\n",
    "        print(\"===========================================\")\n",
    "\n",
    "        query = self.W_query(x)\n",
    "        key = self.W_key(x)\n",
    "        value = self.W_value(x)\n",
    "        self.layer_name = 'self_attention_layer'\n",
    "        query_matched = tensors_within_tolerance(query, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_query')]['Output'], self.atol)\n",
    "        key_matched = tensors_within_tolerance(key,  self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_key')]['Output'], self.atol)\n",
    "        value_matched = tensors_within_tolerance(value,  self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_value')]['Output'], self.atol)\n",
    "\n",
    "        print(f\"query_matched:   {query_matched}\")\n",
    "        print(f\"key_matched:     {key_matched}\")\n",
    "        print(f\"value_matched:   {value_matched}\")\n",
    "        # print(\"Python\", query)\n",
    "        # print(\"C\", self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_w_query')]['Output'])\n",
    "\n",
    "        queries_chnuks  = torch.chunk(query, self.n_heads, -1)\n",
    "        keys_chnuks     = torch.chunk(key , self.n_heads, -1)\n",
    "        values_chnuks   = torch.chunk(value, self.n_heads, -1)\n",
    "\n",
    "        for head in range(0, self.n_heads):\n",
    "            #print(self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_queries_chunks_{head}')])\n",
    "            #print(self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_queries_chunks_{head}')]['Output'].shape)\n",
    "            query_chnuks_matched    = tensors_within_tolerance(queries_chnuks[head],  self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_queries_chunks_{head}')], self.atol)\n",
    "            key_chnuks_matched      = tensors_within_tolerance(keys_chnuks[head],     self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_keys_chunks_{head}')], self.atol)\n",
    "            value_chnuks_matched    = tensors_within_tolerance(values_chnuks[head],   self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_values_chunks_{head}')], self.atol)\n",
    "            print(f\"query_chnuks_matched:   {query_chnuks_matched}\")\n",
    "            print(f\"key_chnuks_matched:     {key_chnuks_matched}\")\n",
    "            print(f\"value_chnuks_matched:   {value_chnuks_matched}\")\n",
    "\n",
    "        context_vecs = []\n",
    "        for head in range(0, self.n_heads):\n",
    "            #print(self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_context_vecs_{head}')])\n",
    "            \n",
    "            print(f\"=================================== HEAD {head} ===================================\\n\")\n",
    "            key_transposed = keys_chnuks[head].transpose(1, 2)\n",
    "            key_transposed_matched = tensors_within_tolerance(key_transposed, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_keys_transposed_{head}')], self.atol)\n",
    "            print(f\"key_transposed_matched:   {key_transposed_matched}\")\n",
    "\n",
    "            attention_scores = queries_chnuks[head] @ key_transposed\n",
    "            attention_scores_matched = tensors_within_tolerance(attention_scores, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_attention_scores_{head}')], self.atol)\n",
    "            print(f\"attention_scores_matched:   {attention_scores_matched}\")\n",
    "\n",
    "\n",
    "            attention_scores_scaled = attention_scores * 1/math.sqrt(keys_chnuks[head].shape[1])\n",
    "            attention_scores_scaled_matched = tensors_within_tolerance(attention_scores_scaled, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_attention_scores_scaled_{head}')], self.atol)\n",
    "            print(f\"attention_scores_scaled_matched:   {attention_scores_scaled_matched}\")\n",
    "\n",
    "            attention_weights = F.softmax(attention_scores_scaled, dim=-1)\n",
    "            attention_weights_matched = tensors_within_tolerance(attention_weights, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_attention_weights_{head}')], self.atol)\n",
    "            print(f\"attention_weights_matched:   {attention_weights_matched}\")\n",
    "\n",
    "            context_vec = attention_weights @ values_chnuks[head]\n",
    "            context_vec_matched = tensors_within_tolerance(context_vec, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_context_vecs_{head}')], self.atol)\n",
    "            print(f\"context_vec_matched:   {context_vec_matched}\")\n",
    "\n",
    "            context_vecs.append(context_vec)\n",
    "        #     print(f\"=================================================================================\\n\")\n",
    "\n",
    "        concat_heads = torch.cat(context_vecs, dim=-1)\n",
    "        concat_heads_matched = tensors_within_tolerance(concat_heads, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_concat_heads')], self.atol)\n",
    "        print(f\"concat_heads_matched:   {concat_heads_matched}\")\n",
    "\n",
    "        projected_context_vecs = self.heads_proj(concat_heads)\n",
    "        projected_context_vecs_matched = tensors_within_tolerance(projected_context_vecs, self.self_attention_layer_c[find_key(self.self_attention_layer_c, f'{self.layer_name}_heads_proj')]['Output'], self.atol)\n",
    "        print(f\"projected_context_vecs_matched:   {projected_context_vecs_matched}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b41c3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, feed_forward_network_c):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(2, 5)\n",
    "        self.output = nn.Linear(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff63f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, transformer_layer_c, n_heads, atol):\n",
    "        super().__init__() \n",
    "        self.n_heads = n_heads\n",
    "        self.atol = atol\n",
    "        self_attention_layer_c = {k: v for k, v in transformer_layer_c.items() if \"self_attention_layer\" in k}\n",
    "        feed_forward_network_c = {k: v for k, v in transformer_layer_c.items() if \"feed_forward_network\" in k}\n",
    "        self.self_attention_multi_head  = SelfAttentionMultiHead(self_attention_layer_c, n_heads, atol)\n",
    "        #self.feed_forward_network       = FeedForwardNetwork(feed_forward_network_c)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention_multi_head(x)\n",
    "        #x = self.feed_forward_network(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b426a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, transformer_block_c, n_layers, n_heads, atol):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.atol = atol\n",
    "        self.transformer_layers = {}\n",
    "        for layer_no in range(0, n_layers):\n",
    "            transformer_layer_c = {k: v for k, v in transformer_block_c.items() if k.startswith(f\"transformer_layer_{layer_no}\")}\n",
    "            self.transformer_layers[layer_no] = Transformer(transformer_layer_c, n_heads, atol)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer_no in range(0, self.n_layers):\n",
    "            x = self.transformer_layers[layer_no](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d47f7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, gpt_model_c, n_layers, n_heads, atol):\n",
    "        super().__init__()   \n",
    "        self.atol = atol\n",
    "           \n",
    "        num_token_embeddings            = gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape[0]\n",
    "        token_embedding_dim             = gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape[1]\n",
    "        num_pos_embeddings              = gpt_model_c['gpt_model.pos_embed_layer']['Weights'].shape[0]\n",
    "        pos_embedding_dim               = gpt_model_c['gpt_model.pos_embed_layer']['Weights'].shape[1]\n",
    "\n",
    "        print(f\"num_token_embeddings: {num_token_embeddings}\")\n",
    "        print(f\"token_embedding_dim: {token_embedding_dim}\")\n",
    "        print(f\"num_pos_embeddings: {num_pos_embeddings}\")\n",
    "        print(f\"pos_embedding_dim: {pos_embedding_dim}\")\n",
    "\n",
    "        self.token_embeddings_layer     = nn.Embedding(num_embeddings=num_token_embeddings, embedding_dim=token_embedding_dim)\n",
    "        self.pos_embeddings_layer       = nn.Embedding(num_embeddings=num_pos_embeddings, embedding_dim=pos_embedding_dim)\n",
    "\n",
    "        self.token_embeddings_layer.weight.data.copy_(gpt_model_c['gpt_model.token_embed_layer']['Weights'])\n",
    "        self.pos_embeddings_layer.weight.data.copy_(gpt_model_c['gpt_model.pos_embed_layer']['Weights'])\n",
    "        \n",
    " \n",
    "        assert(gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape == self.token_embeddings_layer.weight.shape)\n",
    "        assert(gpt_model_c['gpt_model.token_embed_layer']['Weights'].shape == self.pos_embeddings_layer.weight.shape)\n",
    "\n",
    "        \n",
    "        self.token_embeddings_c         = gpt_model_c['gpt_model.token_embed_layer']['Output']\n",
    "        self.pos_embeddings_c           = gpt_model_c['gpt_model.pos_embed_layer']['Output']\n",
    "        self.input_embeddings_c         = gpt_model_c['gpt_model.workspace.input_embeddings']\n",
    "        self.position_indicies_c        = gpt_model_c['gpt_model.workspace.position_indices'].long()\n",
    "\n",
    "        transformer_block_c = {k: v for k, v in gpt_model_c.items() if \"transformer_layer\" in k}\n",
    "        self.transformer_block = TransformerBlock(transformer_block_c, n_layers, n_heads, self.atol)\n",
    "\n",
    "    def forward(self, input_tokens_c):\n",
    "        #print(f\"x.shape:    {input_tokens_c.shape}\")\n",
    "        token_embeddings    = self.token_embeddings_layer(input_tokens_c)[0]\n",
    "        print(self.position_indicies_c)\n",
    "        pos_embeddings      = self.pos_embeddings_layer(self.position_indicies_c)[0]\n",
    "        input_embeddings    = token_embeddings + pos_embeddings\n",
    "\n",
    "\n",
    "        token_embeddings_matched    = tensors_within_tolerance(token_embeddings,    self.token_embeddings_c,    self.atol)\n",
    "        pos_embeddings_matched      = tensors_within_tolerance(pos_embeddings,      self.pos_embeddings_c,      self.atol)\n",
    "        input_embeddings_matched    = tensors_within_tolerance(input_embeddings,    self.input_embeddings_c,    self.atol)\n",
    "\n",
    "        print(f\"token_embeddings_matched:   {token_embeddings_matched}\")\n",
    "        print(f\"pos_embeddings_matched:     {pos_embeddings_matched}\")\n",
    "        print(f\"input_embeddings_matched:   {input_embeddings_matched}\")\n",
    "\n",
    "\n",
    "        # print(\"***********Token Embeddings***********\\n\")\n",
    "        # print(self.token_embeddings_c[0], \"\\n\")\n",
    "        # print(token_embeddings[0], \"\\n\\n\")\n",
    "\n",
    "        # print(\"***********Pose Embeddings***********\\n\")\n",
    "        # print(self.pos_embeddings_c[0], \"\\n\")\n",
    "        # print(pos_embeddings[0], \"\\n\\n\")\n",
    "\n",
    "\n",
    "        # print(\"***********Input Embeddings***********\\n\")\n",
    "        # print(self.input_embeddings_c[0], \"\\n\")\n",
    "        # print(input_embeddings[0], \"\\n\\n\")\n",
    "\n",
    "        # assert(\n",
    "        #     token_embeddings_matched and \n",
    "        #     pos_embeddings_matched and \n",
    "        #     input_embeddings_matched\n",
    "        # )\n",
    "        \n",
    "        contextual_embddings = self.transformer_block(input_embeddings)\n",
    "\n",
    "\n",
    "        # print(token_embeddings - self.token_embed_layer_output_c)\n",
    "        return contextual_embddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2bd660d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_token_embeddings: 6\n",
      "token_embedding_dim: 32\n",
      "num_pos_embeddings: 6\n",
      "pos_embedding_dim: 32\n",
      "tensor([[[0, 1, 2, 3, 4, 5],\n",
      "         [0, 1, 2, 3, 4, 5]]])\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "token_embeddings_matched:   True\n",
      "pos_embeddings_matched:     True\n",
      "input_embeddings_matched:   True\n",
      "===========================================\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "query_matched:   True\n",
      "key_matched:     True\n",
      "value_matched:   True\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "query_chnuks_matched:   True\n",
      "key_chnuks_matched:     True\n",
      "value_chnuks_matched:   True\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "max |diff|: 0.0\n",
      "query_chnuks_matched:   True\n",
      "key_chnuks_matched:     True\n",
      "value_chnuks_matched:   True\n",
      "=================================== HEAD 0 ===================================\n",
      "\n",
      "max |diff|: 0.0\n",
      "key_transposed_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_scaled_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_weights_matched:   True\n",
      "max |diff|: 0.0\n",
      "context_vec_matched:   True\n",
      "=================================== HEAD 1 ===================================\n",
      "\n",
      "max |diff|: 0.0\n",
      "key_transposed_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_scores_scaled_matched:   True\n",
      "max |diff|: 0.0\n",
      "attention_weights_matched:   True\n",
      "max |diff|: 0.0\n",
      "context_vec_matched:   True\n",
      "max |diff|: 0.0\n",
      "concat_heads_matched:   True\n",
      "max |diff|: 0.0\n",
      "projected_context_vecs_matched:   True\n"
     ]
    }
   ],
   "source": [
    "atol = 0.00000001\n",
    "gpt = GPT(gpt_model_c=gpt_model_c, n_layers=1, n_heads=2, atol=atol)\n",
    "input_embeddings = gpt(gpt_model_c['input_tokens'].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65ef5fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_layer_0__self_attention_layer_context_vecs_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_0\n",
      "gpt_model.pos_embed_layer\n",
      "transformer_layer_0__self_attention_layer_attention_scores_1\n",
      "transformer_layer_0__self_attention_layer_context_vecs_2\n",
      "input_tokens\n",
      "transformer_layer_0__self_attention_layer_context_vecs_0\n",
      "transformer_layer_0__self_attention_layer_w_key\n",
      "transformer_layer_0__self_attention_layer_attention_scores_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_2\n",
      "transformer_layer_0__self_attention_layer_context_vecs_1\n",
      "transformer_layer_0__self_attention_layer_context_vecs_5\n",
      "transformer_layer_0__self_attention_layer_values_chunks_9\n",
      "transformer_layer_0__self_attention_layer_attention_scores_6\n",
      "transformer_layer_0__self_attention_layer_attention_scores_7\n",
      "transformer_layer_0__self_attention_layer_values_chunks_8\n",
      "transformer_layer_0__self_attention_layer_context_vecs_4\n",
      "transformer_layer_0__self_attention_layer_context_vecs_6\n",
      "transformer_layer_0__self_attention_layer_attention_scores_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_4\n",
      "transformer_layer_0__self_attention_layer_w_query\n",
      "transformer_layer_0__self_attention_layer_context_vecs_7\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_6\n",
      "gpt_model.token_embed_layer\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_4\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_7\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_4\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_6\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_9\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_5\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_7\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_6\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_8\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_4\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_7\n",
      "transformer_layer_0__self_attention_layer_attention_weights_8\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_0\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_2\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_3\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_1\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_2\n",
      "transformer_layer_0__self_attention_layer_attention_weights_9\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_0\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_3\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_1\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_0\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_2\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_1\n",
      "transformer_layer_0__self_attention_layer_attention_weights_7\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_3\n",
      "transformer_layer_0__self_attention_layer_w_value\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_2\n",
      "transformer_layer_0__self_attention_layer_attention_weights_6\n",
      "transformer_layer_0__self_attention_layer_attention_weights_4\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_0\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_1\n",
      "transformer_layer_0__self_attention_layer_attention_weights_5\n",
      "transformer_layer_0__self_attention_layer_attention_weights_1\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_5\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_9\n",
      "gpt_model.workspace.input_embeddings\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_4\n",
      "transformer_layer_0__self_attention_layer_attention_scores_scaled_8\n",
      "transformer_layer_0__self_attention_layer_attention_weights_0\n",
      "transformer_layer_0__self_attention_layer_attention_weights_2\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_9\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_6\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_8\n",
      "transformer_layer_0__self_attention_layer_queries_chunks_9\n",
      "transformer_layer_0__self_attention_layer_keys_chunks_7\n",
      "transformer_layer_0__self_attention_layer_keys_transposed_8\n",
      "transformer_layer_0__self_attention_layer_attention_weights_3\n",
      "transformer_layer_0__self_attention_layer_values_chunks_6\n",
      "transformer_layer_0__self_attention_layer_attention_scores_9\n",
      "transformer_layer_0__self_attention_layer_attention_scores_8\n",
      "transformer_layer_0__self_attention_layer_values_chunks_7\n",
      "transformer_layer_0__self_attention_layer_context_vecs_9\n",
      "transformer_layer_0__self_attention_layer_values_chunks_5\n",
      "transformer_layer_0__self_attention_layer_values_chunks_4\n",
      "transformer_layer_0__self_attention_layer_context_vecs_8\n",
      "gpt_model.workspace.position_indicies\n",
      "transformer_layer_0__self_attention_layer_values_chunks_0\n",
      "transformer_layer_0__self_attention_layer_concat_heads\n",
      "transformer_layer_0__self_attention_layer_heads_proj\n",
      "transformer_layer_0__feed_forward_network_layer2\n",
      "transformer_layer_0__self_attention_layer_values_chunks_1\n",
      "transformer_layer_0__self_attention_layer_values_chunks_3\n",
      "transformer_layer_0__feed_forward_network_layer1\n",
      "transformer_layer_0__self_attention_layer_values_chunks_2\n"
     ]
    }
   ],
   "source": [
    "for key in gpt_model_c.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "3efab6b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[595], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkey_transposed_global\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "key_transposed_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8a38027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SelfAttention:\n",
    "#     def __init__(self, self_attention_layer_c):\n",
    "#         W_query_weights =   self_attention_layer_c['W_Query']\n",
    "#         W_key_weights =     self_attention_layer_c['W_Key']\n",
    "#         W_value_weights =   self_attention_layer_c['W_Value']\n",
    "\n",
    "#         W_query_weights = W_query_weights.t()\n",
    "#         W_key_weights = W_key_weights.t()\n",
    "#         W_value_weights = W_value_weights.t()\n",
    "\n",
    "#         self.W_query = nn.Linear(W_query_weights.shape[0], W_query_weights.shape[1], bias=False)\n",
    "#         self.W_query.weight = nn.Parameter(W_query_weights)\n",
    "\n",
    "#         self.W_key = nn.Linear(W_key_weights.shape[0], W_key_weights.shape[1], bias=False)\n",
    "#         self.W_key.weight = nn.Parameter(W_key_weights)\n",
    "\n",
    "#         self.W_value = nn.Linear(W_value_weights.shape[0], W_value_weights.shape[1], bias=False)\n",
    "#         self.W_value.weight = nn.Parameter(W_value_weights)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         print(f\"Input Embeddings: \\nShape: {x.shape}\\n{x}\")\n",
    "#         print(f\"W_Query\\nShape: {self.W_query.weight.shape}\\n{self.W_query.weight}\")\n",
    "#         print(f\"W_Key\\nShape:   {self.W_key.weight.shape}\\n{self.W_key.weight}\")\n",
    "#         print(f\"W_Value\\nShape: {self.W_value.weight.shape}\\n{self.W_value.weight}\")\n",
    "#         print(\"===========================================\")\n",
    "\n",
    "#         query = self.W_query(x)\n",
    "#         key = self.W_key(x)\n",
    "#         value = self.W_value(x)\n",
    "        \n",
    "\n",
    "#         # print(f\"Query\\nShape: {query.shape}\\n{query}\")\n",
    "#         # print(f\"Key\\nShape:   {key.shape}\\n{key}\")\n",
    "#         # print(f\"Value\\nShape: {value.shape}\\n{value}\")\n",
    "#         # print(\"===========================================\")\n",
    "\n",
    "#         key_transposed = key.t()\n",
    "#         print(f\"Key transposed\\nShape: {key_transposed.shape}\\n{key_transposed}\")\n",
    "\n",
    "#         attention_scores = query @ key_transposed\n",
    "#         print(f\"Attention Scores\\nShape: {attention_scores.shape}\\n{attention_scores}\")\n",
    "\n",
    "#         attention_scores_scaled = attention_scores * 1/math.sqrt(key.shape[1])\n",
    "#         print(f\"Attention Scores Scaled\\nShape: {attention_scores_scaled.shape}\\n{attention_scores_scaled}\")\n",
    "\n",
    "#         attention_weights = F.softmax(attention_scores_scaled, dim=1)\n",
    "#         print(f\"Attention Weights\\nShape: {attention_weights.shape}\\n{attention_weights}\")\n",
    "\n",
    "#         context_vecs = attention_weights @ value\n",
    "#         print(f\"Context Vecs\\nShape: {context_vecs.shape}\\n{context_vecs}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "56a1f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self_attention = SelfAttention(self_attention_layer_c)\n",
    "\n",
    "# self_attention.forward(input_embeddings_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eac5d4",
   "metadata": {},
   "source": [
    "## Safe Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "e3e0b8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.0.attn.bias\n",
      "h.0.attn.c_attn.bias\n",
      "h.0.attn.c_attn.weight\n",
      "h.0.attn.c_proj.bias\n",
      "h.0.attn.c_proj.weight\n",
      "h.0.ln_1.bias\n",
      "h.0.ln_1.weight\n",
      "h.0.ln_2.bias\n",
      "h.0.ln_2.weight\n",
      "h.0.mlp.c_fc.bias\n",
      "h.0.mlp.c_fc.weight\n",
      "h.0.mlp.c_proj.bias\n",
      "h.0.mlp.c_proj.weight\n",
      "h.1.attn.bias\n",
      "h.1.attn.c_attn.bias\n",
      "h.1.attn.c_attn.weight\n",
      "h.1.attn.c_proj.bias\n",
      "h.1.attn.c_proj.weight\n",
      "h.1.ln_1.bias\n",
      "h.1.ln_1.weight\n",
      "h.1.ln_2.bias\n",
      "h.1.ln_2.weight\n",
      "h.1.mlp.c_fc.bias\n",
      "h.1.mlp.c_fc.weight\n",
      "h.1.mlp.c_proj.bias\n",
      "h.1.mlp.c_proj.weight\n",
      "h.10.attn.bias\n",
      "h.10.attn.c_attn.bias\n",
      "h.10.attn.c_attn.weight\n",
      "h.10.attn.c_proj.bias\n",
      "h.10.attn.c_proj.weight\n",
      "h.10.ln_1.bias\n",
      "h.10.ln_1.weight\n",
      "h.10.ln_2.bias\n",
      "h.10.ln_2.weight\n",
      "h.10.mlp.c_fc.bias\n",
      "h.10.mlp.c_fc.weight\n",
      "h.10.mlp.c_proj.bias\n",
      "h.10.mlp.c_proj.weight\n",
      "h.11.attn.bias\n",
      "h.11.attn.c_attn.bias\n",
      "h.11.attn.c_attn.weight\n",
      "h.11.attn.c_proj.bias\n",
      "h.11.attn.c_proj.weight\n",
      "h.11.ln_1.bias\n",
      "h.11.ln_1.weight\n",
      "h.11.ln_2.bias\n",
      "h.11.ln_2.weight\n",
      "h.11.mlp.c_fc.bias\n",
      "h.11.mlp.c_fc.weight\n",
      "h.11.mlp.c_proj.bias\n",
      "h.11.mlp.c_proj.weight\n",
      "h.2.attn.bias\n",
      "h.2.attn.c_attn.bias\n",
      "h.2.attn.c_attn.weight\n",
      "h.2.attn.c_proj.bias\n",
      "h.2.attn.c_proj.weight\n",
      "h.2.ln_1.bias\n",
      "h.2.ln_1.weight\n",
      "h.2.ln_2.bias\n",
      "h.2.ln_2.weight\n",
      "h.2.mlp.c_fc.bias\n",
      "h.2.mlp.c_fc.weight\n",
      "h.2.mlp.c_proj.bias\n",
      "h.2.mlp.c_proj.weight\n",
      "h.3.attn.bias\n",
      "h.3.attn.c_attn.bias\n",
      "h.3.attn.c_attn.weight\n",
      "h.3.attn.c_proj.bias\n",
      "h.3.attn.c_proj.weight\n",
      "h.3.ln_1.bias\n",
      "h.3.ln_1.weight\n",
      "h.3.ln_2.bias\n",
      "h.3.ln_2.weight\n",
      "h.3.mlp.c_fc.bias\n",
      "h.3.mlp.c_fc.weight\n",
      "h.3.mlp.c_proj.bias\n",
      "h.3.mlp.c_proj.weight\n",
      "h.4.attn.bias\n",
      "h.4.attn.c_attn.bias\n",
      "h.4.attn.c_attn.weight\n",
      "h.4.attn.c_proj.bias\n",
      "h.4.attn.c_proj.weight\n",
      "h.4.ln_1.bias\n",
      "h.4.ln_1.weight\n",
      "h.4.ln_2.bias\n",
      "h.4.ln_2.weight\n",
      "h.4.mlp.c_fc.bias\n",
      "h.4.mlp.c_fc.weight\n",
      "h.4.mlp.c_proj.bias\n",
      "h.4.mlp.c_proj.weight\n",
      "h.5.attn.bias\n",
      "h.5.attn.c_attn.bias\n",
      "h.5.attn.c_attn.weight\n",
      "h.5.attn.c_proj.bias\n",
      "h.5.attn.c_proj.weight\n",
      "h.5.ln_1.bias\n",
      "h.5.ln_1.weight\n",
      "h.5.ln_2.bias\n",
      "h.5.ln_2.weight\n",
      "h.5.mlp.c_fc.bias\n",
      "h.5.mlp.c_fc.weight\n",
      "h.5.mlp.c_proj.bias\n",
      "h.5.mlp.c_proj.weight\n",
      "h.6.attn.bias\n",
      "h.6.attn.c_attn.bias\n",
      "h.6.attn.c_attn.weight\n",
      "h.6.attn.c_proj.bias\n",
      "h.6.attn.c_proj.weight\n",
      "h.6.ln_1.bias\n",
      "h.6.ln_1.weight\n",
      "h.6.ln_2.bias\n",
      "h.6.ln_2.weight\n",
      "h.6.mlp.c_fc.bias\n",
      "h.6.mlp.c_fc.weight\n",
      "h.6.mlp.c_proj.bias\n",
      "h.6.mlp.c_proj.weight\n",
      "h.7.attn.bias\n",
      "h.7.attn.c_attn.bias\n",
      "h.7.attn.c_attn.weight\n",
      "h.7.attn.c_proj.bias\n",
      "h.7.attn.c_proj.weight\n",
      "h.7.ln_1.bias\n",
      "h.7.ln_1.weight\n",
      "h.7.ln_2.bias\n",
      "h.7.ln_2.weight\n",
      "h.7.mlp.c_fc.bias\n",
      "h.7.mlp.c_fc.weight\n",
      "h.7.mlp.c_proj.bias\n",
      "h.7.mlp.c_proj.weight\n",
      "h.8.attn.bias\n",
      "h.8.attn.c_attn.bias\n",
      "h.8.attn.c_attn.weight\n",
      "h.8.attn.c_proj.bias\n",
      "h.8.attn.c_proj.weight\n",
      "h.8.ln_1.bias\n",
      "h.8.ln_1.weight\n",
      "h.8.ln_2.bias\n",
      "h.8.ln_2.weight\n",
      "h.8.mlp.c_fc.bias\n",
      "h.8.mlp.c_fc.weight\n",
      "h.8.mlp.c_proj.bias\n",
      "h.8.mlp.c_proj.weight\n",
      "h.9.attn.bias\n",
      "h.9.attn.c_attn.bias\n",
      "h.9.attn.c_attn.weight\n",
      "h.9.attn.c_proj.bias\n",
      "h.9.attn.c_proj.weight\n",
      "h.9.ln_1.bias\n",
      "h.9.ln_1.weight\n",
      "h.9.ln_2.bias\n",
      "h.9.ln_2.weight\n",
      "h.9.mlp.c_fc.bias\n",
      "h.9.mlp.c_fc.weight\n",
      "h.9.mlp.c_proj.bias\n",
      "h.9.mlp.c_proj.weight\n",
      "ln_f.bias\n",
      "ln_f.weight\n",
      "wpe.weight\n",
      "wte.weight\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/Users/uonliaquat/Downloads/model.safetensors\"\n",
    "\n",
    "with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    keys = list(f.keys())\n",
    "\n",
    "for key in keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "c0e45d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte: torch.Size([1, 4, 768])\n",
      "wpe: torch.Size([1, 4, 768])\n",
      "drop: torch.Size([1, 4, 768])\n",
      "h.0.ln_1: torch.Size([1, 4, 768])\n",
      "h.0.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.0.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.0.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.0.attn: tuple\n",
      "h.0.ln_2: torch.Size([1, 4, 768])\n",
      "h.0.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.0.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.0.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.0.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.0.mlp: torch.Size([1, 4, 768])\n",
      "h.0: tuple\n",
      "h.1.ln_1: torch.Size([1, 4, 768])\n",
      "h.1.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.1.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.1.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.1.attn: tuple\n",
      "h.1.ln_2: torch.Size([1, 4, 768])\n",
      "h.1.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.1.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.1.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.1.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.1.mlp: torch.Size([1, 4, 768])\n",
      "h.1: tuple\n",
      "h.2.ln_1: torch.Size([1, 4, 768])\n",
      "h.2.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.2.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.2.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.2.attn: tuple\n",
      "h.2.ln_2: torch.Size([1, 4, 768])\n",
      "h.2.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.2.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.2.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.2.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.2.mlp: torch.Size([1, 4, 768])\n",
      "h.2: tuple\n",
      "h.3.ln_1: torch.Size([1, 4, 768])\n",
      "h.3.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.3.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.3.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.3.attn: tuple\n",
      "h.3.ln_2: torch.Size([1, 4, 768])\n",
      "h.3.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.3.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.3.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.3.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.3.mlp: torch.Size([1, 4, 768])\n",
      "h.3: tuple\n",
      "h.4.ln_1: torch.Size([1, 4, 768])\n",
      "h.4.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.4.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.4.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.4.attn: tuple\n",
      "h.4.ln_2: torch.Size([1, 4, 768])\n",
      "h.4.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.4.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.4.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.4.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.4.mlp: torch.Size([1, 4, 768])\n",
      "h.4: tuple\n",
      "h.5.ln_1: torch.Size([1, 4, 768])\n",
      "h.5.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.5.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.5.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.5.attn: tuple\n",
      "h.5.ln_2: torch.Size([1, 4, 768])\n",
      "h.5.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.5.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.5.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.5.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.5.mlp: torch.Size([1, 4, 768])\n",
      "h.5: tuple\n",
      "h.6.ln_1: torch.Size([1, 4, 768])\n",
      "h.6.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.6.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.6.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.6.attn: tuple\n",
      "h.6.ln_2: torch.Size([1, 4, 768])\n",
      "h.6.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.6.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.6.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.6.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.6.mlp: torch.Size([1, 4, 768])\n",
      "h.6: tuple\n",
      "h.7.ln_1: torch.Size([1, 4, 768])\n",
      "h.7.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.7.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.7.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.7.attn: tuple\n",
      "h.7.ln_2: torch.Size([1, 4, 768])\n",
      "h.7.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.7.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.7.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.7.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.7.mlp: torch.Size([1, 4, 768])\n",
      "h.7: tuple\n",
      "h.8.ln_1: torch.Size([1, 4, 768])\n",
      "h.8.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.8.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.8.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.8.attn: tuple\n",
      "h.8.ln_2: torch.Size([1, 4, 768])\n",
      "h.8.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.8.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.8.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.8.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.8.mlp: torch.Size([1, 4, 768])\n",
      "h.8: tuple\n",
      "h.9.ln_1: torch.Size([1, 4, 768])\n",
      "h.9.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.9.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.9.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.9.attn: tuple\n",
      "h.9.ln_2: torch.Size([1, 4, 768])\n",
      "h.9.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.9.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.9.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.9.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.9.mlp: torch.Size([1, 4, 768])\n",
      "h.9: tuple\n",
      "h.10.ln_1: torch.Size([1, 4, 768])\n",
      "h.10.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.10.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.10.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.10.attn: tuple\n",
      "h.10.ln_2: torch.Size([1, 4, 768])\n",
      "h.10.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.10.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.10.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.10.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.10.mlp: torch.Size([1, 4, 768])\n",
      "h.10: tuple\n",
      "h.11.ln_1: torch.Size([1, 4, 768])\n",
      "h.11.attn.c_attn: torch.Size([1, 4, 2304])\n",
      "h.11.attn.c_proj: torch.Size([1, 4, 768])\n",
      "h.11.attn.resid_dropout: torch.Size([1, 4, 768])\n",
      "h.11.attn: tuple\n",
      "h.11.ln_2: torch.Size([1, 4, 768])\n",
      "h.11.mlp.c_fc: torch.Size([1, 4, 3072])\n",
      "h.11.mlp.act: torch.Size([1, 4, 3072])\n",
      "h.11.mlp.c_proj: torch.Size([1, 4, 768])\n",
      "h.11.mlp.dropout: torch.Size([1, 4, 768])\n",
      "h.11.mlp: torch.Size([1, 4, 768])\n",
      "h.11: tuple\n",
      "ln_f: torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model\n",
    "\n",
    "# Load GPT-2 small\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# Dictionary to store all activations\n",
    "activations = {}\n",
    "\n",
    "# Generic hook for all modules\n",
    "def register_all_hooks(model):\n",
    "    \"\"\"\n",
    "    Registers a forward hook for every submodule in the model.\n",
    "    Stores the output of each module in the activations dict.\n",
    "    \"\"\"\n",
    "    def make_hook(name):\n",
    "        def hook(module, inputs, output):\n",
    "            # Only store tensors (or tuples of tensors)\n",
    "            if torch.is_tensor(output):\n",
    "                activations[name] = output.detach().cpu()\n",
    "            elif isinstance(output, (tuple, list)):\n",
    "                # handle modules that return tuples\n",
    "                activations[name] = tuple(\n",
    "                    o.detach().cpu() if torch.is_tensor(o) else o\n",
    "                    for o in output\n",
    "                )\n",
    "        return hook\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        module.register_forward_hook(make_hook(name))\n",
    "\n",
    "# Register hooks on all modules\n",
    "register_all_hooks(model)\n",
    "\n",
    "# Example input\n",
    "input_ids = torch.tensor([[464, 318, 257, 1332]])  # \"Once upon a time\"\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(input_ids)\n",
    "\n",
    "# Check all stored activations\n",
    "for k in activations:\n",
    "    print(f\"{k}: {activations[k].shape if torch.is_tensor(activations[k]) else 'tuple'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "66f5f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model\n",
    "\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def register_all_hooks(model):\n",
    "    def make_hook(name):\n",
    "        def hook(module, inputs, output):\n",
    "            if torch.is_tensor(output):\n",
    "                activations[name] = output.detach().cpu()\n",
    "            elif isinstance(output, (tuple, list)):\n",
    "                # handle modules that return tuples (e.g., attention)\n",
    "                activations[name] = tuple(\n",
    "                    o.detach().cpu() if torch.is_tensor(o) else o\n",
    "                    for o in output\n",
    "                )\n",
    "        return hook\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        module.register_forward_hook(make_hook(name))\n",
    "\n",
    "register_all_hooks(model)\n",
    "\n",
    "# Example run\n",
    "input_ids = torch.tensor([[464, 318, 257, 1332]])  # \"Once upon a time\"\n",
    "with torch.no_grad():\n",
    "    model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "3f975acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte\n",
      "wpe\n",
      "drop\n",
      "h.0.ln_1\n",
      "h.0.attn.c_attn\n",
      "h.0.attn.c_proj\n",
      "h.0.attn.resid_dropout\n",
      "h.0.attn\n",
      "h.0.ln_2\n",
      "h.0.mlp.c_fc\n",
      "h.0.mlp.act\n",
      "h.0.mlp.c_proj\n",
      "h.0.mlp.dropout\n",
      "h.0.mlp\n",
      "h.0\n",
      "h.1.ln_1\n",
      "h.1.attn.c_attn\n",
      "h.1.attn.c_proj\n",
      "h.1.attn.resid_dropout\n",
      "h.1.attn\n",
      "h.1.ln_2\n",
      "h.1.mlp.c_fc\n",
      "h.1.mlp.act\n",
      "h.1.mlp.c_proj\n",
      "h.1.mlp.dropout\n",
      "h.1.mlp\n",
      "h.1\n",
      "h.2.ln_1\n",
      "h.2.attn.c_attn\n",
      "h.2.attn.c_proj\n",
      "h.2.attn.resid_dropout\n",
      "h.2.attn\n",
      "h.2.ln_2\n",
      "h.2.mlp.c_fc\n",
      "h.2.mlp.act\n",
      "h.2.mlp.c_proj\n",
      "h.2.mlp.dropout\n",
      "h.2.mlp\n",
      "h.2\n",
      "h.3.ln_1\n",
      "h.3.attn.c_attn\n",
      "h.3.attn.c_proj\n",
      "h.3.attn.resid_dropout\n",
      "h.3.attn\n",
      "h.3.ln_2\n",
      "h.3.mlp.c_fc\n",
      "h.3.mlp.act\n",
      "h.3.mlp.c_proj\n",
      "h.3.mlp.dropout\n",
      "h.3.mlp\n",
      "h.3\n",
      "h.4.ln_1\n",
      "h.4.attn.c_attn\n",
      "h.4.attn.c_proj\n",
      "h.4.attn.resid_dropout\n",
      "h.4.attn\n",
      "h.4.ln_2\n",
      "h.4.mlp.c_fc\n",
      "h.4.mlp.act\n",
      "h.4.mlp.c_proj\n",
      "h.4.mlp.dropout\n",
      "h.4.mlp\n",
      "h.4\n",
      "h.5.ln_1\n",
      "h.5.attn.c_attn\n",
      "h.5.attn.c_proj\n",
      "h.5.attn.resid_dropout\n",
      "h.5.attn\n",
      "h.5.ln_2\n",
      "h.5.mlp.c_fc\n",
      "h.5.mlp.act\n",
      "h.5.mlp.c_proj\n",
      "h.5.mlp.dropout\n",
      "h.5.mlp\n",
      "h.5\n",
      "h.6.ln_1\n",
      "h.6.attn.c_attn\n",
      "h.6.attn.c_proj\n",
      "h.6.attn.resid_dropout\n",
      "h.6.attn\n",
      "h.6.ln_2\n",
      "h.6.mlp.c_fc\n",
      "h.6.mlp.act\n",
      "h.6.mlp.c_proj\n",
      "h.6.mlp.dropout\n",
      "h.6.mlp\n",
      "h.6\n",
      "h.7.ln_1\n",
      "h.7.attn.c_attn\n",
      "h.7.attn.c_proj\n",
      "h.7.attn.resid_dropout\n",
      "h.7.attn\n",
      "h.7.ln_2\n",
      "h.7.mlp.c_fc\n",
      "h.7.mlp.act\n",
      "h.7.mlp.c_proj\n",
      "h.7.mlp.dropout\n",
      "h.7.mlp\n",
      "h.7\n",
      "h.8.ln_1\n",
      "h.8.attn.c_attn\n",
      "h.8.attn.c_proj\n",
      "h.8.attn.resid_dropout\n",
      "h.8.attn\n",
      "h.8.ln_2\n",
      "h.8.mlp.c_fc\n",
      "h.8.mlp.act\n",
      "h.8.mlp.c_proj\n",
      "h.8.mlp.dropout\n",
      "h.8.mlp\n",
      "h.8\n",
      "h.9.ln_1\n",
      "h.9.attn.c_attn\n",
      "h.9.attn.c_proj\n",
      "h.9.attn.resid_dropout\n",
      "h.9.attn\n",
      "h.9.ln_2\n",
      "h.9.mlp.c_fc\n",
      "h.9.mlp.act\n",
      "h.9.mlp.c_proj\n",
      "h.9.mlp.dropout\n",
      "h.9.mlp\n",
      "h.9\n",
      "h.10.ln_1\n",
      "h.10.attn.c_attn\n",
      "h.10.attn.c_proj\n",
      "h.10.attn.resid_dropout\n",
      "h.10.attn\n",
      "h.10.ln_2\n",
      "h.10.mlp.c_fc\n",
      "h.10.mlp.act\n",
      "h.10.mlp.c_proj\n",
      "h.10.mlp.dropout\n",
      "h.10.mlp\n",
      "h.10\n",
      "h.11.ln_1\n",
      "h.11.attn.c_attn\n",
      "h.11.attn.c_proj\n",
      "h.11.attn.resid_dropout\n",
      "h.11.attn\n",
      "h.11.ln_2\n",
      "h.11.mlp.c_fc\n",
      "h.11.mlp.act\n",
      "h.11.mlp.c_proj\n",
      "h.11.mlp.dropout\n",
      "h.11.mlp\n",
      "h.11\n",
      "ln_f\n"
     ]
    }
   ],
   "source": [
    "for key in activations.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "df874053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[\"h.0.ln_1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "8cf141a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.0.attn.bias\n",
      "h.0.attn.c_attn.bias\n",
      "h.0.attn.c_attn.weight\n",
      "h.0.attn.c_proj.bias\n",
      "h.0.attn.c_proj.weight\n",
      "h.0.ln_1.bias\n",
      "h.0.ln_1.weight\n",
      "h.0.ln_2.bias\n",
      "h.0.ln_2.weight\n",
      "h.0.mlp.c_fc.bias\n",
      "h.0.mlp.c_fc.weight\n",
      "h.0.mlp.c_proj.bias\n",
      "h.0.mlp.c_proj.weight\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "path = \"/Users/uonliaquat/Downloads/model.safetensors\"\n",
    "\n",
    "with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    h0_keys = [k for k in f.keys() if k.startswith(\"h.0\")]\n",
    "\n",
    "for key in h0_keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "id": "aaefd18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768]), torch.Size([768]))"
      ]
     },
     "execution_count": 1276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/uonliaquat/Downloads/model.safetensors\"\n",
    "\n",
    "with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    # ln_weight = f.get_tensor(\"h.0.ln_1.weight\")\n",
    "    # ln_bias = f.get_tensor(\"h.0.ln_1.bias\")\n",
    "    h0_ln1_weight = f.get_tensor(\"h.0.ln_1.weight\")\n",
    "    h0_ln1_bias = f.get_tensor(\"h.0.ln_1.bias\")\n",
    "\n",
    "\n",
    "h0_ln1_weight.shape, h0_ln1_bias.shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdb99f",
   "metadata": {},
   "source": [
    "## Read C Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "84265b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1512\n",
      "['h.0.attn.0.attn_score', 'h.0.attn.0.attn_score_scaled', 'h.0.attn.0.attn_weight', 'h.0.attn.0.ctx_vec', 'h.0.attn.0.k_head', 'h.0.attn.0.key_transposed', 'h.0.attn.0.q_head', 'h.0.attn.0.v_head', 'h.0.attn.1.attn_score', 'h.0.attn.1.attn_score_scaled', 'h.0.attn.1.attn_weight', 'h.0.attn.1.ctx_vec', 'h.0.attn.1.k_head', 'h.0.attn.1.key_transposed', 'h.0.attn.1.q_head', 'h.0.attn.1.v_head', 'h.0.attn.10.attn_score', 'h.0.attn.10.attn_score_scaled', 'h.0.attn.10.attn_weight', 'h.0.attn.10.ctx_vec', 'h.0.attn.10.k_head', 'h.0.attn.10.key_transposed', 'h.0.attn.10.q_head', 'h.0.attn.10.v_head', 'h.0.attn.11.attn_score', 'h.0.attn.11.attn_score_scaled', 'h.0.attn.11.attn_weight', 'h.0.attn.11.ctx_vec', 'h.0.attn.11.k_head', 'h.0.attn.11.key_transposed', 'h.0.attn.11.q_head', 'h.0.attn.11.v_head', 'h.0.attn.2.attn_score', 'h.0.attn.2.attn_score_scaled', 'h.0.attn.2.attn_weight', 'h.0.attn.2.ctx_vec', 'h.0.attn.2.k_head', 'h.0.attn.2.key_transposed', 'h.0.attn.2.q_head', 'h.0.attn.2.v_head', 'h.0.attn.3.attn_score', 'h.0.attn.3.attn_score_scaled', 'h.0.attn.3.attn_weight', 'h.0.attn.3.ctx_vec', 'h.0.attn.3.k_head', 'h.0.attn.3.key_transposed', 'h.0.attn.3.q_head', 'h.0.attn.3.v_head', 'h.0.attn.4.attn_score', 'h.0.attn.4.attn_score_scaled', 'h.0.attn.4.attn_weight', 'h.0.attn.4.ctx_vec', 'h.0.attn.4.k_head', 'h.0.attn.4.key_transposed', 'h.0.attn.4.q_head', 'h.0.attn.4.v_head', 'h.0.attn.5.attn_score', 'h.0.attn.5.attn_score_scaled', 'h.0.attn.5.attn_weight', 'h.0.attn.5.ctx_vec', 'h.0.attn.5.k_head', 'h.0.attn.5.key_transposed', 'h.0.attn.5.q_head', 'h.0.attn.5.v_head', 'h.0.attn.6.attn_score', 'h.0.attn.6.attn_score_scaled', 'h.0.attn.6.attn_weight', 'h.0.attn.6.ctx_vec', 'h.0.attn.6.k_head', 'h.0.attn.6.key_transposed', 'h.0.attn.6.q_head', 'h.0.attn.6.v_head', 'h.0.attn.7.attn_score', 'h.0.attn.7.attn_score_scaled', 'h.0.attn.7.attn_weight', 'h.0.attn.7.ctx_vec', 'h.0.attn.7.k_head', 'h.0.attn.7.key_transposed', 'h.0.attn.7.q_head', 'h.0.attn.7.v_head', 'h.0.attn.8.attn_score', 'h.0.attn.8.attn_score_scaled', 'h.0.attn.8.attn_weight', 'h.0.attn.8.ctx_vec', 'h.0.attn.8.k_head', 'h.0.attn.8.key_transposed', 'h.0.attn.8.q_head', 'h.0.attn.8.v_head', 'h.0.attn.9.attn_score', 'h.0.attn.9.attn_score_scaled', 'h.0.attn.9.attn_weight', 'h.0.attn.9.ctx_vec', 'h.0.attn.9.k_head', 'h.0.attn.9.key_transposed', 'h.0.attn.9.q_head', 'h.0.attn.9.v_head', 'h.0.attn.c_attn', 'h.0.attn.c_attn.a', 'h.0.attn.c_proj', 'h.0.attn.c_proj.a', 'h.0.attn.concat_vecs', 'h.0.attn.k', 'h.0.attn.output', 'h.0.attn.q', 'h.0.attn.v', 'h.0.input_embedding', 'h.0.ln_1', 'h.0.ln_1.mean_var', 'h.0.ln_1.x_norm', 'h.0.ln_1.x_norm_scaled', 'h.0.ln_1.x_norm_shifted', 'h.0.ln_2', 'h.0.ln_2.mean_var', 'h.0.ln_2.x_norm', 'h.0.ln_2.x_norm_scaled', 'h.0.ln_2.x_norm_shifted', 'h.0.mlp', 'h.0.mlp.c_fc', 'h.0.mlp.c_fc.a', 'h.0.mlp.c_proj', 'h.0.mlp.c_proj.a', 'h.0.mlp.gelu', 'h.0.output', 'h.0.resid.1', 'h.0.resid.2', 'h.1.attn.0.attn_score', 'h.1.attn.0.attn_score_scaled', 'h.1.attn.0.attn_weight', 'h.1.attn.0.ctx_vec', 'h.1.attn.0.k_head', 'h.1.attn.0.key_transposed', 'h.1.attn.0.q_head', 'h.1.attn.0.v_head', 'h.1.attn.1.attn_score', 'h.1.attn.1.attn_score_scaled', 'h.1.attn.1.attn_weight', 'h.1.attn.1.ctx_vec', 'h.1.attn.1.k_head', 'h.1.attn.1.key_transposed', 'h.1.attn.1.q_head', 'h.1.attn.1.v_head', 'h.1.attn.10.attn_score', 'h.1.attn.10.attn_score_scaled', 'h.1.attn.10.attn_weight', 'h.1.attn.10.ctx_vec', 'h.1.attn.10.k_head', 'h.1.attn.10.key_transposed', 'h.1.attn.10.q_head', 'h.1.attn.10.v_head', 'h.1.attn.11.attn_score', 'h.1.attn.11.attn_score_scaled', 'h.1.attn.11.attn_weight', 'h.1.attn.11.ctx_vec', 'h.1.attn.11.k_head', 'h.1.attn.11.key_transposed', 'h.1.attn.11.q_head', 'h.1.attn.11.v_head', 'h.1.attn.2.attn_score', 'h.1.attn.2.attn_score_scaled', 'h.1.attn.2.attn_weight', 'h.1.attn.2.ctx_vec', 'h.1.attn.2.k_head', 'h.1.attn.2.key_transposed', 'h.1.attn.2.q_head', 'h.1.attn.2.v_head', 'h.1.attn.3.attn_score', 'h.1.attn.3.attn_score_scaled', 'h.1.attn.3.attn_weight', 'h.1.attn.3.ctx_vec', 'h.1.attn.3.k_head', 'h.1.attn.3.key_transposed', 'h.1.attn.3.q_head', 'h.1.attn.3.v_head', 'h.1.attn.4.attn_score', 'h.1.attn.4.attn_score_scaled', 'h.1.attn.4.attn_weight', 'h.1.attn.4.ctx_vec', 'h.1.attn.4.k_head', 'h.1.attn.4.key_transposed', 'h.1.attn.4.q_head', 'h.1.attn.4.v_head', 'h.1.attn.5.attn_score', 'h.1.attn.5.attn_score_scaled', 'h.1.attn.5.attn_weight', 'h.1.attn.5.ctx_vec', 'h.1.attn.5.k_head', 'h.1.attn.5.key_transposed', 'h.1.attn.5.q_head', 'h.1.attn.5.v_head', 'h.1.attn.6.attn_score', 'h.1.attn.6.attn_score_scaled', 'h.1.attn.6.attn_weight', 'h.1.attn.6.ctx_vec', 'h.1.attn.6.k_head', 'h.1.attn.6.key_transposed', 'h.1.attn.6.q_head', 'h.1.attn.6.v_head', 'h.1.attn.7.attn_score', 'h.1.attn.7.attn_score_scaled', 'h.1.attn.7.attn_weight', 'h.1.attn.7.ctx_vec', 'h.1.attn.7.k_head', 'h.1.attn.7.key_transposed', 'h.1.attn.7.q_head', 'h.1.attn.7.v_head', 'h.1.attn.8.attn_score', 'h.1.attn.8.attn_score_scaled', 'h.1.attn.8.attn_weight', 'h.1.attn.8.ctx_vec', 'h.1.attn.8.k_head', 'h.1.attn.8.key_transposed', 'h.1.attn.8.q_head', 'h.1.attn.8.v_head', 'h.1.attn.9.attn_score', 'h.1.attn.9.attn_score_scaled', 'h.1.attn.9.attn_weight', 'h.1.attn.9.ctx_vec', 'h.1.attn.9.k_head', 'h.1.attn.9.key_transposed', 'h.1.attn.9.q_head', 'h.1.attn.9.v_head', 'h.1.attn.c_attn', 'h.1.attn.c_attn.a', 'h.1.attn.c_proj', 'h.1.attn.c_proj.a', 'h.1.attn.concat_vecs', 'h.1.attn.k', 'h.1.attn.output', 'h.1.attn.q', 'h.1.attn.v', 'h.1.input_embedding', 'h.1.ln_1', 'h.1.ln_1.mean_var', 'h.1.ln_1.x_norm', 'h.1.ln_1.x_norm_scaled', 'h.1.ln_1.x_norm_shifted', 'h.1.ln_2', 'h.1.ln_2.mean_var', 'h.1.ln_2.x_norm', 'h.1.ln_2.x_norm_scaled', 'h.1.ln_2.x_norm_shifted', 'h.1.mlp', 'h.1.mlp.c_fc', 'h.1.mlp.c_fc.a', 'h.1.mlp.c_proj', 'h.1.mlp.c_proj.a', 'h.1.mlp.gelu', 'h.1.output', 'h.1.resid.1', 'h.1.resid.2', 'h.10.attn.0.attn_score', 'h.10.attn.0.attn_score_scaled', 'h.10.attn.0.attn_weight', 'h.10.attn.0.ctx_vec', 'h.10.attn.0.k_head', 'h.10.attn.0.key_transposed', 'h.10.attn.0.q_head', 'h.10.attn.0.v_head', 'h.10.attn.1.attn_score', 'h.10.attn.1.attn_score_scaled', 'h.10.attn.1.attn_weight', 'h.10.attn.1.ctx_vec', 'h.10.attn.1.k_head', 'h.10.attn.1.key_transposed', 'h.10.attn.1.q_head', 'h.10.attn.1.v_head', 'h.10.attn.10.attn_score', 'h.10.attn.10.attn_score_scaled', 'h.10.attn.10.attn_weight', 'h.10.attn.10.ctx_vec', 'h.10.attn.10.k_head', 'h.10.attn.10.key_transposed', 'h.10.attn.10.q_head', 'h.10.attn.10.v_head', 'h.10.attn.11.attn_score', 'h.10.attn.11.attn_score_scaled', 'h.10.attn.11.attn_weight', 'h.10.attn.11.ctx_vec', 'h.10.attn.11.k_head', 'h.10.attn.11.key_transposed', 'h.10.attn.11.q_head', 'h.10.attn.11.v_head', 'h.10.attn.2.attn_score', 'h.10.attn.2.attn_score_scaled', 'h.10.attn.2.attn_weight', 'h.10.attn.2.ctx_vec', 'h.10.attn.2.k_head', 'h.10.attn.2.key_transposed', 'h.10.attn.2.q_head', 'h.10.attn.2.v_head', 'h.10.attn.3.attn_score', 'h.10.attn.3.attn_score_scaled', 'h.10.attn.3.attn_weight', 'h.10.attn.3.ctx_vec', 'h.10.attn.3.k_head', 'h.10.attn.3.key_transposed', 'h.10.attn.3.q_head', 'h.10.attn.3.v_head', 'h.10.attn.4.attn_score', 'h.10.attn.4.attn_score_scaled', 'h.10.attn.4.attn_weight', 'h.10.attn.4.ctx_vec', 'h.10.attn.4.k_head', 'h.10.attn.4.key_transposed', 'h.10.attn.4.q_head', 'h.10.attn.4.v_head', 'h.10.attn.5.attn_score', 'h.10.attn.5.attn_score_scaled', 'h.10.attn.5.attn_weight', 'h.10.attn.5.ctx_vec', 'h.10.attn.5.k_head', 'h.10.attn.5.key_transposed', 'h.10.attn.5.q_head', 'h.10.attn.5.v_head', 'h.10.attn.6.attn_score', 'h.10.attn.6.attn_score_scaled', 'h.10.attn.6.attn_weight', 'h.10.attn.6.ctx_vec', 'h.10.attn.6.k_head', 'h.10.attn.6.key_transposed', 'h.10.attn.6.q_head', 'h.10.attn.6.v_head', 'h.10.attn.7.attn_score', 'h.10.attn.7.attn_score_scaled', 'h.10.attn.7.attn_weight', 'h.10.attn.7.ctx_vec', 'h.10.attn.7.k_head', 'h.10.attn.7.key_transposed', 'h.10.attn.7.q_head', 'h.10.attn.7.v_head', 'h.10.attn.8.attn_score', 'h.10.attn.8.attn_score_scaled', 'h.10.attn.8.attn_weight', 'h.10.attn.8.ctx_vec', 'h.10.attn.8.k_head', 'h.10.attn.8.key_transposed', 'h.10.attn.8.q_head', 'h.10.attn.8.v_head', 'h.10.attn.9.attn_score', 'h.10.attn.9.attn_score_scaled', 'h.10.attn.9.attn_weight', 'h.10.attn.9.ctx_vec', 'h.10.attn.9.k_head', 'h.10.attn.9.key_transposed', 'h.10.attn.9.q_head', 'h.10.attn.9.v_head', 'h.10.attn.c_attn', 'h.10.attn.c_attn.a', 'h.10.attn.c_proj', 'h.10.attn.c_proj.a', 'h.10.attn.concat_vecs', 'h.10.attn.k', 'h.10.attn.output', 'h.10.attn.q', 'h.10.attn.v', 'h.10.input_embedding', 'h.10.ln_1', 'h.10.ln_1.mean_var', 'h.10.ln_1.x_norm', 'h.10.ln_1.x_norm_scaled', 'h.10.ln_1.x_norm_shifted', 'h.10.ln_2', 'h.10.ln_2.mean_var', 'h.10.ln_2.x_norm', 'h.10.ln_2.x_norm_scaled', 'h.10.ln_2.x_norm_shifted', 'h.10.mlp', 'h.10.mlp.c_fc', 'h.10.mlp.c_fc.a', 'h.10.mlp.c_proj', 'h.10.mlp.c_proj.a', 'h.10.mlp.gelu', 'h.10.output', 'h.10.resid.1', 'h.10.resid.2', 'h.11.attn.0.attn_score', 'h.11.attn.0.attn_score_scaled', 'h.11.attn.0.attn_weight', 'h.11.attn.0.ctx_vec', 'h.11.attn.0.k_head', 'h.11.attn.0.key_transposed', 'h.11.attn.0.q_head', 'h.11.attn.0.v_head', 'h.11.attn.1.attn_score', 'h.11.attn.1.attn_score_scaled', 'h.11.attn.1.attn_weight', 'h.11.attn.1.ctx_vec', 'h.11.attn.1.k_head', 'h.11.attn.1.key_transposed', 'h.11.attn.1.q_head', 'h.11.attn.1.v_head', 'h.11.attn.10.attn_score', 'h.11.attn.10.attn_score_scaled', 'h.11.attn.10.attn_weight', 'h.11.attn.10.ctx_vec', 'h.11.attn.10.k_head', 'h.11.attn.10.key_transposed', 'h.11.attn.10.q_head', 'h.11.attn.10.v_head', 'h.11.attn.11.attn_score', 'h.11.attn.11.attn_score_scaled', 'h.11.attn.11.attn_weight', 'h.11.attn.11.ctx_vec', 'h.11.attn.11.k_head', 'h.11.attn.11.key_transposed', 'h.11.attn.11.q_head', 'h.11.attn.11.v_head', 'h.11.attn.2.attn_score', 'h.11.attn.2.attn_score_scaled', 'h.11.attn.2.attn_weight', 'h.11.attn.2.ctx_vec', 'h.11.attn.2.k_head', 'h.11.attn.2.key_transposed', 'h.11.attn.2.q_head', 'h.11.attn.2.v_head', 'h.11.attn.3.attn_score', 'h.11.attn.3.attn_score_scaled', 'h.11.attn.3.attn_weight', 'h.11.attn.3.ctx_vec', 'h.11.attn.3.k_head', 'h.11.attn.3.key_transposed', 'h.11.attn.3.q_head', 'h.11.attn.3.v_head', 'h.11.attn.4.attn_score', 'h.11.attn.4.attn_score_scaled', 'h.11.attn.4.attn_weight', 'h.11.attn.4.ctx_vec', 'h.11.attn.4.k_head', 'h.11.attn.4.key_transposed', 'h.11.attn.4.q_head', 'h.11.attn.4.v_head', 'h.11.attn.5.attn_score', 'h.11.attn.5.attn_score_scaled', 'h.11.attn.5.attn_weight', 'h.11.attn.5.ctx_vec', 'h.11.attn.5.k_head', 'h.11.attn.5.key_transposed', 'h.11.attn.5.q_head', 'h.11.attn.5.v_head', 'h.11.attn.6.attn_score', 'h.11.attn.6.attn_score_scaled', 'h.11.attn.6.attn_weight', 'h.11.attn.6.ctx_vec', 'h.11.attn.6.k_head', 'h.11.attn.6.key_transposed', 'h.11.attn.6.q_head', 'h.11.attn.6.v_head', 'h.11.attn.7.attn_score', 'h.11.attn.7.attn_score_scaled', 'h.11.attn.7.attn_weight', 'h.11.attn.7.ctx_vec', 'h.11.attn.7.k_head', 'h.11.attn.7.key_transposed', 'h.11.attn.7.q_head', 'h.11.attn.7.v_head', 'h.11.attn.8.attn_score', 'h.11.attn.8.attn_score_scaled', 'h.11.attn.8.attn_weight', 'h.11.attn.8.ctx_vec', 'h.11.attn.8.k_head', 'h.11.attn.8.key_transposed', 'h.11.attn.8.q_head', 'h.11.attn.8.v_head', 'h.11.attn.9.attn_score', 'h.11.attn.9.attn_score_scaled', 'h.11.attn.9.attn_weight', 'h.11.attn.9.ctx_vec', 'h.11.attn.9.k_head', 'h.11.attn.9.key_transposed', 'h.11.attn.9.q_head', 'h.11.attn.9.v_head', 'h.11.attn.c_attn', 'h.11.attn.c_attn.a', 'h.11.attn.c_proj', 'h.11.attn.c_proj.a', 'h.11.attn.concat_vecs', 'h.11.attn.k', 'h.11.attn.output', 'h.11.attn.q', 'h.11.attn.v', 'h.11.input_embedding', 'h.11.ln_1', 'h.11.ln_1.mean_var', 'h.11.ln_1.x_norm', 'h.11.ln_1.x_norm_scaled', 'h.11.ln_1.x_norm_shifted', 'h.11.ln_2', 'h.11.ln_2.mean_var', 'h.11.ln_2.x_norm', 'h.11.ln_2.x_norm_scaled', 'h.11.ln_2.x_norm_shifted', 'h.11.mlp', 'h.11.mlp.c_fc', 'h.11.mlp.c_fc.a', 'h.11.mlp.c_proj', 'h.11.mlp.c_proj.a', 'h.11.mlp.gelu', 'h.11.output', 'h.11.resid.1', 'h.11.resid.2', 'h.12.input_embedding', 'h.2.attn.0.attn_score', 'h.2.attn.0.attn_score_scaled', 'h.2.attn.0.attn_weight', 'h.2.attn.0.ctx_vec', 'h.2.attn.0.k_head', 'h.2.attn.0.key_transposed', 'h.2.attn.0.q_head', 'h.2.attn.0.v_head', 'h.2.attn.1.attn_score', 'h.2.attn.1.attn_score_scaled', 'h.2.attn.1.attn_weight', 'h.2.attn.1.ctx_vec', 'h.2.attn.1.k_head', 'h.2.attn.1.key_transposed', 'h.2.attn.1.q_head', 'h.2.attn.1.v_head', 'h.2.attn.10.attn_score', 'h.2.attn.10.attn_score_scaled', 'h.2.attn.10.attn_weight', 'h.2.attn.10.ctx_vec', 'h.2.attn.10.k_head', 'h.2.attn.10.key_transposed', 'h.2.attn.10.q_head', 'h.2.attn.10.v_head', 'h.2.attn.11.attn_score', 'h.2.attn.11.attn_score_scaled', 'h.2.attn.11.attn_weight', 'h.2.attn.11.ctx_vec', 'h.2.attn.11.k_head', 'h.2.attn.11.key_transposed', 'h.2.attn.11.q_head', 'h.2.attn.11.v_head', 'h.2.attn.2.attn_score', 'h.2.attn.2.attn_score_scaled', 'h.2.attn.2.attn_weight', 'h.2.attn.2.ctx_vec', 'h.2.attn.2.k_head', 'h.2.attn.2.key_transposed', 'h.2.attn.2.q_head', 'h.2.attn.2.v_head', 'h.2.attn.3.attn_score', 'h.2.attn.3.attn_score_scaled', 'h.2.attn.3.attn_weight', 'h.2.attn.3.ctx_vec', 'h.2.attn.3.k_head', 'h.2.attn.3.key_transposed', 'h.2.attn.3.q_head', 'h.2.attn.3.v_head', 'h.2.attn.4.attn_score', 'h.2.attn.4.attn_score_scaled', 'h.2.attn.4.attn_weight', 'h.2.attn.4.ctx_vec', 'h.2.attn.4.k_head', 'h.2.attn.4.key_transposed', 'h.2.attn.4.q_head', 'h.2.attn.4.v_head', 'h.2.attn.5.attn_score', 'h.2.attn.5.attn_score_scaled', 'h.2.attn.5.attn_weight', 'h.2.attn.5.ctx_vec', 'h.2.attn.5.k_head', 'h.2.attn.5.key_transposed', 'h.2.attn.5.q_head', 'h.2.attn.5.v_head', 'h.2.attn.6.attn_score', 'h.2.attn.6.attn_score_scaled', 'h.2.attn.6.attn_weight', 'h.2.attn.6.ctx_vec', 'h.2.attn.6.k_head', 'h.2.attn.6.key_transposed', 'h.2.attn.6.q_head', 'h.2.attn.6.v_head', 'h.2.attn.7.attn_score', 'h.2.attn.7.attn_score_scaled', 'h.2.attn.7.attn_weight', 'h.2.attn.7.ctx_vec', 'h.2.attn.7.k_head', 'h.2.attn.7.key_transposed', 'h.2.attn.7.q_head', 'h.2.attn.7.v_head', 'h.2.attn.8.attn_score', 'h.2.attn.8.attn_score_scaled', 'h.2.attn.8.attn_weight', 'h.2.attn.8.ctx_vec', 'h.2.attn.8.k_head', 'h.2.attn.8.key_transposed', 'h.2.attn.8.q_head', 'h.2.attn.8.v_head', 'h.2.attn.9.attn_score', 'h.2.attn.9.attn_score_scaled', 'h.2.attn.9.attn_weight', 'h.2.attn.9.ctx_vec', 'h.2.attn.9.k_head', 'h.2.attn.9.key_transposed', 'h.2.attn.9.q_head', 'h.2.attn.9.v_head', 'h.2.attn.c_attn', 'h.2.attn.c_attn.a', 'h.2.attn.c_proj', 'h.2.attn.c_proj.a', 'h.2.attn.concat_vecs', 'h.2.attn.k', 'h.2.attn.output', 'h.2.attn.q', 'h.2.attn.v', 'h.2.input_embedding', 'h.2.ln_1', 'h.2.ln_1.mean_var', 'h.2.ln_1.x_norm', 'h.2.ln_1.x_norm_scaled', 'h.2.ln_1.x_norm_shifted', 'h.2.ln_2', 'h.2.ln_2.mean_var', 'h.2.ln_2.x_norm', 'h.2.ln_2.x_norm_scaled', 'h.2.ln_2.x_norm_shifted', 'h.2.mlp', 'h.2.mlp.c_fc', 'h.2.mlp.c_fc.a', 'h.2.mlp.c_proj', 'h.2.mlp.c_proj.a', 'h.2.mlp.gelu', 'h.2.output', 'h.2.resid.1', 'h.2.resid.2', 'h.3.attn.0.attn_score', 'h.3.attn.0.attn_score_scaled', 'h.3.attn.0.attn_weight', 'h.3.attn.0.ctx_vec', 'h.3.attn.0.k_head', 'h.3.attn.0.key_transposed', 'h.3.attn.0.q_head', 'h.3.attn.0.v_head', 'h.3.attn.1.attn_score', 'h.3.attn.1.attn_score_scaled', 'h.3.attn.1.attn_weight', 'h.3.attn.1.ctx_vec', 'h.3.attn.1.k_head', 'h.3.attn.1.key_transposed', 'h.3.attn.1.q_head', 'h.3.attn.1.v_head', 'h.3.attn.10.attn_score', 'h.3.attn.10.attn_score_scaled', 'h.3.attn.10.attn_weight', 'h.3.attn.10.ctx_vec', 'h.3.attn.10.k_head', 'h.3.attn.10.key_transposed', 'h.3.attn.10.q_head', 'h.3.attn.10.v_head', 'h.3.attn.11.attn_score', 'h.3.attn.11.attn_score_scaled', 'h.3.attn.11.attn_weight', 'h.3.attn.11.ctx_vec', 'h.3.attn.11.k_head', 'h.3.attn.11.key_transposed', 'h.3.attn.11.q_head', 'h.3.attn.11.v_head', 'h.3.attn.2.attn_score', 'h.3.attn.2.attn_score_scaled', 'h.3.attn.2.attn_weight', 'h.3.attn.2.ctx_vec', 'h.3.attn.2.k_head', 'h.3.attn.2.key_transposed', 'h.3.attn.2.q_head', 'h.3.attn.2.v_head', 'h.3.attn.3.attn_score', 'h.3.attn.3.attn_score_scaled', 'h.3.attn.3.attn_weight', 'h.3.attn.3.ctx_vec', 'h.3.attn.3.k_head', 'h.3.attn.3.key_transposed', 'h.3.attn.3.q_head', 'h.3.attn.3.v_head', 'h.3.attn.4.attn_score', 'h.3.attn.4.attn_score_scaled', 'h.3.attn.4.attn_weight', 'h.3.attn.4.ctx_vec', 'h.3.attn.4.k_head', 'h.3.attn.4.key_transposed', 'h.3.attn.4.q_head', 'h.3.attn.4.v_head', 'h.3.attn.5.attn_score', 'h.3.attn.5.attn_score_scaled', 'h.3.attn.5.attn_weight', 'h.3.attn.5.ctx_vec', 'h.3.attn.5.k_head', 'h.3.attn.5.key_transposed', 'h.3.attn.5.q_head', 'h.3.attn.5.v_head', 'h.3.attn.6.attn_score', 'h.3.attn.6.attn_score_scaled', 'h.3.attn.6.attn_weight', 'h.3.attn.6.ctx_vec', 'h.3.attn.6.k_head', 'h.3.attn.6.key_transposed', 'h.3.attn.6.q_head', 'h.3.attn.6.v_head', 'h.3.attn.7.attn_score', 'h.3.attn.7.attn_score_scaled', 'h.3.attn.7.attn_weight', 'h.3.attn.7.ctx_vec', 'h.3.attn.7.k_head', 'h.3.attn.7.key_transposed', 'h.3.attn.7.q_head', 'h.3.attn.7.v_head', 'h.3.attn.8.attn_score', 'h.3.attn.8.attn_score_scaled', 'h.3.attn.8.attn_weight', 'h.3.attn.8.ctx_vec', 'h.3.attn.8.k_head', 'h.3.attn.8.key_transposed', 'h.3.attn.8.q_head', 'h.3.attn.8.v_head', 'h.3.attn.9.attn_score', 'h.3.attn.9.attn_score_scaled', 'h.3.attn.9.attn_weight', 'h.3.attn.9.ctx_vec', 'h.3.attn.9.k_head', 'h.3.attn.9.key_transposed', 'h.3.attn.9.q_head', 'h.3.attn.9.v_head', 'h.3.attn.c_attn', 'h.3.attn.c_attn.a', 'h.3.attn.c_proj', 'h.3.attn.c_proj.a', 'h.3.attn.concat_vecs', 'h.3.attn.k', 'h.3.attn.output', 'h.3.attn.q', 'h.3.attn.v', 'h.3.input_embedding', 'h.3.ln_1', 'h.3.ln_1.mean_var', 'h.3.ln_1.x_norm', 'h.3.ln_1.x_norm_scaled', 'h.3.ln_1.x_norm_shifted', 'h.3.ln_2', 'h.3.ln_2.mean_var', 'h.3.ln_2.x_norm', 'h.3.ln_2.x_norm_scaled', 'h.3.ln_2.x_norm_shifted', 'h.3.mlp', 'h.3.mlp.c_fc', 'h.3.mlp.c_fc.a', 'h.3.mlp.c_proj', 'h.3.mlp.c_proj.a', 'h.3.mlp.gelu', 'h.3.output', 'h.3.resid.1', 'h.3.resid.2', 'h.4.attn.0.attn_score', 'h.4.attn.0.attn_score_scaled', 'h.4.attn.0.attn_weight', 'h.4.attn.0.ctx_vec', 'h.4.attn.0.k_head', 'h.4.attn.0.key_transposed', 'h.4.attn.0.q_head', 'h.4.attn.0.v_head', 'h.4.attn.1.attn_score', 'h.4.attn.1.attn_score_scaled', 'h.4.attn.1.attn_weight', 'h.4.attn.1.ctx_vec', 'h.4.attn.1.k_head', 'h.4.attn.1.key_transposed', 'h.4.attn.1.q_head', 'h.4.attn.1.v_head', 'h.4.attn.10.attn_score', 'h.4.attn.10.attn_score_scaled', 'h.4.attn.10.attn_weight', 'h.4.attn.10.ctx_vec', 'h.4.attn.10.k_head', 'h.4.attn.10.key_transposed', 'h.4.attn.10.q_head', 'h.4.attn.10.v_head', 'h.4.attn.11.attn_score', 'h.4.attn.11.attn_score_scaled', 'h.4.attn.11.attn_weight', 'h.4.attn.11.ctx_vec', 'h.4.attn.11.k_head', 'h.4.attn.11.key_transposed', 'h.4.attn.11.q_head', 'h.4.attn.11.v_head', 'h.4.attn.2.attn_score', 'h.4.attn.2.attn_score_scaled', 'h.4.attn.2.attn_weight', 'h.4.attn.2.ctx_vec', 'h.4.attn.2.k_head', 'h.4.attn.2.key_transposed', 'h.4.attn.2.q_head', 'h.4.attn.2.v_head', 'h.4.attn.3.attn_score', 'h.4.attn.3.attn_score_scaled', 'h.4.attn.3.attn_weight', 'h.4.attn.3.ctx_vec', 'h.4.attn.3.k_head', 'h.4.attn.3.key_transposed', 'h.4.attn.3.q_head', 'h.4.attn.3.v_head', 'h.4.attn.4.attn_score', 'h.4.attn.4.attn_score_scaled', 'h.4.attn.4.attn_weight', 'h.4.attn.4.ctx_vec', 'h.4.attn.4.k_head', 'h.4.attn.4.key_transposed', 'h.4.attn.4.q_head', 'h.4.attn.4.v_head', 'h.4.attn.5.attn_score', 'h.4.attn.5.attn_score_scaled', 'h.4.attn.5.attn_weight', 'h.4.attn.5.ctx_vec', 'h.4.attn.5.k_head', 'h.4.attn.5.key_transposed', 'h.4.attn.5.q_head', 'h.4.attn.5.v_head', 'h.4.attn.6.attn_score', 'h.4.attn.6.attn_score_scaled', 'h.4.attn.6.attn_weight', 'h.4.attn.6.ctx_vec', 'h.4.attn.6.k_head', 'h.4.attn.6.key_transposed', 'h.4.attn.6.q_head', 'h.4.attn.6.v_head', 'h.4.attn.7.attn_score', 'h.4.attn.7.attn_score_scaled', 'h.4.attn.7.attn_weight', 'h.4.attn.7.ctx_vec', 'h.4.attn.7.k_head', 'h.4.attn.7.key_transposed', 'h.4.attn.7.q_head', 'h.4.attn.7.v_head', 'h.4.attn.8.attn_score', 'h.4.attn.8.attn_score_scaled', 'h.4.attn.8.attn_weight', 'h.4.attn.8.ctx_vec', 'h.4.attn.8.k_head', 'h.4.attn.8.key_transposed', 'h.4.attn.8.q_head', 'h.4.attn.8.v_head', 'h.4.attn.9.attn_score', 'h.4.attn.9.attn_score_scaled', 'h.4.attn.9.attn_weight', 'h.4.attn.9.ctx_vec', 'h.4.attn.9.k_head', 'h.4.attn.9.key_transposed', 'h.4.attn.9.q_head', 'h.4.attn.9.v_head', 'h.4.attn.c_attn', 'h.4.attn.c_attn.a', 'h.4.attn.c_proj', 'h.4.attn.c_proj.a', 'h.4.attn.concat_vecs', 'h.4.attn.k', 'h.4.attn.output', 'h.4.attn.q', 'h.4.attn.v', 'h.4.input_embedding', 'h.4.ln_1', 'h.4.ln_1.mean_var', 'h.4.ln_1.x_norm', 'h.4.ln_1.x_norm_scaled', 'h.4.ln_1.x_norm_shifted', 'h.4.ln_2', 'h.4.ln_2.mean_var', 'h.4.ln_2.x_norm', 'h.4.ln_2.x_norm_scaled', 'h.4.ln_2.x_norm_shifted', 'h.4.mlp', 'h.4.mlp.c_fc', 'h.4.mlp.c_fc.a', 'h.4.mlp.c_proj', 'h.4.mlp.c_proj.a', 'h.4.mlp.gelu', 'h.4.output', 'h.4.resid.1', 'h.4.resid.2', 'h.5.attn.0.attn_score', 'h.5.attn.0.attn_score_scaled', 'h.5.attn.0.attn_weight', 'h.5.attn.0.ctx_vec', 'h.5.attn.0.k_head', 'h.5.attn.0.key_transposed', 'h.5.attn.0.q_head', 'h.5.attn.0.v_head', 'h.5.attn.1.attn_score', 'h.5.attn.1.attn_score_scaled', 'h.5.attn.1.attn_weight', 'h.5.attn.1.ctx_vec', 'h.5.attn.1.k_head', 'h.5.attn.1.key_transposed', 'h.5.attn.1.q_head', 'h.5.attn.1.v_head', 'h.5.attn.10.attn_score', 'h.5.attn.10.attn_score_scaled', 'h.5.attn.10.attn_weight', 'h.5.attn.10.ctx_vec', 'h.5.attn.10.k_head', 'h.5.attn.10.key_transposed', 'h.5.attn.10.q_head', 'h.5.attn.10.v_head', 'h.5.attn.11.attn_score', 'h.5.attn.11.attn_score_scaled', 'h.5.attn.11.attn_weight', 'h.5.attn.11.ctx_vec', 'h.5.attn.11.k_head', 'h.5.attn.11.key_transposed', 'h.5.attn.11.q_head', 'h.5.attn.11.v_head', 'h.5.attn.2.attn_score', 'h.5.attn.2.attn_score_scaled', 'h.5.attn.2.attn_weight', 'h.5.attn.2.ctx_vec', 'h.5.attn.2.k_head', 'h.5.attn.2.key_transposed', 'h.5.attn.2.q_head', 'h.5.attn.2.v_head', 'h.5.attn.3.attn_score', 'h.5.attn.3.attn_score_scaled', 'h.5.attn.3.attn_weight', 'h.5.attn.3.ctx_vec', 'h.5.attn.3.k_head', 'h.5.attn.3.key_transposed', 'h.5.attn.3.q_head', 'h.5.attn.3.v_head', 'h.5.attn.4.attn_score', 'h.5.attn.4.attn_score_scaled', 'h.5.attn.4.attn_weight', 'h.5.attn.4.ctx_vec', 'h.5.attn.4.k_head', 'h.5.attn.4.key_transposed', 'h.5.attn.4.q_head', 'h.5.attn.4.v_head', 'h.5.attn.5.attn_score', 'h.5.attn.5.attn_score_scaled', 'h.5.attn.5.attn_weight', 'h.5.attn.5.ctx_vec', 'h.5.attn.5.k_head', 'h.5.attn.5.key_transposed', 'h.5.attn.5.q_head', 'h.5.attn.5.v_head', 'h.5.attn.6.attn_score', 'h.5.attn.6.attn_score_scaled', 'h.5.attn.6.attn_weight', 'h.5.attn.6.ctx_vec', 'h.5.attn.6.k_head', 'h.5.attn.6.key_transposed', 'h.5.attn.6.q_head', 'h.5.attn.6.v_head', 'h.5.attn.7.attn_score', 'h.5.attn.7.attn_score_scaled', 'h.5.attn.7.attn_weight', 'h.5.attn.7.ctx_vec', 'h.5.attn.7.k_head', 'h.5.attn.7.key_transposed', 'h.5.attn.7.q_head', 'h.5.attn.7.v_head', 'h.5.attn.8.attn_score', 'h.5.attn.8.attn_score_scaled', 'h.5.attn.8.attn_weight', 'h.5.attn.8.ctx_vec', 'h.5.attn.8.k_head', 'h.5.attn.8.key_transposed', 'h.5.attn.8.q_head', 'h.5.attn.8.v_head', 'h.5.attn.9.attn_score', 'h.5.attn.9.attn_score_scaled', 'h.5.attn.9.attn_weight', 'h.5.attn.9.ctx_vec', 'h.5.attn.9.k_head', 'h.5.attn.9.key_transposed', 'h.5.attn.9.q_head', 'h.5.attn.9.v_head', 'h.5.attn.c_attn', 'h.5.attn.c_attn.a', 'h.5.attn.c_proj', 'h.5.attn.c_proj.a', 'h.5.attn.concat_vecs', 'h.5.attn.k', 'h.5.attn.output', 'h.5.attn.q', 'h.5.attn.v', 'h.5.input_embedding', 'h.5.ln_1', 'h.5.ln_1.mean_var', 'h.5.ln_1.x_norm', 'h.5.ln_1.x_norm_scaled', 'h.5.ln_1.x_norm_shifted', 'h.5.ln_2', 'h.5.ln_2.mean_var', 'h.5.ln_2.x_norm', 'h.5.ln_2.x_norm_scaled', 'h.5.ln_2.x_norm_shifted', 'h.5.mlp', 'h.5.mlp.c_fc', 'h.5.mlp.c_fc.a', 'h.5.mlp.c_proj', 'h.5.mlp.c_proj.a', 'h.5.mlp.gelu', 'h.5.output', 'h.5.resid.1', 'h.5.resid.2', 'h.6.attn.0.attn_score', 'h.6.attn.0.attn_score_scaled', 'h.6.attn.0.attn_weight', 'h.6.attn.0.ctx_vec', 'h.6.attn.0.k_head', 'h.6.attn.0.key_transposed', 'h.6.attn.0.q_head', 'h.6.attn.0.v_head', 'h.6.attn.1.attn_score', 'h.6.attn.1.attn_score_scaled', 'h.6.attn.1.attn_weight', 'h.6.attn.1.ctx_vec', 'h.6.attn.1.k_head', 'h.6.attn.1.key_transposed', 'h.6.attn.1.q_head', 'h.6.attn.1.v_head', 'h.6.attn.10.attn_score', 'h.6.attn.10.attn_score_scaled', 'h.6.attn.10.attn_weight', 'h.6.attn.10.ctx_vec', 'h.6.attn.10.k_head', 'h.6.attn.10.key_transposed', 'h.6.attn.10.q_head', 'h.6.attn.10.v_head', 'h.6.attn.11.attn_score', 'h.6.attn.11.attn_score_scaled', 'h.6.attn.11.attn_weight', 'h.6.attn.11.ctx_vec', 'h.6.attn.11.k_head', 'h.6.attn.11.key_transposed', 'h.6.attn.11.q_head', 'h.6.attn.11.v_head', 'h.6.attn.2.attn_score', 'h.6.attn.2.attn_score_scaled', 'h.6.attn.2.attn_weight', 'h.6.attn.2.ctx_vec', 'h.6.attn.2.k_head', 'h.6.attn.2.key_transposed', 'h.6.attn.2.q_head', 'h.6.attn.2.v_head', 'h.6.attn.3.attn_score', 'h.6.attn.3.attn_score_scaled', 'h.6.attn.3.attn_weight', 'h.6.attn.3.ctx_vec', 'h.6.attn.3.k_head', 'h.6.attn.3.key_transposed', 'h.6.attn.3.q_head', 'h.6.attn.3.v_head', 'h.6.attn.4.attn_score', 'h.6.attn.4.attn_score_scaled', 'h.6.attn.4.attn_weight', 'h.6.attn.4.ctx_vec', 'h.6.attn.4.k_head', 'h.6.attn.4.key_transposed', 'h.6.attn.4.q_head', 'h.6.attn.4.v_head', 'h.6.attn.5.attn_score', 'h.6.attn.5.attn_score_scaled', 'h.6.attn.5.attn_weight', 'h.6.attn.5.ctx_vec', 'h.6.attn.5.k_head', 'h.6.attn.5.key_transposed', 'h.6.attn.5.q_head', 'h.6.attn.5.v_head', 'h.6.attn.6.attn_score', 'h.6.attn.6.attn_score_scaled', 'h.6.attn.6.attn_weight', 'h.6.attn.6.ctx_vec', 'h.6.attn.6.k_head', 'h.6.attn.6.key_transposed', 'h.6.attn.6.q_head', 'h.6.attn.6.v_head', 'h.6.attn.7.attn_score', 'h.6.attn.7.attn_score_scaled', 'h.6.attn.7.attn_weight', 'h.6.attn.7.ctx_vec', 'h.6.attn.7.k_head', 'h.6.attn.7.key_transposed', 'h.6.attn.7.q_head', 'h.6.attn.7.v_head', 'h.6.attn.8.attn_score', 'h.6.attn.8.attn_score_scaled', 'h.6.attn.8.attn_weight', 'h.6.attn.8.ctx_vec', 'h.6.attn.8.k_head', 'h.6.attn.8.key_transposed', 'h.6.attn.8.q_head', 'h.6.attn.8.v_head', 'h.6.attn.9.attn_score', 'h.6.attn.9.attn_score_scaled', 'h.6.attn.9.attn_weight', 'h.6.attn.9.ctx_vec', 'h.6.attn.9.k_head', 'h.6.attn.9.key_transposed', 'h.6.attn.9.q_head', 'h.6.attn.9.v_head', 'h.6.attn.c_attn', 'h.6.attn.c_attn.a', 'h.6.attn.c_proj', 'h.6.attn.c_proj.a', 'h.6.attn.concat_vecs', 'h.6.attn.k', 'h.6.attn.output', 'h.6.attn.q', 'h.6.attn.v', 'h.6.input_embedding', 'h.6.ln_1', 'h.6.ln_1.mean_var', 'h.6.ln_1.x_norm', 'h.6.ln_1.x_norm_scaled', 'h.6.ln_1.x_norm_shifted', 'h.6.ln_2', 'h.6.ln_2.mean_var', 'h.6.ln_2.x_norm', 'h.6.ln_2.x_norm_scaled', 'h.6.ln_2.x_norm_shifted', 'h.6.mlp', 'h.6.mlp.c_fc', 'h.6.mlp.c_fc.a', 'h.6.mlp.c_proj', 'h.6.mlp.c_proj.a', 'h.6.mlp.gelu', 'h.6.output', 'h.6.resid.1', 'h.6.resid.2', 'h.7.attn.0.attn_score', 'h.7.attn.0.attn_score_scaled', 'h.7.attn.0.attn_weight', 'h.7.attn.0.ctx_vec', 'h.7.attn.0.k_head', 'h.7.attn.0.key_transposed', 'h.7.attn.0.q_head', 'h.7.attn.0.v_head', 'h.7.attn.1.attn_score', 'h.7.attn.1.attn_score_scaled', 'h.7.attn.1.attn_weight', 'h.7.attn.1.ctx_vec', 'h.7.attn.1.k_head', 'h.7.attn.1.key_transposed', 'h.7.attn.1.q_head', 'h.7.attn.1.v_head', 'h.7.attn.10.attn_score', 'h.7.attn.10.attn_score_scaled', 'h.7.attn.10.attn_weight', 'h.7.attn.10.ctx_vec', 'h.7.attn.10.k_head', 'h.7.attn.10.key_transposed', 'h.7.attn.10.q_head', 'h.7.attn.10.v_head', 'h.7.attn.11.attn_score', 'h.7.attn.11.attn_score_scaled', 'h.7.attn.11.attn_weight', 'h.7.attn.11.ctx_vec', 'h.7.attn.11.k_head', 'h.7.attn.11.key_transposed', 'h.7.attn.11.q_head', 'h.7.attn.11.v_head', 'h.7.attn.2.attn_score', 'h.7.attn.2.attn_score_scaled', 'h.7.attn.2.attn_weight', 'h.7.attn.2.ctx_vec', 'h.7.attn.2.k_head', 'h.7.attn.2.key_transposed', 'h.7.attn.2.q_head', 'h.7.attn.2.v_head', 'h.7.attn.3.attn_score', 'h.7.attn.3.attn_score_scaled', 'h.7.attn.3.attn_weight', 'h.7.attn.3.ctx_vec', 'h.7.attn.3.k_head', 'h.7.attn.3.key_transposed', 'h.7.attn.3.q_head', 'h.7.attn.3.v_head', 'h.7.attn.4.attn_score', 'h.7.attn.4.attn_score_scaled', 'h.7.attn.4.attn_weight', 'h.7.attn.4.ctx_vec', 'h.7.attn.4.k_head', 'h.7.attn.4.key_transposed', 'h.7.attn.4.q_head', 'h.7.attn.4.v_head', 'h.7.attn.5.attn_score', 'h.7.attn.5.attn_score_scaled', 'h.7.attn.5.attn_weight', 'h.7.attn.5.ctx_vec', 'h.7.attn.5.k_head', 'h.7.attn.5.key_transposed', 'h.7.attn.5.q_head', 'h.7.attn.5.v_head', 'h.7.attn.6.attn_score', 'h.7.attn.6.attn_score_scaled', 'h.7.attn.6.attn_weight', 'h.7.attn.6.ctx_vec', 'h.7.attn.6.k_head', 'h.7.attn.6.key_transposed', 'h.7.attn.6.q_head', 'h.7.attn.6.v_head', 'h.7.attn.7.attn_score', 'h.7.attn.7.attn_score_scaled', 'h.7.attn.7.attn_weight', 'h.7.attn.7.ctx_vec', 'h.7.attn.7.k_head', 'h.7.attn.7.key_transposed', 'h.7.attn.7.q_head', 'h.7.attn.7.v_head', 'h.7.attn.8.attn_score', 'h.7.attn.8.attn_score_scaled', 'h.7.attn.8.attn_weight', 'h.7.attn.8.ctx_vec', 'h.7.attn.8.k_head', 'h.7.attn.8.key_transposed', 'h.7.attn.8.q_head', 'h.7.attn.8.v_head', 'h.7.attn.9.attn_score', 'h.7.attn.9.attn_score_scaled', 'h.7.attn.9.attn_weight', 'h.7.attn.9.ctx_vec', 'h.7.attn.9.k_head', 'h.7.attn.9.key_transposed', 'h.7.attn.9.q_head', 'h.7.attn.9.v_head', 'h.7.attn.c_attn', 'h.7.attn.c_attn.a', 'h.7.attn.c_proj', 'h.7.attn.c_proj.a', 'h.7.attn.concat_vecs', 'h.7.attn.k', 'h.7.attn.output', 'h.7.attn.q', 'h.7.attn.v', 'h.7.input_embedding', 'h.7.ln_1', 'h.7.ln_1.mean_var', 'h.7.ln_1.x_norm', 'h.7.ln_1.x_norm_scaled', 'h.7.ln_1.x_norm_shifted', 'h.7.ln_2', 'h.7.ln_2.mean_var', 'h.7.ln_2.x_norm', 'h.7.ln_2.x_norm_scaled', 'h.7.ln_2.x_norm_shifted', 'h.7.mlp', 'h.7.mlp.c_fc', 'h.7.mlp.c_fc.a', 'h.7.mlp.c_proj', 'h.7.mlp.c_proj.a', 'h.7.mlp.gelu', 'h.7.output', 'h.7.resid.1', 'h.7.resid.2', 'h.8.attn.0.attn_score', 'h.8.attn.0.attn_score_scaled', 'h.8.attn.0.attn_weight', 'h.8.attn.0.ctx_vec', 'h.8.attn.0.k_head', 'h.8.attn.0.key_transposed', 'h.8.attn.0.q_head', 'h.8.attn.0.v_head', 'h.8.attn.1.attn_score', 'h.8.attn.1.attn_score_scaled', 'h.8.attn.1.attn_weight', 'h.8.attn.1.ctx_vec', 'h.8.attn.1.k_head', 'h.8.attn.1.key_transposed', 'h.8.attn.1.q_head', 'h.8.attn.1.v_head', 'h.8.attn.10.attn_score', 'h.8.attn.10.attn_score_scaled', 'h.8.attn.10.attn_weight', 'h.8.attn.10.ctx_vec', 'h.8.attn.10.k_head', 'h.8.attn.10.key_transposed', 'h.8.attn.10.q_head', 'h.8.attn.10.v_head', 'h.8.attn.11.attn_score', 'h.8.attn.11.attn_score_scaled', 'h.8.attn.11.attn_weight', 'h.8.attn.11.ctx_vec', 'h.8.attn.11.k_head', 'h.8.attn.11.key_transposed', 'h.8.attn.11.q_head', 'h.8.attn.11.v_head', 'h.8.attn.2.attn_score', 'h.8.attn.2.attn_score_scaled', 'h.8.attn.2.attn_weight', 'h.8.attn.2.ctx_vec', 'h.8.attn.2.k_head', 'h.8.attn.2.key_transposed', 'h.8.attn.2.q_head', 'h.8.attn.2.v_head', 'h.8.attn.3.attn_score', 'h.8.attn.3.attn_score_scaled', 'h.8.attn.3.attn_weight', 'h.8.attn.3.ctx_vec', 'h.8.attn.3.k_head', 'h.8.attn.3.key_transposed', 'h.8.attn.3.q_head', 'h.8.attn.3.v_head', 'h.8.attn.4.attn_score', 'h.8.attn.4.attn_score_scaled', 'h.8.attn.4.attn_weight', 'h.8.attn.4.ctx_vec', 'h.8.attn.4.k_head', 'h.8.attn.4.key_transposed', 'h.8.attn.4.q_head', 'h.8.attn.4.v_head', 'h.8.attn.5.attn_score', 'h.8.attn.5.attn_score_scaled', 'h.8.attn.5.attn_weight', 'h.8.attn.5.ctx_vec', 'h.8.attn.5.k_head', 'h.8.attn.5.key_transposed', 'h.8.attn.5.q_head', 'h.8.attn.5.v_head', 'h.8.attn.6.attn_score', 'h.8.attn.6.attn_score_scaled', 'h.8.attn.6.attn_weight', 'h.8.attn.6.ctx_vec', 'h.8.attn.6.k_head', 'h.8.attn.6.key_transposed', 'h.8.attn.6.q_head', 'h.8.attn.6.v_head', 'h.8.attn.7.attn_score', 'h.8.attn.7.attn_score_scaled', 'h.8.attn.7.attn_weight', 'h.8.attn.7.ctx_vec', 'h.8.attn.7.k_head', 'h.8.attn.7.key_transposed', 'h.8.attn.7.q_head', 'h.8.attn.7.v_head', 'h.8.attn.8.attn_score', 'h.8.attn.8.attn_score_scaled', 'h.8.attn.8.attn_weight', 'h.8.attn.8.ctx_vec', 'h.8.attn.8.k_head', 'h.8.attn.8.key_transposed', 'h.8.attn.8.q_head', 'h.8.attn.8.v_head', 'h.8.attn.9.attn_score', 'h.8.attn.9.attn_score_scaled', 'h.8.attn.9.attn_weight', 'h.8.attn.9.ctx_vec', 'h.8.attn.9.k_head', 'h.8.attn.9.key_transposed', 'h.8.attn.9.q_head', 'h.8.attn.9.v_head', 'h.8.attn.c_attn', 'h.8.attn.c_attn.a', 'h.8.attn.c_proj', 'h.8.attn.c_proj.a', 'h.8.attn.concat_vecs', 'h.8.attn.k', 'h.8.attn.output', 'h.8.attn.q', 'h.8.attn.v', 'h.8.input_embedding', 'h.8.ln_1', 'h.8.ln_1.mean_var', 'h.8.ln_1.x_norm', 'h.8.ln_1.x_norm_scaled', 'h.8.ln_1.x_norm_shifted', 'h.8.ln_2', 'h.8.ln_2.mean_var', 'h.8.ln_2.x_norm', 'h.8.ln_2.x_norm_scaled', 'h.8.ln_2.x_norm_shifted', 'h.8.mlp', 'h.8.mlp.c_fc', 'h.8.mlp.c_fc.a', 'h.8.mlp.c_proj', 'h.8.mlp.c_proj.a', 'h.8.mlp.gelu', 'h.8.output', 'h.8.resid.1', 'h.8.resid.2', 'h.9.attn.0.attn_score', 'h.9.attn.0.attn_score_scaled', 'h.9.attn.0.attn_weight', 'h.9.attn.0.ctx_vec', 'h.9.attn.0.k_head', 'h.9.attn.0.key_transposed', 'h.9.attn.0.q_head', 'h.9.attn.0.v_head', 'h.9.attn.1.attn_score', 'h.9.attn.1.attn_score_scaled', 'h.9.attn.1.attn_weight', 'h.9.attn.1.ctx_vec', 'h.9.attn.1.k_head', 'h.9.attn.1.key_transposed', 'h.9.attn.1.q_head', 'h.9.attn.1.v_head', 'h.9.attn.10.attn_score', 'h.9.attn.10.attn_score_scaled', 'h.9.attn.10.attn_weight', 'h.9.attn.10.ctx_vec', 'h.9.attn.10.k_head', 'h.9.attn.10.key_transposed', 'h.9.attn.10.q_head', 'h.9.attn.10.v_head', 'h.9.attn.11.attn_score', 'h.9.attn.11.attn_score_scaled', 'h.9.attn.11.attn_weight', 'h.9.attn.11.ctx_vec', 'h.9.attn.11.k_head', 'h.9.attn.11.key_transposed', 'h.9.attn.11.q_head', 'h.9.attn.11.v_head', 'h.9.attn.2.attn_score', 'h.9.attn.2.attn_score_scaled', 'h.9.attn.2.attn_weight', 'h.9.attn.2.ctx_vec', 'h.9.attn.2.k_head', 'h.9.attn.2.key_transposed', 'h.9.attn.2.q_head', 'h.9.attn.2.v_head', 'h.9.attn.3.attn_score', 'h.9.attn.3.attn_score_scaled', 'h.9.attn.3.attn_weight', 'h.9.attn.3.ctx_vec', 'h.9.attn.3.k_head', 'h.9.attn.3.key_transposed', 'h.9.attn.3.q_head', 'h.9.attn.3.v_head', 'h.9.attn.4.attn_score', 'h.9.attn.4.attn_score_scaled', 'h.9.attn.4.attn_weight', 'h.9.attn.4.ctx_vec', 'h.9.attn.4.k_head', 'h.9.attn.4.key_transposed', 'h.9.attn.4.q_head', 'h.9.attn.4.v_head', 'h.9.attn.5.attn_score', 'h.9.attn.5.attn_score_scaled', 'h.9.attn.5.attn_weight', 'h.9.attn.5.ctx_vec', 'h.9.attn.5.k_head', 'h.9.attn.5.key_transposed', 'h.9.attn.5.q_head', 'h.9.attn.5.v_head', 'h.9.attn.6.attn_score', 'h.9.attn.6.attn_score_scaled', 'h.9.attn.6.attn_weight', 'h.9.attn.6.ctx_vec', 'h.9.attn.6.k_head', 'h.9.attn.6.key_transposed', 'h.9.attn.6.q_head', 'h.9.attn.6.v_head', 'h.9.attn.7.attn_score', 'h.9.attn.7.attn_score_scaled', 'h.9.attn.7.attn_weight', 'h.9.attn.7.ctx_vec', 'h.9.attn.7.k_head', 'h.9.attn.7.key_transposed', 'h.9.attn.7.q_head', 'h.9.attn.7.v_head', 'h.9.attn.8.attn_score', 'h.9.attn.8.attn_score_scaled', 'h.9.attn.8.attn_weight', 'h.9.attn.8.ctx_vec', 'h.9.attn.8.k_head', 'h.9.attn.8.key_transposed', 'h.9.attn.8.q_head', 'h.9.attn.8.v_head', 'h.9.attn.9.attn_score', 'h.9.attn.9.attn_score_scaled', 'h.9.attn.9.attn_weight', 'h.9.attn.9.ctx_vec', 'h.9.attn.9.k_head', 'h.9.attn.9.key_transposed', 'h.9.attn.9.q_head', 'h.9.attn.9.v_head', 'h.9.attn.c_attn', 'h.9.attn.c_attn.a', 'h.9.attn.c_proj', 'h.9.attn.c_proj.a', 'h.9.attn.concat_vecs', 'h.9.attn.k', 'h.9.attn.output', 'h.9.attn.q', 'h.9.attn.v', 'h.9.input_embedding', 'h.9.ln_1', 'h.9.ln_1.mean_var', 'h.9.ln_1.x_norm', 'h.9.ln_1.x_norm_scaled', 'h.9.ln_1.x_norm_shifted', 'h.9.ln_2', 'h.9.ln_2.mean_var', 'h.9.ln_2.x_norm', 'h.9.ln_2.x_norm_scaled', 'h.9.ln_2.x_norm_shifted', 'h.9.mlp', 'h.9.mlp.c_fc', 'h.9.mlp.c_fc.a', 'h.9.mlp.c_proj', 'h.9.mlp.c_proj.a', 'h.9.mlp.gelu', 'h.9.output', 'h.9.resid.1', 'h.9.resid.2', 'head', 'head.a', 'ln_f', 'ln_f.mean_var', 'ln_f.x_norm', 'ln_f.x_norm_scaled', 'ln_f.x_norm_shifted', 'next_token_prob_dist', 'pos_emb', 'probs', 'token_emb']\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "filename = \"/Users/uonliaquat/workspace/zerograd/c_model.safetensors\"\n",
    "\n",
    "output_c = {}\n",
    "\n",
    "with safe_open(filename, framework=\"pt\", device=\"cpu\") as f:\n",
    "    for key in f.keys():\n",
    "        output_c[key] = f.get_tensor(key)\n",
    "\n",
    "# Inspect\n",
    "print(len(output_c))\n",
    "print(list(output_c.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, weight, bias, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "\n",
    "        # MUST be Parameters\n",
    "        self.weight = nn.Parameter(weight.clone())\n",
    "        self.bias = nn.Parameter(bias.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute mean & variance over last dim\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # Explicit reshape (matches PyTorch internals)\n",
    "        return x_hat * self.weight.view(1, 1, -1) + self.bias.view(1, 1, -1)\n",
    "    \n",
    "\n",
    "ln_ref = torch.nn.LayerNorm(768)\n",
    "ln_custom = LayerNorm(\n",
    "    768,\n",
    "    ln_ref.weight.data,\n",
    "    ln_ref.bias.data,\n",
    "    ln_ref.eps\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 5, 768)\n",
    "\n",
    "print(torch.allclose(ln_ref(x), ln_custom(x), atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d21e67c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3544,  0.3964,  0.2084,  ...,  0.0833,  0.1578,  1.7399],\n",
       "         [ 0.6083,  1.5527,  1.6953,  ..., -0.0332, -1.1895,  0.5943],\n",
       "         [ 0.4841, -0.3757, -1.1099,  ..., -2.5548,  0.6279,  0.8806],\n",
       "         [ 0.3732,  0.2516, -1.4893,  ...,  1.1755,  0.0857,  0.6072],\n",
       "         [ 0.9265,  1.1821, -1.2627,  ..., -0.8215, -0.0867,  0.4206]],\n",
       "\n",
       "        [[ 0.2936, -0.1163,  2.9201,  ..., -0.8842, -0.5789, -0.0855],\n",
       "         [ 1.7568, -1.1778, -0.5492,  ...,  0.8829,  1.2080, -0.4937],\n",
       "         [ 0.4102, -0.0173, -0.9737,  ...,  0.4693,  0.6395, -0.6874],\n",
       "         [ 0.2136, -0.4779,  0.1569,  ...,  0.8548,  0.1354, -0.6189],\n",
       "         [-0.5753,  0.5244,  0.9420,  ...,  1.0487,  0.5651, -0.0618]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_ref(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f51758",
   "metadata": {},
   "source": [
    "## Genrating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "3a3272eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(list(range(tokenizer.vocab_size)))\n",
    "\n",
    "# Choose a separator (must not appear in any token)\n",
    "SEP = \"|\"  # safe ASCII separator\n",
    "\n",
    "def clean_token(tok):\n",
    "    tok = tok.replace('', ' ')   # space marker\n",
    "    tok = tok.replace('', '<NL>')  # newline marker as visible string\n",
    "    # Optional: replace any other non-ASCII sequences\n",
    "    tok = ''.join(c if ord(c) < 128 else '<U>' for c in tok)\n",
    "    return tok\n",
    "\n",
    "with open(\"gpt2_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(SEP.join(clean_token(tok) for tok in tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415effdc",
   "metadata": {},
   "source": [
    "## Python GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "1558cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape,\n",
    "        eps: float = 1e-5,\n",
    "        elementwise_affine: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = tuple(normalized_shape)\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "\n",
    "        if elementwise_affine:\n",
    "            self.weight = nn.Parameter(\n",
    "                torch.ones(self.normalized_shape, **factory_kwargs)\n",
    "            )\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.zeros(self.normalized_shape, **factory_kwargs)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter(\"weight\", None)\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x, out, h, l) -> torch.Tensor:\n",
    "        # Normalize over the last len(normalized_shape) dimensions\n",
    "        dims = tuple(range(-len(self.normalized_shape), 0))\n",
    "\n",
    "        mean = x.mean(dim=dims, keepdim=True)\n",
    "        #out[f'h.{h}.ln_1.mean'] = mean\n",
    "        var = x.var(dim=dims, keepdim=True, unbiased=False)\n",
    "        #out[f'h.{h}.ln_1.var'] = var\n",
    "        out[f'h.{h}.ln_{l}.mean_var'] = torch.cat([mean, var], dim=-1)\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        out[f'h.{h}.ln_{l}.x_norm'] = x_hat\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            x_hat = x_hat * self.weight\n",
    "            out[f'h.{h}.ln_{l}.x_norm_scaled'] = x_hat\n",
    "            x_hat = x_hat + self.bias\n",
    "            out[f'h.{h}.ln_{l}.x_norm_shifted'] = x_hat\n",
    "        return x_hat\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return (\n",
    "            f\"normalized_shape={self.normalized_shape}, \"\n",
    "            f\"eps={self.eps}, \"\n",
    "            f\"elementwise_affine={self.elementwise_affine}\"\n",
    "        )\n",
    "    \n",
    "\n",
    "class GPT2_Full_Debug(nn.Module):\n",
    "    def __init__(self, model_name=\"gpt2\", device=\"cpu\", dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        gpt2 = GPT2Model.from_pretrained(model_name)\n",
    "        gpt2.eval()\n",
    "\n",
    "        self.n_heads = gpt2.config.n_head\n",
    "        self.hidden_size = gpt2.config.n_embd\n",
    "        self.head_dim = self.hidden_size // self.n_heads\n",
    "        self.n_layers = gpt2.config.n_layer\n",
    "        self.vocab_size = gpt2.config.vocab_size\n",
    "\n",
    "        # Embeddings\n",
    "        self.wte = nn.Embedding.from_pretrained(gpt2.wte.weight.detach().to(dtype), freeze=True)\n",
    "        self.wpe = nn.Embedding.from_pretrained(gpt2.wpe.weight.detach().to(dtype), freeze=True)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i, block in enumerate(gpt2.h):\n",
    "            b = nn.ModuleDict({\n",
    "                \"ln_1\": LayerNorm(self.hidden_size, eps=block.ln_1.eps),\n",
    "                \"c_attn\": nn.Linear(self.hidden_size, 3 * self.hidden_size),\n",
    "                \"c_proj_attn\": nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                \"ln_2\": LayerNorm(self.hidden_size, eps=block.ln_2.eps),\n",
    "                \"c_fc\": nn.Linear(self.hidden_size, 4 * self.hidden_size),\n",
    "                \"c_proj_mlp\": nn.Linear(4 * self.hidden_size, self.hidden_size),\n",
    "            })\n",
    "            # Copy weights\n",
    "            b[\"ln_1\"].weight.data.copy_(block.ln_1.weight)\n",
    "            b[\"ln_1\"].bias.data.copy_(block.ln_1.bias)\n",
    "\n",
    "            b[\"c_attn\"].weight.data.copy_(block.attn.c_attn.weight.T)\n",
    "            b[\"c_attn\"].bias.data.copy_(block.attn.c_attn.bias)\n",
    "\n",
    "            b[\"c_proj_attn\"].weight.data.copy_(block.attn.c_proj.weight.T)\n",
    "            b[\"c_proj_attn\"].bias.data.copy_(block.attn.c_proj.bias)\n",
    "\n",
    "            b[\"ln_2\"].weight.data.copy_(block.ln_2.weight)\n",
    "            b[\"ln_2\"].bias.data.copy_(block.ln_2.bias)\n",
    "\n",
    "            b[\"c_fc\"].weight.data.copy_(block.mlp.c_fc.weight.T)\n",
    "            b[\"c_fc\"].bias.data.copy_(block.mlp.c_fc.bias)\n",
    "\n",
    "            b[\"c_proj_mlp\"].weight.data.copy_(block.mlp.c_proj.weight.T)\n",
    "            b[\"c_proj_mlp\"].bias.data.copy_(block.mlp.c_proj.bias)\n",
    "\n",
    "            self.blocks.append(b)\n",
    "\n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(self.hidden_size, eps=gpt2.ln_f.eps)\n",
    "        self.ln_f.weight.data.copy_(gpt2.ln_f.weight)\n",
    "        self.ln_f.bias.data.copy_(gpt2.ln_f.bias)\n",
    "\n",
    "        # LM head (weight tied)\n",
    "        self.lm_head = nn.Linear(self.hidden_size, self.vocab_size, bias=False)\n",
    "        self.lm_head.weight.data.copy_(self.wte.weight)\n",
    "\n",
    "        self.to(device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        bsz, seq_len = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        out = {}\n",
    "\n",
    "        # Embeddings\n",
    "        pos_ids = torch.arange(seq_len, device=device).unsqueeze(0)\n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(pos_ids)\n",
    "        x = tok_emb + pos_emb\n",
    "        out[\"token_emb\"] = tok_emb\n",
    "        out[\"pos_emb\"] = pos_emb\n",
    "\n",
    "        # Iterate over blocks\n",
    "        for layer_idx, b in enumerate(self.blocks):\n",
    "            #layer_out = {}\n",
    "            out[f\"h.{layer_idx}.input_embedding\"] = x\n",
    "\n",
    "            # LN1\n",
    "            x_ln1 = b[\"ln_1\"](x, out, layer_idx, 1)\n",
    "            out[f\"h.{layer_idx}.ln_1\"] = x_ln1\n",
    "\n",
    "            # QKV\n",
    "            qkv = b[\"c_attn\"](x_ln1)\n",
    "            q, k, v = qkv.split(self.hidden_size, dim=2)\n",
    "            out[f\"h.{layer_idx}.qkv\"] = qkv\n",
    "            out[f\"h.{layer_idx}.attn.q\"] = q\n",
    "            out[f\"h.{layer_idx}.attn.k\"] = k\n",
    "            out[f\"h.{layer_idx}.attn.v\"] = v\n",
    "\n",
    "            # Split heads\n",
    "            def split_heads(x):\n",
    "                return x.view(bsz, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "            qh = split_heads(q)[0]\n",
    "            kh = split_heads(k)[0]\n",
    "            vh = split_heads(v)[0]\n",
    "\n",
    "            qh_list = torch.split(qh, 1, dim=0)  # split into chunks of size 1\n",
    "            kh_list = torch.split(kh, 1, dim=0)  # split into chunks of size 1\n",
    "            vh_list = torch.split(vh, 1, dim=0)  # split into chunks of size 1 \n",
    "            \n",
    "            for i, (q, k, v) in enumerate(zip(qh_list, kh_list, vh_list)):\n",
    "\n",
    "                out[f\"h.{layer_idx}.attn.{i}.q_head\"] = q\n",
    "                out[f\"h.{layer_idx}.attn.{i}.k_head\"] = k\n",
    "                out[f\"h.{layer_idx}.attn.{i}.v_head\"] = v\n",
    "\n",
    "            # Attention\n",
    "            kh_t = kh.transpose(-2, -1)\n",
    "            qk = torch.matmul(qh, kh_t)\n",
    "            qk_scaled = qk / math.sqrt(self.head_dim)\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).view(1, 1, seq_len, seq_len)\n",
    "            qk_masked = qk_scaled.masked_fill(causal_mask == 0, float(\"-inf\"))\n",
    "            attn_probs = torch.softmax(qk_masked, dim=-1)\n",
    "            attn_ctx_heads = torch.matmul(attn_probs, vh)\n",
    "            attn_ctx = attn_ctx_heads.transpose(1, 2).contiguous().view(bsz, seq_len, self.hidden_size)\n",
    "            attn_out = b[\"c_proj_attn\"](attn_ctx)\n",
    "\n",
    "            out[f\"h.{layer_idx}.attn.c_proj\"] = attn_out\n",
    "\n",
    "            #resid 1\n",
    "            x = x + attn_out\n",
    "            out[f\"h.{layer_idx}.resid.1\"] = x\n",
    "\n",
    "            # LN2 + MLP\n",
    "            x_ln2 = b[\"ln_2\"](x, out, layer_idx, 2)\n",
    "            mlp_out = b[\"c_proj_mlp\"](torch.nn.functional.gelu(b[\"c_fc\"](x_ln2)))\n",
    "            out[f\"h.{layer_idx}.mlp\"] = mlp_out\n",
    "\n",
    "            # resid 2\n",
    "            x = x + mlp_out\n",
    "            out[f\"h.{layer_idx}.resid.2\"] = x\n",
    "\n",
    "            # out[f\"h.{layer_idx}.ln_2\"] = x_ln2\n",
    "            # out[f\"h.{layer_idx}.mlp_fc_out\"] = mlp_out\n",
    "            # out[f\"h.{layer_idx}.block_output\"] = x\n",
    "\n",
    "            #out[f\"layer_{layer_idx}\"] = layer_out\n",
    "\n",
    "        # Final layer norm\n",
    "        x_final_ln = self.ln_f(x)\n",
    "        out[\"ln_f\"] = x_final_ln\n",
    "\n",
    "        # LM head\n",
    "        logits = self.lm_head(x_final_ln)\n",
    "        out[\"head\"] = logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        # out[\"logits\"] = logits\n",
    "        out[\"probs\"] = probs\n",
    "\n",
    "        # Greedy next token\n",
    "        next_token_prob_dist = probs[:, -1, :]\n",
    "        out['next_token_prob_dist'] = next_token_prob_dist\n",
    "        next_token_id = torch.argmax(next_token_prob_dist, dim=-1)\n",
    "        out[\"next_token_id\"] = next_token_id\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "e75ab983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In recent years, artificial intelligence has become an important tool in many industries, helping researchers analyze data, automate tasks, and improve decision making. Machine learning models are trained on large datasets and can generate text, recognize patterns, and predict outcomes based on previous examples.\n",
      "\n",
      "The goal of the study was to determine whether machine learning models can be used to predict the future of a large dataset. The goal of the study was to determine whether machine learning models can be used to predict the future of a large dataset.\n",
      "\n",
      "The study was funded by the National Science Foundation (NSF) and the National Science Foundation (NSF) by the National Science Foundation (NSF) and the National Science Foundation (NSF) by the National Science Foundation (NSF) and the National Science Foundation (NSF) by the National Science Foundation (NSF) and the National Science Foundation (NSF) by the National Science Foundation (NSF) and the National Science Foundation (NSF) by the National Science Foundation (NSF) and the National Science Foundation (NSF) by the National Science Foundation (NSF) and the National Science Foundation (NSF) by the National Science Foundation (NSF) and the National Science Foundation (NSF)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# -------------------------------\n",
    "# Device\n",
    "# -------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------------\n",
    "# Paragraph\n",
    "# -------------------------------\n",
    "#paragraph = (\"Hello I'm a language model\")\n",
    "\n",
    "paragraph = (\n",
    "    \"In recent years, artificial intelligence has become an important tool \"\n",
    "    \"in many industries, helping researchers analyze data, automate tasks, \"\n",
    "    \"and improve decision making. Machine learning models are trained on \"\n",
    "    \"large datasets and can generate text, recognize patterns, and predict \"\n",
    "    \"outcomes based on previous examples.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenizer\n",
    "# -------------------------------\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_ids = tokenizer.encode(paragraph, return_tensors=\"pt\").to(device)\n",
    "# print(\"token_ids: \", input_ids)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Set context window to 6\n",
    "# -------------------------------\n",
    "context_len = 52\n",
    "if input_ids.shape[1] > context_len:\n",
    "    input_ids = input_ids[:, -context_len:]  # keep last 52 tokens\n",
    "\n",
    "cur_len = input_ids.shape[1]\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize GPT2 debug model\n",
    "# -------------------------------\n",
    "model = GPT2_Full_Debug(model_name=\"gpt2\", device=device, dtype=torch.float32)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare for generation\n",
    "# -------------------------------\n",
    "num_generate = 200  # number of new tokens\n",
    "generated_ids = input_ids[0].tolist()  # start with initial context\n",
    "\n",
    "# Store intermediate outputs for all steps\n",
    "all_steps_outputs = []\n",
    "\n",
    "# -------------------------------\n",
    "# Generation loop\n",
    "# -------------------------------\n",
    "for step in range(num_generate):\n",
    "    cur_input = torch.tensor([generated_ids[-context_len:]], device=device)\n",
    "    #print(cur_input)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(cur_input)\n",
    "\n",
    "    # Save intermediate outputs for this step\n",
    "    all_steps_outputs.append(outputs)\n",
    "\n",
    "    # Get next token (greedy)\n",
    "    next_token_id = outputs['next_token_id'].item()\n",
    "\n",
    "    # Append to generated list\n",
    "    generated_ids.append(next_token_id)\n",
    "\n",
    "# -------------------------------\n",
    "# Decode full generated sequence\n",
    "# -------------------------------\n",
    "full_text = tokenizer.decode(generated_ids)\n",
    "#print(\"\\n=== Full generated text (initial + 50 new tokens) ===\\n\")\n",
    "print(full_text)\n",
    "\n",
    "# -------------------------------\n",
    "# Optional: print next token for first 5 steps\n",
    "# -------------------------------\n",
    "# print(\"\\n=== First 5 generated token IDs and strings ===\")\n",
    "# for step_out in all_steps_outputs:\n",
    "#     tid = step_out['next_token_id'].item()\n",
    "#     print(tid, tokenizer.decode([tid]))\n",
    "\n",
    "output_p = all_steps_outputs[0]\n",
    "# list(output_p.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "19948167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([198])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "0b36fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "atol = 1e-4\n",
    "h = 0\n",
    "for h in range(0, 12):\n",
    "    print(torch.allclose(output_p['token_emb'],  output_c['token_emb'], atol=atol))\n",
    "    print(torch.allclose(output_p['pos_emb'],  output_c['pos_emb'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.input_embedding'], output_c[f'h.{h}.input_embedding'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1.mean_var'], output_c[f'h.{h}.ln_1.mean_var'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1.x_norm'], output_c[f'h.{h}.ln_1.x_norm'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1.x_norm_shifted'], output_c[f'h.{h}.ln_1.x_norm_shifted'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1.x_norm_scaled'], output_c[f'h.{h}.ln_1.x_norm_scaled'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_1'], output_c[f'h.{h}.ln_1'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.attn.q'], output_c[f'h.{h}.attn.q'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.attn.k'], output_c[f'h.{h}.attn.k'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.attn.v'], output_c[f'h.{h}.attn.v'], atol=atol))\n",
    "\n",
    "\n",
    "    for i in range(0, 12):\n",
    "        print(torch.allclose(output_p[f\"h.{h}.attn.{i}.q_head\"], output_c[f\"h.{h}.attn.{i}.q_head\"], atol=atol))\n",
    "        print(torch.allclose(output_p[f\"h.{h}.attn.{i}.k_head\"], output_c[f\"h.{h}.attn.{i}.k_head\"], atol=atol))\n",
    "        print(torch.allclose(output_p[f\"h.{h}.attn.{i}.v_head\"], output_c[f\"h.{h}.attn.{i}.v_head\"], atol=atol))\n",
    "\n",
    "\n",
    "    #c_proj\n",
    "    print(torch.allclose(output_p[f\"h.{0}.attn.c_proj\"], output_c[f\"h.{0}.attn.c_proj\"], atol=atol))\n",
    "\n",
    "    #residual connection 1\n",
    "    print(torch.allclose(output_p[f\"h.{0}.resid.1\"], output_c[f\"h.{0}.resid.1\"], atol=atol))\n",
    "\n",
    "    # ln_2\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_2.mean_var'], output_c[f'h.{h}.ln_2.mean_var'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_2.x_norm'], output_c[f'h.{h}.ln_2.x_norm'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_2.x_norm_shifted'], output_c[f'h.{h}.ln_2.x_norm_shifted'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.ln_2.x_norm_scaled'], output_c[f'h.{h}.ln_2.x_norm_scaled'], atol=atol))\n",
    "    print(torch.allclose(output_p[f'h.{h}.mlp'], output_c[f'h.{h}.mlp'], atol=atol))\n",
    "\n",
    "    #residual connection 2\n",
    "    print(torch.allclose(output_p[f\"h.{0}.resid.2\"], output_c[f\"h.{0}.resid.2\"], atol=atol))\n",
    "\n",
    "\n",
    "print(torch.allclose(output_p[f\"ln_f\"], output_c[f\"ln_f\"], atol=atol))\n",
    "print(torch.allclose(output_p[f\"head\"], output_c[f\"head\"], atol=atol))\n",
    "print(torch.allclose(output_p[f\"probs\"], output_c[f\"probs\"], atol=atol))\n",
    "print(torch.allclose(output_p[f\"next_token_prob_dist\"], output_c[f\"next_token_prob_dist\"], atol=atol))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "38533960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([263])\n"
     ]
    }
   ],
   "source": [
    "print(output_p['next_token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "fa80cd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50257])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_p[f\"probs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "6afb1601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5170, -0.7297,  1.0580,  ..., -0.9128,  0.1060, -1.7272],\n",
       "         [-0.6918, -0.1707,  0.4360,  ..., -0.7993,  0.4888,  0.2977],\n",
       "         [-0.1759, -1.1365, -1.0272,  ..., -0.3013, -0.3815,  0.3796],\n",
       "         [-0.6328,  0.5997,  0.4721,  ...,  0.0642, -0.2357, -0.1439],\n",
       "         [-1.5716,  0.3739, -0.8954,  ...,  0.1883,  0.7078,  1.2277],\n",
       "         [-1.6475, -0.7412, -0.5769,  ..., -0.8543,  0.3709,  0.8488]]])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_p[f'h.{0}.mlp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6ccce953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 64])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors_list = torch.split(output_p[f\"h.{h}.attn.q_heads\"][0], 1, dim=0)\n",
    "tensors_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "c40e039a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h.0.attn.0.attn_score',\n",
       " 'h.0.attn.0.attn_score_scaled',\n",
       " 'h.0.attn.0.attn_weight',\n",
       " 'h.0.attn.0.ctx_vec',\n",
       " 'h.0.attn.0.k_head',\n",
       " 'h.0.attn.0.key_transposed',\n",
       " 'h.0.attn.0.q_head',\n",
       " 'h.0.attn.0.v_head',\n",
       " 'h.0.attn.1.attn_score',\n",
       " 'h.0.attn.1.attn_score_scaled',\n",
       " 'h.0.attn.1.attn_weight',\n",
       " 'h.0.attn.1.ctx_vec',\n",
       " 'h.0.attn.1.k_head',\n",
       " 'h.0.attn.1.key_transposed',\n",
       " 'h.0.attn.1.q_head',\n",
       " 'h.0.attn.1.v_head',\n",
       " 'h.0.attn.10.attn_score',\n",
       " 'h.0.attn.10.attn_score_scaled',\n",
       " 'h.0.attn.10.attn_weight',\n",
       " 'h.0.attn.10.ctx_vec',\n",
       " 'h.0.attn.10.k_head',\n",
       " 'h.0.attn.10.key_transposed',\n",
       " 'h.0.attn.10.q_head',\n",
       " 'h.0.attn.10.v_head',\n",
       " 'h.0.attn.11.attn_score',\n",
       " 'h.0.attn.11.attn_score_scaled',\n",
       " 'h.0.attn.11.attn_weight',\n",
       " 'h.0.attn.11.ctx_vec',\n",
       " 'h.0.attn.11.k_head',\n",
       " 'h.0.attn.11.key_transposed',\n",
       " 'h.0.attn.11.q_head',\n",
       " 'h.0.attn.11.v_head',\n",
       " 'h.0.attn.2.attn_score',\n",
       " 'h.0.attn.2.attn_score_scaled',\n",
       " 'h.0.attn.2.attn_weight',\n",
       " 'h.0.attn.2.ctx_vec',\n",
       " 'h.0.attn.2.k_head',\n",
       " 'h.0.attn.2.key_transposed',\n",
       " 'h.0.attn.2.q_head',\n",
       " 'h.0.attn.2.v_head',\n",
       " 'h.0.attn.3.attn_score',\n",
       " 'h.0.attn.3.attn_score_scaled',\n",
       " 'h.0.attn.3.attn_weight',\n",
       " 'h.0.attn.3.ctx_vec',\n",
       " 'h.0.attn.3.k_head',\n",
       " 'h.0.attn.3.key_transposed',\n",
       " 'h.0.attn.3.q_head',\n",
       " 'h.0.attn.3.v_head',\n",
       " 'h.0.attn.4.attn_score',\n",
       " 'h.0.attn.4.attn_score_scaled',\n",
       " 'h.0.attn.4.attn_weight',\n",
       " 'h.0.attn.4.ctx_vec',\n",
       " 'h.0.attn.4.k_head',\n",
       " 'h.0.attn.4.key_transposed',\n",
       " 'h.0.attn.4.q_head',\n",
       " 'h.0.attn.4.v_head',\n",
       " 'h.0.attn.5.attn_score',\n",
       " 'h.0.attn.5.attn_score_scaled',\n",
       " 'h.0.attn.5.attn_weight',\n",
       " 'h.0.attn.5.ctx_vec',\n",
       " 'h.0.attn.5.k_head',\n",
       " 'h.0.attn.5.key_transposed',\n",
       " 'h.0.attn.5.q_head',\n",
       " 'h.0.attn.5.v_head',\n",
       " 'h.0.attn.6.attn_score',\n",
       " 'h.0.attn.6.attn_score_scaled',\n",
       " 'h.0.attn.6.attn_weight',\n",
       " 'h.0.attn.6.ctx_vec',\n",
       " 'h.0.attn.6.k_head',\n",
       " 'h.0.attn.6.key_transposed',\n",
       " 'h.0.attn.6.q_head',\n",
       " 'h.0.attn.6.v_head',\n",
       " 'h.0.attn.7.attn_score',\n",
       " 'h.0.attn.7.attn_score_scaled',\n",
       " 'h.0.attn.7.attn_weight',\n",
       " 'h.0.attn.7.ctx_vec',\n",
       " 'h.0.attn.7.k_head',\n",
       " 'h.0.attn.7.key_transposed',\n",
       " 'h.0.attn.7.q_head',\n",
       " 'h.0.attn.7.v_head',\n",
       " 'h.0.attn.8.attn_score',\n",
       " 'h.0.attn.8.attn_score_scaled',\n",
       " 'h.0.attn.8.attn_weight',\n",
       " 'h.0.attn.8.ctx_vec',\n",
       " 'h.0.attn.8.k_head',\n",
       " 'h.0.attn.8.key_transposed',\n",
       " 'h.0.attn.8.q_head',\n",
       " 'h.0.attn.8.v_head',\n",
       " 'h.0.attn.9.attn_score',\n",
       " 'h.0.attn.9.attn_score_scaled',\n",
       " 'h.0.attn.9.attn_weight',\n",
       " 'h.0.attn.9.ctx_vec',\n",
       " 'h.0.attn.9.k_head',\n",
       " 'h.0.attn.9.key_transposed',\n",
       " 'h.0.attn.9.q_head',\n",
       " 'h.0.attn.9.v_head',\n",
       " 'h.0.attn.c_attn.a',\n",
       " 'h.0.attn.c_attn.output',\n",
       " 'h.0.attn.c_proj.a',\n",
       " 'h.0.attn.c_proj.output',\n",
       " 'h.0.attn.concat_vecs',\n",
       " 'h.0.attn.k',\n",
       " 'h.0.attn.output',\n",
       " 'h.0.attn.q',\n",
       " 'h.0.attn.v',\n",
       " 'h.0.input_embedding',\n",
       " 'h.0.ln_1',\n",
       " 'h.0.ln_1.mean_var',\n",
       " 'h.0.ln_1.x_norm',\n",
       " 'h.0.ln_1.x_norm_scaled',\n",
       " 'h.0.ln_1.x_norm_shifted',\n",
       " 'h.0.ln_2',\n",
       " 'h.0.ln_2.mean_var',\n",
       " 'h.0.ln_2.x_norm',\n",
       " 'h.0.ln_2.x_norm_scaled',\n",
       " 'h.0.ln_2.x_norm_shifted',\n",
       " 'h.0.mlp.c_fc.a',\n",
       " 'h.0.mlp.c_fc.output',\n",
       " 'h.0.mlp.c_proj.a',\n",
       " 'h.0.mlp.c_proj.output',\n",
       " 'h.0.mlp.gelu',\n",
       " 'h.0.mlp.output',\n",
       " 'h.0.output',\n",
       " 'h.0.resid.1',\n",
       " 'h.0.resid.2',\n",
       " 'h.1.attn.0.attn_score',\n",
       " 'h.1.attn.0.attn_score_scaled',\n",
       " 'h.1.attn.0.attn_weight',\n",
       " 'h.1.attn.0.ctx_vec',\n",
       " 'h.1.attn.0.k_head',\n",
       " 'h.1.attn.0.key_transposed',\n",
       " 'h.1.attn.0.q_head',\n",
       " 'h.1.attn.0.v_head',\n",
       " 'h.1.attn.1.attn_score',\n",
       " 'h.1.attn.1.attn_score_scaled',\n",
       " 'h.1.attn.1.attn_weight',\n",
       " 'h.1.attn.1.ctx_vec',\n",
       " 'h.1.attn.1.k_head',\n",
       " 'h.1.attn.1.key_transposed',\n",
       " 'h.1.attn.1.q_head',\n",
       " 'h.1.attn.1.v_head',\n",
       " 'h.1.attn.10.attn_score',\n",
       " 'h.1.attn.10.attn_score_scaled',\n",
       " 'h.1.attn.10.attn_weight',\n",
       " 'h.1.attn.10.ctx_vec',\n",
       " 'h.1.attn.10.k_head',\n",
       " 'h.1.attn.10.key_transposed',\n",
       " 'h.1.attn.10.q_head',\n",
       " 'h.1.attn.10.v_head',\n",
       " 'h.1.attn.11.attn_score',\n",
       " 'h.1.attn.11.attn_score_scaled',\n",
       " 'h.1.attn.11.attn_weight',\n",
       " 'h.1.attn.11.ctx_vec',\n",
       " 'h.1.attn.11.k_head',\n",
       " 'h.1.attn.11.key_transposed',\n",
       " 'h.1.attn.11.q_head',\n",
       " 'h.1.attn.11.v_head',\n",
       " 'h.1.attn.2.attn_score',\n",
       " 'h.1.attn.2.attn_score_scaled',\n",
       " 'h.1.attn.2.attn_weight',\n",
       " 'h.1.attn.2.ctx_vec',\n",
       " 'h.1.attn.2.k_head',\n",
       " 'h.1.attn.2.key_transposed',\n",
       " 'h.1.attn.2.q_head',\n",
       " 'h.1.attn.2.v_head',\n",
       " 'h.1.attn.3.attn_score',\n",
       " 'h.1.attn.3.attn_score_scaled',\n",
       " 'h.1.attn.3.attn_weight',\n",
       " 'h.1.attn.3.ctx_vec',\n",
       " 'h.1.attn.3.k_head',\n",
       " 'h.1.attn.3.key_transposed',\n",
       " 'h.1.attn.3.q_head',\n",
       " 'h.1.attn.3.v_head',\n",
       " 'h.1.attn.4.attn_score',\n",
       " 'h.1.attn.4.attn_score_scaled',\n",
       " 'h.1.attn.4.attn_weight',\n",
       " 'h.1.attn.4.ctx_vec',\n",
       " 'h.1.attn.4.k_head',\n",
       " 'h.1.attn.4.key_transposed',\n",
       " 'h.1.attn.4.q_head',\n",
       " 'h.1.attn.4.v_head',\n",
       " 'h.1.attn.5.attn_score',\n",
       " 'h.1.attn.5.attn_score_scaled',\n",
       " 'h.1.attn.5.attn_weight',\n",
       " 'h.1.attn.5.ctx_vec',\n",
       " 'h.1.attn.5.k_head',\n",
       " 'h.1.attn.5.key_transposed',\n",
       " 'h.1.attn.5.q_head',\n",
       " 'h.1.attn.5.v_head',\n",
       " 'h.1.attn.6.attn_score',\n",
       " 'h.1.attn.6.attn_score_scaled',\n",
       " 'h.1.attn.6.attn_weight',\n",
       " 'h.1.attn.6.ctx_vec',\n",
       " 'h.1.attn.6.k_head',\n",
       " 'h.1.attn.6.key_transposed',\n",
       " 'h.1.attn.6.q_head',\n",
       " 'h.1.attn.6.v_head',\n",
       " 'h.1.attn.7.attn_score',\n",
       " 'h.1.attn.7.attn_score_scaled',\n",
       " 'h.1.attn.7.attn_weight',\n",
       " 'h.1.attn.7.ctx_vec',\n",
       " 'h.1.attn.7.k_head',\n",
       " 'h.1.attn.7.key_transposed',\n",
       " 'h.1.attn.7.q_head',\n",
       " 'h.1.attn.7.v_head',\n",
       " 'h.1.attn.8.attn_score',\n",
       " 'h.1.attn.8.attn_score_scaled',\n",
       " 'h.1.attn.8.attn_weight',\n",
       " 'h.1.attn.8.ctx_vec',\n",
       " 'h.1.attn.8.k_head',\n",
       " 'h.1.attn.8.key_transposed',\n",
       " 'h.1.attn.8.q_head',\n",
       " 'h.1.attn.8.v_head',\n",
       " 'h.1.attn.9.attn_score',\n",
       " 'h.1.attn.9.attn_score_scaled',\n",
       " 'h.1.attn.9.attn_weight',\n",
       " 'h.1.attn.9.ctx_vec',\n",
       " 'h.1.attn.9.k_head',\n",
       " 'h.1.attn.9.key_transposed',\n",
       " 'h.1.attn.9.q_head',\n",
       " 'h.1.attn.9.v_head',\n",
       " 'h.1.attn.c_attn.a',\n",
       " 'h.1.attn.c_attn.output',\n",
       " 'h.1.attn.c_proj.a',\n",
       " 'h.1.attn.c_proj.output',\n",
       " 'h.1.attn.concat_vecs',\n",
       " 'h.1.attn.k',\n",
       " 'h.1.attn.output',\n",
       " 'h.1.attn.q',\n",
       " 'h.1.attn.v',\n",
       " 'h.1.input_embedding',\n",
       " 'h.1.ln_1',\n",
       " 'h.1.ln_1.mean_var',\n",
       " 'h.1.ln_1.x_norm',\n",
       " 'h.1.ln_1.x_norm_scaled',\n",
       " 'h.1.ln_1.x_norm_shifted',\n",
       " 'h.1.ln_2',\n",
       " 'h.1.ln_2.mean_var',\n",
       " 'h.1.ln_2.x_norm',\n",
       " 'h.1.ln_2.x_norm_scaled',\n",
       " 'h.1.ln_2.x_norm_shifted',\n",
       " 'h.1.mlp.c_fc.a',\n",
       " 'h.1.mlp.c_fc.output',\n",
       " 'h.1.mlp.c_proj.a',\n",
       " 'h.1.mlp.c_proj.output',\n",
       " 'h.1.mlp.gelu',\n",
       " 'h.1.mlp.output',\n",
       " 'h.1.output',\n",
       " 'h.1.resid.1',\n",
       " 'h.1.resid.2',\n",
       " 'h.10.attn.0.attn_score',\n",
       " 'h.10.attn.0.attn_score_scaled',\n",
       " 'h.10.attn.0.attn_weight',\n",
       " 'h.10.attn.0.ctx_vec',\n",
       " 'h.10.attn.0.k_head',\n",
       " 'h.10.attn.0.key_transposed',\n",
       " 'h.10.attn.0.q_head',\n",
       " 'h.10.attn.0.v_head',\n",
       " 'h.10.attn.1.attn_score',\n",
       " 'h.10.attn.1.attn_score_scaled',\n",
       " 'h.10.attn.1.attn_weight',\n",
       " 'h.10.attn.1.ctx_vec',\n",
       " 'h.10.attn.1.k_head',\n",
       " 'h.10.attn.1.key_transposed',\n",
       " 'h.10.attn.1.q_head',\n",
       " 'h.10.attn.1.v_head',\n",
       " 'h.10.attn.10.attn_score',\n",
       " 'h.10.attn.10.attn_score_scaled',\n",
       " 'h.10.attn.10.attn_weight',\n",
       " 'h.10.attn.10.ctx_vec',\n",
       " 'h.10.attn.10.k_head',\n",
       " 'h.10.attn.10.key_transposed',\n",
       " 'h.10.attn.10.q_head',\n",
       " 'h.10.attn.10.v_head',\n",
       " 'h.10.attn.11.attn_score',\n",
       " 'h.10.attn.11.attn_score_scaled',\n",
       " 'h.10.attn.11.attn_weight',\n",
       " 'h.10.attn.11.ctx_vec',\n",
       " 'h.10.attn.11.k_head',\n",
       " 'h.10.attn.11.key_transposed',\n",
       " 'h.10.attn.11.q_head',\n",
       " 'h.10.attn.11.v_head',\n",
       " 'h.10.attn.2.attn_score',\n",
       " 'h.10.attn.2.attn_score_scaled',\n",
       " 'h.10.attn.2.attn_weight',\n",
       " 'h.10.attn.2.ctx_vec',\n",
       " 'h.10.attn.2.k_head',\n",
       " 'h.10.attn.2.key_transposed',\n",
       " 'h.10.attn.2.q_head',\n",
       " 'h.10.attn.2.v_head',\n",
       " 'h.10.attn.3.attn_score',\n",
       " 'h.10.attn.3.attn_score_scaled',\n",
       " 'h.10.attn.3.attn_weight',\n",
       " 'h.10.attn.3.ctx_vec',\n",
       " 'h.10.attn.3.k_head',\n",
       " 'h.10.attn.3.key_transposed',\n",
       " 'h.10.attn.3.q_head',\n",
       " 'h.10.attn.3.v_head',\n",
       " 'h.10.attn.4.attn_score',\n",
       " 'h.10.attn.4.attn_score_scaled',\n",
       " 'h.10.attn.4.attn_weight',\n",
       " 'h.10.attn.4.ctx_vec',\n",
       " 'h.10.attn.4.k_head',\n",
       " 'h.10.attn.4.key_transposed',\n",
       " 'h.10.attn.4.q_head',\n",
       " 'h.10.attn.4.v_head',\n",
       " 'h.10.attn.5.attn_score',\n",
       " 'h.10.attn.5.attn_score_scaled',\n",
       " 'h.10.attn.5.attn_weight',\n",
       " 'h.10.attn.5.ctx_vec',\n",
       " 'h.10.attn.5.k_head',\n",
       " 'h.10.attn.5.key_transposed',\n",
       " 'h.10.attn.5.q_head',\n",
       " 'h.10.attn.5.v_head',\n",
       " 'h.10.attn.6.attn_score',\n",
       " 'h.10.attn.6.attn_score_scaled',\n",
       " 'h.10.attn.6.attn_weight',\n",
       " 'h.10.attn.6.ctx_vec',\n",
       " 'h.10.attn.6.k_head',\n",
       " 'h.10.attn.6.key_transposed',\n",
       " 'h.10.attn.6.q_head',\n",
       " 'h.10.attn.6.v_head',\n",
       " 'h.10.attn.7.attn_score',\n",
       " 'h.10.attn.7.attn_score_scaled',\n",
       " 'h.10.attn.7.attn_weight',\n",
       " 'h.10.attn.7.ctx_vec',\n",
       " 'h.10.attn.7.k_head',\n",
       " 'h.10.attn.7.key_transposed',\n",
       " 'h.10.attn.7.q_head',\n",
       " 'h.10.attn.7.v_head',\n",
       " 'h.10.attn.8.attn_score',\n",
       " 'h.10.attn.8.attn_score_scaled',\n",
       " 'h.10.attn.8.attn_weight',\n",
       " 'h.10.attn.8.ctx_vec',\n",
       " 'h.10.attn.8.k_head',\n",
       " 'h.10.attn.8.key_transposed',\n",
       " 'h.10.attn.8.q_head',\n",
       " 'h.10.attn.8.v_head',\n",
       " 'h.10.attn.9.attn_score',\n",
       " 'h.10.attn.9.attn_score_scaled',\n",
       " 'h.10.attn.9.attn_weight',\n",
       " 'h.10.attn.9.ctx_vec',\n",
       " 'h.10.attn.9.k_head',\n",
       " 'h.10.attn.9.key_transposed',\n",
       " 'h.10.attn.9.q_head',\n",
       " 'h.10.attn.9.v_head',\n",
       " 'h.10.attn.c_attn.a',\n",
       " 'h.10.attn.c_attn.output',\n",
       " 'h.10.attn.c_proj.a',\n",
       " 'h.10.attn.c_proj.output',\n",
       " 'h.10.attn.concat_vecs',\n",
       " 'h.10.attn.k',\n",
       " 'h.10.attn.output',\n",
       " 'h.10.attn.q',\n",
       " 'h.10.attn.v',\n",
       " 'h.10.input_embedding',\n",
       " 'h.10.ln_1',\n",
       " 'h.10.ln_1.mean_var',\n",
       " 'h.10.ln_1.x_norm',\n",
       " 'h.10.ln_1.x_norm_scaled',\n",
       " 'h.10.ln_1.x_norm_shifted',\n",
       " 'h.10.ln_2',\n",
       " 'h.10.ln_2.mean_var',\n",
       " 'h.10.ln_2.x_norm',\n",
       " 'h.10.ln_2.x_norm_scaled',\n",
       " 'h.10.ln_2.x_norm_shifted',\n",
       " 'h.10.mlp.c_fc.a',\n",
       " 'h.10.mlp.c_fc.output',\n",
       " 'h.10.mlp.c_proj.a',\n",
       " 'h.10.mlp.c_proj.output',\n",
       " 'h.10.mlp.gelu',\n",
       " 'h.10.mlp.output',\n",
       " 'h.10.output',\n",
       " 'h.10.resid.1',\n",
       " 'h.10.resid.2',\n",
       " 'h.11.attn.0.attn_score',\n",
       " 'h.11.attn.0.attn_score_scaled',\n",
       " 'h.11.attn.0.attn_weight',\n",
       " 'h.11.attn.0.ctx_vec',\n",
       " 'h.11.attn.0.k_head',\n",
       " 'h.11.attn.0.key_transposed',\n",
       " 'h.11.attn.0.q_head',\n",
       " 'h.11.attn.0.v_head',\n",
       " 'h.11.attn.1.attn_score',\n",
       " 'h.11.attn.1.attn_score_scaled',\n",
       " 'h.11.attn.1.attn_weight',\n",
       " 'h.11.attn.1.ctx_vec',\n",
       " 'h.11.attn.1.k_head',\n",
       " 'h.11.attn.1.key_transposed',\n",
       " 'h.11.attn.1.q_head',\n",
       " 'h.11.attn.1.v_head',\n",
       " 'h.11.attn.10.attn_score',\n",
       " 'h.11.attn.10.attn_score_scaled',\n",
       " 'h.11.attn.10.attn_weight',\n",
       " 'h.11.attn.10.ctx_vec',\n",
       " 'h.11.attn.10.k_head',\n",
       " 'h.11.attn.10.key_transposed',\n",
       " 'h.11.attn.10.q_head',\n",
       " 'h.11.attn.10.v_head',\n",
       " 'h.11.attn.11.attn_score',\n",
       " 'h.11.attn.11.attn_score_scaled',\n",
       " 'h.11.attn.11.attn_weight',\n",
       " 'h.11.attn.11.ctx_vec',\n",
       " 'h.11.attn.11.k_head',\n",
       " 'h.11.attn.11.key_transposed',\n",
       " 'h.11.attn.11.q_head',\n",
       " 'h.11.attn.11.v_head',\n",
       " 'h.11.attn.2.attn_score',\n",
       " 'h.11.attn.2.attn_score_scaled',\n",
       " 'h.11.attn.2.attn_weight',\n",
       " 'h.11.attn.2.ctx_vec',\n",
       " 'h.11.attn.2.k_head',\n",
       " 'h.11.attn.2.key_transposed',\n",
       " 'h.11.attn.2.q_head',\n",
       " 'h.11.attn.2.v_head',\n",
       " 'h.11.attn.3.attn_score',\n",
       " 'h.11.attn.3.attn_score_scaled',\n",
       " 'h.11.attn.3.attn_weight',\n",
       " 'h.11.attn.3.ctx_vec',\n",
       " 'h.11.attn.3.k_head',\n",
       " 'h.11.attn.3.key_transposed',\n",
       " 'h.11.attn.3.q_head',\n",
       " 'h.11.attn.3.v_head',\n",
       " 'h.11.attn.4.attn_score',\n",
       " 'h.11.attn.4.attn_score_scaled',\n",
       " 'h.11.attn.4.attn_weight',\n",
       " 'h.11.attn.4.ctx_vec',\n",
       " 'h.11.attn.4.k_head',\n",
       " 'h.11.attn.4.key_transposed',\n",
       " 'h.11.attn.4.q_head',\n",
       " 'h.11.attn.4.v_head',\n",
       " 'h.11.attn.5.attn_score',\n",
       " 'h.11.attn.5.attn_score_scaled',\n",
       " 'h.11.attn.5.attn_weight',\n",
       " 'h.11.attn.5.ctx_vec',\n",
       " 'h.11.attn.5.k_head',\n",
       " 'h.11.attn.5.key_transposed',\n",
       " 'h.11.attn.5.q_head',\n",
       " 'h.11.attn.5.v_head',\n",
       " 'h.11.attn.6.attn_score',\n",
       " 'h.11.attn.6.attn_score_scaled',\n",
       " 'h.11.attn.6.attn_weight',\n",
       " 'h.11.attn.6.ctx_vec',\n",
       " 'h.11.attn.6.k_head',\n",
       " 'h.11.attn.6.key_transposed',\n",
       " 'h.11.attn.6.q_head',\n",
       " 'h.11.attn.6.v_head',\n",
       " 'h.11.attn.7.attn_score',\n",
       " 'h.11.attn.7.attn_score_scaled',\n",
       " 'h.11.attn.7.attn_weight',\n",
       " 'h.11.attn.7.ctx_vec',\n",
       " 'h.11.attn.7.k_head',\n",
       " 'h.11.attn.7.key_transposed',\n",
       " 'h.11.attn.7.q_head',\n",
       " 'h.11.attn.7.v_head',\n",
       " 'h.11.attn.8.attn_score',\n",
       " 'h.11.attn.8.attn_score_scaled',\n",
       " 'h.11.attn.8.attn_weight',\n",
       " 'h.11.attn.8.ctx_vec',\n",
       " 'h.11.attn.8.k_head',\n",
       " 'h.11.attn.8.key_transposed',\n",
       " 'h.11.attn.8.q_head',\n",
       " 'h.11.attn.8.v_head',\n",
       " 'h.11.attn.9.attn_score',\n",
       " 'h.11.attn.9.attn_score_scaled',\n",
       " 'h.11.attn.9.attn_weight',\n",
       " 'h.11.attn.9.ctx_vec',\n",
       " 'h.11.attn.9.k_head',\n",
       " 'h.11.attn.9.key_transposed',\n",
       " 'h.11.attn.9.q_head',\n",
       " 'h.11.attn.9.v_head',\n",
       " 'h.11.attn.c_attn.a',\n",
       " 'h.11.attn.c_attn.output',\n",
       " 'h.11.attn.c_proj.a',\n",
       " 'h.11.attn.c_proj.output',\n",
       " 'h.11.attn.concat_vecs',\n",
       " 'h.11.attn.k',\n",
       " 'h.11.attn.output',\n",
       " 'h.11.attn.q',\n",
       " 'h.11.attn.v',\n",
       " 'h.11.input_embedding',\n",
       " 'h.11.ln_1',\n",
       " 'h.11.ln_1.mean_var',\n",
       " 'h.11.ln_1.x_norm',\n",
       " 'h.11.ln_1.x_norm_scaled',\n",
       " 'h.11.ln_1.x_norm_shifted',\n",
       " 'h.11.ln_2',\n",
       " 'h.11.ln_2.mean_var',\n",
       " 'h.11.ln_2.x_norm',\n",
       " 'h.11.ln_2.x_norm_scaled',\n",
       " 'h.11.ln_2.x_norm_shifted',\n",
       " 'h.11.mlp.c_fc.a',\n",
       " 'h.11.mlp.c_fc.output',\n",
       " 'h.11.mlp.c_proj.a',\n",
       " 'h.11.mlp.c_proj.output',\n",
       " 'h.11.mlp.gelu',\n",
       " 'h.11.mlp.output',\n",
       " 'h.11.output',\n",
       " 'h.11.resid.1',\n",
       " 'h.11.resid.2',\n",
       " 'h.12.input_embedding',\n",
       " 'h.2.attn.0.attn_score',\n",
       " 'h.2.attn.0.attn_score_scaled',\n",
       " 'h.2.attn.0.attn_weight',\n",
       " 'h.2.attn.0.ctx_vec',\n",
       " 'h.2.attn.0.k_head',\n",
       " 'h.2.attn.0.key_transposed',\n",
       " 'h.2.attn.0.q_head',\n",
       " 'h.2.attn.0.v_head',\n",
       " 'h.2.attn.1.attn_score',\n",
       " 'h.2.attn.1.attn_score_scaled',\n",
       " 'h.2.attn.1.attn_weight',\n",
       " 'h.2.attn.1.ctx_vec',\n",
       " 'h.2.attn.1.k_head',\n",
       " 'h.2.attn.1.key_transposed',\n",
       " 'h.2.attn.1.q_head',\n",
       " 'h.2.attn.1.v_head',\n",
       " 'h.2.attn.10.attn_score',\n",
       " 'h.2.attn.10.attn_score_scaled',\n",
       " 'h.2.attn.10.attn_weight',\n",
       " 'h.2.attn.10.ctx_vec',\n",
       " 'h.2.attn.10.k_head',\n",
       " 'h.2.attn.10.key_transposed',\n",
       " 'h.2.attn.10.q_head',\n",
       " 'h.2.attn.10.v_head',\n",
       " 'h.2.attn.11.attn_score',\n",
       " 'h.2.attn.11.attn_score_scaled',\n",
       " 'h.2.attn.11.attn_weight',\n",
       " 'h.2.attn.11.ctx_vec',\n",
       " 'h.2.attn.11.k_head',\n",
       " 'h.2.attn.11.key_transposed',\n",
       " 'h.2.attn.11.q_head',\n",
       " 'h.2.attn.11.v_head',\n",
       " 'h.2.attn.2.attn_score',\n",
       " 'h.2.attn.2.attn_score_scaled',\n",
       " 'h.2.attn.2.attn_weight',\n",
       " 'h.2.attn.2.ctx_vec',\n",
       " 'h.2.attn.2.k_head',\n",
       " 'h.2.attn.2.key_transposed',\n",
       " 'h.2.attn.2.q_head',\n",
       " 'h.2.attn.2.v_head',\n",
       " 'h.2.attn.3.attn_score',\n",
       " 'h.2.attn.3.attn_score_scaled',\n",
       " 'h.2.attn.3.attn_weight',\n",
       " 'h.2.attn.3.ctx_vec',\n",
       " 'h.2.attn.3.k_head',\n",
       " 'h.2.attn.3.key_transposed',\n",
       " 'h.2.attn.3.q_head',\n",
       " 'h.2.attn.3.v_head',\n",
       " 'h.2.attn.4.attn_score',\n",
       " 'h.2.attn.4.attn_score_scaled',\n",
       " 'h.2.attn.4.attn_weight',\n",
       " 'h.2.attn.4.ctx_vec',\n",
       " 'h.2.attn.4.k_head',\n",
       " 'h.2.attn.4.key_transposed',\n",
       " 'h.2.attn.4.q_head',\n",
       " 'h.2.attn.4.v_head',\n",
       " 'h.2.attn.5.attn_score',\n",
       " 'h.2.attn.5.attn_score_scaled',\n",
       " 'h.2.attn.5.attn_weight',\n",
       " 'h.2.attn.5.ctx_vec',\n",
       " 'h.2.attn.5.k_head',\n",
       " 'h.2.attn.5.key_transposed',\n",
       " 'h.2.attn.5.q_head',\n",
       " 'h.2.attn.5.v_head',\n",
       " 'h.2.attn.6.attn_score',\n",
       " 'h.2.attn.6.attn_score_scaled',\n",
       " 'h.2.attn.6.attn_weight',\n",
       " 'h.2.attn.6.ctx_vec',\n",
       " 'h.2.attn.6.k_head',\n",
       " 'h.2.attn.6.key_transposed',\n",
       " 'h.2.attn.6.q_head',\n",
       " 'h.2.attn.6.v_head',\n",
       " 'h.2.attn.7.attn_score',\n",
       " 'h.2.attn.7.attn_score_scaled',\n",
       " 'h.2.attn.7.attn_weight',\n",
       " 'h.2.attn.7.ctx_vec',\n",
       " 'h.2.attn.7.k_head',\n",
       " 'h.2.attn.7.key_transposed',\n",
       " 'h.2.attn.7.q_head',\n",
       " 'h.2.attn.7.v_head',\n",
       " 'h.2.attn.8.attn_score',\n",
       " 'h.2.attn.8.attn_score_scaled',\n",
       " 'h.2.attn.8.attn_weight',\n",
       " 'h.2.attn.8.ctx_vec',\n",
       " 'h.2.attn.8.k_head',\n",
       " 'h.2.attn.8.key_transposed',\n",
       " 'h.2.attn.8.q_head',\n",
       " 'h.2.attn.8.v_head',\n",
       " 'h.2.attn.9.attn_score',\n",
       " 'h.2.attn.9.attn_score_scaled',\n",
       " 'h.2.attn.9.attn_weight',\n",
       " 'h.2.attn.9.ctx_vec',\n",
       " 'h.2.attn.9.k_head',\n",
       " 'h.2.attn.9.key_transposed',\n",
       " 'h.2.attn.9.q_head',\n",
       " 'h.2.attn.9.v_head',\n",
       " 'h.2.attn.c_attn.a',\n",
       " 'h.2.attn.c_attn.output',\n",
       " 'h.2.attn.c_proj.a',\n",
       " 'h.2.attn.c_proj.output',\n",
       " 'h.2.attn.concat_vecs',\n",
       " 'h.2.attn.k',\n",
       " 'h.2.attn.output',\n",
       " 'h.2.attn.q',\n",
       " 'h.2.attn.v',\n",
       " 'h.2.input_embedding',\n",
       " 'h.2.ln_1',\n",
       " 'h.2.ln_1.mean_var',\n",
       " 'h.2.ln_1.x_norm',\n",
       " 'h.2.ln_1.x_norm_scaled',\n",
       " 'h.2.ln_1.x_norm_shifted',\n",
       " 'h.2.ln_2',\n",
       " 'h.2.ln_2.mean_var',\n",
       " 'h.2.ln_2.x_norm',\n",
       " 'h.2.ln_2.x_norm_scaled',\n",
       " 'h.2.ln_2.x_norm_shifted',\n",
       " 'h.2.mlp.c_fc.a',\n",
       " 'h.2.mlp.c_fc.output',\n",
       " 'h.2.mlp.c_proj.a',\n",
       " 'h.2.mlp.c_proj.output',\n",
       " 'h.2.mlp.gelu',\n",
       " 'h.2.mlp.output',\n",
       " 'h.2.output',\n",
       " 'h.2.resid.1',\n",
       " 'h.2.resid.2',\n",
       " 'h.3.attn.0.attn_score',\n",
       " 'h.3.attn.0.attn_score_scaled',\n",
       " 'h.3.attn.0.attn_weight',\n",
       " 'h.3.attn.0.ctx_vec',\n",
       " 'h.3.attn.0.k_head',\n",
       " 'h.3.attn.0.key_transposed',\n",
       " 'h.3.attn.0.q_head',\n",
       " 'h.3.attn.0.v_head',\n",
       " 'h.3.attn.1.attn_score',\n",
       " 'h.3.attn.1.attn_score_scaled',\n",
       " 'h.3.attn.1.attn_weight',\n",
       " 'h.3.attn.1.ctx_vec',\n",
       " 'h.3.attn.1.k_head',\n",
       " 'h.3.attn.1.key_transposed',\n",
       " 'h.3.attn.1.q_head',\n",
       " 'h.3.attn.1.v_head',\n",
       " 'h.3.attn.10.attn_score',\n",
       " 'h.3.attn.10.attn_score_scaled',\n",
       " 'h.3.attn.10.attn_weight',\n",
       " 'h.3.attn.10.ctx_vec',\n",
       " 'h.3.attn.10.k_head',\n",
       " 'h.3.attn.10.key_transposed',\n",
       " 'h.3.attn.10.q_head',\n",
       " 'h.3.attn.10.v_head',\n",
       " 'h.3.attn.11.attn_score',\n",
       " 'h.3.attn.11.attn_score_scaled',\n",
       " 'h.3.attn.11.attn_weight',\n",
       " 'h.3.attn.11.ctx_vec',\n",
       " 'h.3.attn.11.k_head',\n",
       " 'h.3.attn.11.key_transposed',\n",
       " 'h.3.attn.11.q_head',\n",
       " 'h.3.attn.11.v_head',\n",
       " 'h.3.attn.2.attn_score',\n",
       " 'h.3.attn.2.attn_score_scaled',\n",
       " 'h.3.attn.2.attn_weight',\n",
       " 'h.3.attn.2.ctx_vec',\n",
       " 'h.3.attn.2.k_head',\n",
       " 'h.3.attn.2.key_transposed',\n",
       " 'h.3.attn.2.q_head',\n",
       " 'h.3.attn.2.v_head',\n",
       " 'h.3.attn.3.attn_score',\n",
       " 'h.3.attn.3.attn_score_scaled',\n",
       " 'h.3.attn.3.attn_weight',\n",
       " 'h.3.attn.3.ctx_vec',\n",
       " 'h.3.attn.3.k_head',\n",
       " 'h.3.attn.3.key_transposed',\n",
       " 'h.3.attn.3.q_head',\n",
       " 'h.3.attn.3.v_head',\n",
       " 'h.3.attn.4.attn_score',\n",
       " 'h.3.attn.4.attn_score_scaled',\n",
       " 'h.3.attn.4.attn_weight',\n",
       " 'h.3.attn.4.ctx_vec',\n",
       " 'h.3.attn.4.k_head',\n",
       " 'h.3.attn.4.key_transposed',\n",
       " 'h.3.attn.4.q_head',\n",
       " 'h.3.attn.4.v_head',\n",
       " 'h.3.attn.5.attn_score',\n",
       " 'h.3.attn.5.attn_score_scaled',\n",
       " 'h.3.attn.5.attn_weight',\n",
       " 'h.3.attn.5.ctx_vec',\n",
       " 'h.3.attn.5.k_head',\n",
       " 'h.3.attn.5.key_transposed',\n",
       " 'h.3.attn.5.q_head',\n",
       " 'h.3.attn.5.v_head',\n",
       " 'h.3.attn.6.attn_score',\n",
       " 'h.3.attn.6.attn_score_scaled',\n",
       " 'h.3.attn.6.attn_weight',\n",
       " 'h.3.attn.6.ctx_vec',\n",
       " 'h.3.attn.6.k_head',\n",
       " 'h.3.attn.6.key_transposed',\n",
       " 'h.3.attn.6.q_head',\n",
       " 'h.3.attn.6.v_head',\n",
       " 'h.3.attn.7.attn_score',\n",
       " 'h.3.attn.7.attn_score_scaled',\n",
       " 'h.3.attn.7.attn_weight',\n",
       " 'h.3.attn.7.ctx_vec',\n",
       " 'h.3.attn.7.k_head',\n",
       " 'h.3.attn.7.key_transposed',\n",
       " 'h.3.attn.7.q_head',\n",
       " 'h.3.attn.7.v_head',\n",
       " 'h.3.attn.8.attn_score',\n",
       " 'h.3.attn.8.attn_score_scaled',\n",
       " 'h.3.attn.8.attn_weight',\n",
       " 'h.3.attn.8.ctx_vec',\n",
       " 'h.3.attn.8.k_head',\n",
       " 'h.3.attn.8.key_transposed',\n",
       " 'h.3.attn.8.q_head',\n",
       " 'h.3.attn.8.v_head',\n",
       " 'h.3.attn.9.attn_score',\n",
       " 'h.3.attn.9.attn_score_scaled',\n",
       " 'h.3.attn.9.attn_weight',\n",
       " 'h.3.attn.9.ctx_vec',\n",
       " 'h.3.attn.9.k_head',\n",
       " 'h.3.attn.9.key_transposed',\n",
       " 'h.3.attn.9.q_head',\n",
       " 'h.3.attn.9.v_head',\n",
       " 'h.3.attn.c_attn.a',\n",
       " 'h.3.attn.c_attn.output',\n",
       " 'h.3.attn.c_proj.a',\n",
       " 'h.3.attn.c_proj.output',\n",
       " 'h.3.attn.concat_vecs',\n",
       " 'h.3.attn.k',\n",
       " 'h.3.attn.output',\n",
       " 'h.3.attn.q',\n",
       " 'h.3.attn.v',\n",
       " 'h.3.input_embedding',\n",
       " 'h.3.ln_1',\n",
       " 'h.3.ln_1.mean_var',\n",
       " 'h.3.ln_1.x_norm',\n",
       " 'h.3.ln_1.x_norm_scaled',\n",
       " 'h.3.ln_1.x_norm_shifted',\n",
       " 'h.3.ln_2',\n",
       " 'h.3.ln_2.mean_var',\n",
       " 'h.3.ln_2.x_norm',\n",
       " 'h.3.ln_2.x_norm_scaled',\n",
       " 'h.3.ln_2.x_norm_shifted',\n",
       " 'h.3.mlp.c_fc.a',\n",
       " 'h.3.mlp.c_fc.output',\n",
       " 'h.3.mlp.c_proj.a',\n",
       " 'h.3.mlp.c_proj.output',\n",
       " 'h.3.mlp.gelu',\n",
       " 'h.3.mlp.output',\n",
       " 'h.3.output',\n",
       " 'h.3.resid.1',\n",
       " 'h.3.resid.2',\n",
       " 'h.4.attn.0.attn_score',\n",
       " 'h.4.attn.0.attn_score_scaled',\n",
       " 'h.4.attn.0.attn_weight',\n",
       " 'h.4.attn.0.ctx_vec',\n",
       " 'h.4.attn.0.k_head',\n",
       " 'h.4.attn.0.key_transposed',\n",
       " 'h.4.attn.0.q_head',\n",
       " 'h.4.attn.0.v_head',\n",
       " 'h.4.attn.1.attn_score',\n",
       " 'h.4.attn.1.attn_score_scaled',\n",
       " 'h.4.attn.1.attn_weight',\n",
       " 'h.4.attn.1.ctx_vec',\n",
       " 'h.4.attn.1.k_head',\n",
       " 'h.4.attn.1.key_transposed',\n",
       " 'h.4.attn.1.q_head',\n",
       " 'h.4.attn.1.v_head',\n",
       " 'h.4.attn.10.attn_score',\n",
       " 'h.4.attn.10.attn_score_scaled',\n",
       " 'h.4.attn.10.attn_weight',\n",
       " 'h.4.attn.10.ctx_vec',\n",
       " 'h.4.attn.10.k_head',\n",
       " 'h.4.attn.10.key_transposed',\n",
       " 'h.4.attn.10.q_head',\n",
       " 'h.4.attn.10.v_head',\n",
       " 'h.4.attn.11.attn_score',\n",
       " 'h.4.attn.11.attn_score_scaled',\n",
       " 'h.4.attn.11.attn_weight',\n",
       " 'h.4.attn.11.ctx_vec',\n",
       " 'h.4.attn.11.k_head',\n",
       " 'h.4.attn.11.key_transposed',\n",
       " 'h.4.attn.11.q_head',\n",
       " 'h.4.attn.11.v_head',\n",
       " 'h.4.attn.2.attn_score',\n",
       " 'h.4.attn.2.attn_score_scaled',\n",
       " 'h.4.attn.2.attn_weight',\n",
       " 'h.4.attn.2.ctx_vec',\n",
       " 'h.4.attn.2.k_head',\n",
       " 'h.4.attn.2.key_transposed',\n",
       " 'h.4.attn.2.q_head',\n",
       " 'h.4.attn.2.v_head',\n",
       " 'h.4.attn.3.attn_score',\n",
       " 'h.4.attn.3.attn_score_scaled',\n",
       " 'h.4.attn.3.attn_weight',\n",
       " 'h.4.attn.3.ctx_vec',\n",
       " 'h.4.attn.3.k_head',\n",
       " 'h.4.attn.3.key_transposed',\n",
       " 'h.4.attn.3.q_head',\n",
       " 'h.4.attn.3.v_head',\n",
       " 'h.4.attn.4.attn_score',\n",
       " 'h.4.attn.4.attn_score_scaled',\n",
       " 'h.4.attn.4.attn_weight',\n",
       " 'h.4.attn.4.ctx_vec',\n",
       " 'h.4.attn.4.k_head',\n",
       " 'h.4.attn.4.key_transposed',\n",
       " 'h.4.attn.4.q_head',\n",
       " 'h.4.attn.4.v_head',\n",
       " 'h.4.attn.5.attn_score',\n",
       " 'h.4.attn.5.attn_score_scaled',\n",
       " 'h.4.attn.5.attn_weight',\n",
       " 'h.4.attn.5.ctx_vec',\n",
       " 'h.4.attn.5.k_head',\n",
       " 'h.4.attn.5.key_transposed',\n",
       " 'h.4.attn.5.q_head',\n",
       " 'h.4.attn.5.v_head',\n",
       " 'h.4.attn.6.attn_score',\n",
       " 'h.4.attn.6.attn_score_scaled',\n",
       " 'h.4.attn.6.attn_weight',\n",
       " 'h.4.attn.6.ctx_vec',\n",
       " 'h.4.attn.6.k_head',\n",
       " 'h.4.attn.6.key_transposed',\n",
       " 'h.4.attn.6.q_head',\n",
       " 'h.4.attn.6.v_head',\n",
       " 'h.4.attn.7.attn_score',\n",
       " 'h.4.attn.7.attn_score_scaled',\n",
       " 'h.4.attn.7.attn_weight',\n",
       " 'h.4.attn.7.ctx_vec',\n",
       " 'h.4.attn.7.k_head',\n",
       " 'h.4.attn.7.key_transposed',\n",
       " 'h.4.attn.7.q_head',\n",
       " 'h.4.attn.7.v_head',\n",
       " 'h.4.attn.8.attn_score',\n",
       " 'h.4.attn.8.attn_score_scaled',\n",
       " 'h.4.attn.8.attn_weight',\n",
       " 'h.4.attn.8.ctx_vec',\n",
       " 'h.4.attn.8.k_head',\n",
       " 'h.4.attn.8.key_transposed',\n",
       " 'h.4.attn.8.q_head',\n",
       " 'h.4.attn.8.v_head',\n",
       " 'h.4.attn.9.attn_score',\n",
       " 'h.4.attn.9.attn_score_scaled',\n",
       " 'h.4.attn.9.attn_weight',\n",
       " 'h.4.attn.9.ctx_vec',\n",
       " 'h.4.attn.9.k_head',\n",
       " 'h.4.attn.9.key_transposed',\n",
       " 'h.4.attn.9.q_head',\n",
       " 'h.4.attn.9.v_head',\n",
       " 'h.4.attn.c_attn.a',\n",
       " 'h.4.attn.c_attn.output',\n",
       " 'h.4.attn.c_proj.a',\n",
       " 'h.4.attn.c_proj.output',\n",
       " 'h.4.attn.concat_vecs',\n",
       " 'h.4.attn.k',\n",
       " 'h.4.attn.output',\n",
       " 'h.4.attn.q',\n",
       " 'h.4.attn.v',\n",
       " 'h.4.input_embedding',\n",
       " 'h.4.ln_1',\n",
       " 'h.4.ln_1.mean_var',\n",
       " 'h.4.ln_1.x_norm',\n",
       " 'h.4.ln_1.x_norm_scaled',\n",
       " 'h.4.ln_1.x_norm_shifted',\n",
       " 'h.4.ln_2',\n",
       " 'h.4.ln_2.mean_var',\n",
       " 'h.4.ln_2.x_norm',\n",
       " 'h.4.ln_2.x_norm_scaled',\n",
       " 'h.4.ln_2.x_norm_shifted',\n",
       " 'h.4.mlp.c_fc.a',\n",
       " 'h.4.mlp.c_fc.output',\n",
       " 'h.4.mlp.c_proj.a',\n",
       " 'h.4.mlp.c_proj.output',\n",
       " 'h.4.mlp.gelu',\n",
       " 'h.4.mlp.output',\n",
       " 'h.4.output',\n",
       " 'h.4.resid.1',\n",
       " 'h.4.resid.2',\n",
       " 'h.5.attn.0.attn_score',\n",
       " 'h.5.attn.0.attn_score_scaled',\n",
       " 'h.5.attn.0.attn_weight',\n",
       " 'h.5.attn.0.ctx_vec',\n",
       " 'h.5.attn.0.k_head',\n",
       " 'h.5.attn.0.key_transposed',\n",
       " 'h.5.attn.0.q_head',\n",
       " 'h.5.attn.0.v_head',\n",
       " 'h.5.attn.1.attn_score',\n",
       " 'h.5.attn.1.attn_score_scaled',\n",
       " 'h.5.attn.1.attn_weight',\n",
       " 'h.5.attn.1.ctx_vec',\n",
       " 'h.5.attn.1.k_head',\n",
       " 'h.5.attn.1.key_transposed',\n",
       " 'h.5.attn.1.q_head',\n",
       " 'h.5.attn.1.v_head',\n",
       " 'h.5.attn.10.attn_score',\n",
       " 'h.5.attn.10.attn_score_scaled',\n",
       " 'h.5.attn.10.attn_weight',\n",
       " 'h.5.attn.10.ctx_vec',\n",
       " 'h.5.attn.10.k_head',\n",
       " 'h.5.attn.10.key_transposed',\n",
       " 'h.5.attn.10.q_head',\n",
       " 'h.5.attn.10.v_head',\n",
       " 'h.5.attn.11.attn_score',\n",
       " 'h.5.attn.11.attn_score_scaled',\n",
       " 'h.5.attn.11.attn_weight',\n",
       " 'h.5.attn.11.ctx_vec',\n",
       " 'h.5.attn.11.k_head',\n",
       " 'h.5.attn.11.key_transposed',\n",
       " 'h.5.attn.11.q_head',\n",
       " 'h.5.attn.11.v_head',\n",
       " 'h.5.attn.2.attn_score',\n",
       " 'h.5.attn.2.attn_score_scaled',\n",
       " 'h.5.attn.2.attn_weight',\n",
       " 'h.5.attn.2.ctx_vec',\n",
       " 'h.5.attn.2.k_head',\n",
       " 'h.5.attn.2.key_transposed',\n",
       " 'h.5.attn.2.q_head',\n",
       " 'h.5.attn.2.v_head',\n",
       " 'h.5.attn.3.attn_score',\n",
       " 'h.5.attn.3.attn_score_scaled',\n",
       " 'h.5.attn.3.attn_weight',\n",
       " 'h.5.attn.3.ctx_vec',\n",
       " 'h.5.attn.3.k_head',\n",
       " 'h.5.attn.3.key_transposed',\n",
       " 'h.5.attn.3.q_head',\n",
       " 'h.5.attn.3.v_head',\n",
       " 'h.5.attn.4.attn_score',\n",
       " 'h.5.attn.4.attn_score_scaled',\n",
       " 'h.5.attn.4.attn_weight',\n",
       " 'h.5.attn.4.ctx_vec',\n",
       " 'h.5.attn.4.k_head',\n",
       " 'h.5.attn.4.key_transposed',\n",
       " 'h.5.attn.4.q_head',\n",
       " 'h.5.attn.4.v_head',\n",
       " 'h.5.attn.5.attn_score',\n",
       " 'h.5.attn.5.attn_score_scaled',\n",
       " 'h.5.attn.5.attn_weight',\n",
       " 'h.5.attn.5.ctx_vec',\n",
       " 'h.5.attn.5.k_head',\n",
       " 'h.5.attn.5.key_transposed',\n",
       " 'h.5.attn.5.q_head',\n",
       " 'h.5.attn.5.v_head',\n",
       " 'h.5.attn.6.attn_score',\n",
       " 'h.5.attn.6.attn_score_scaled',\n",
       " 'h.5.attn.6.attn_weight',\n",
       " 'h.5.attn.6.ctx_vec',\n",
       " 'h.5.attn.6.k_head',\n",
       " 'h.5.attn.6.key_transposed',\n",
       " 'h.5.attn.6.q_head',\n",
       " 'h.5.attn.6.v_head',\n",
       " 'h.5.attn.7.attn_score',\n",
       " 'h.5.attn.7.attn_score_scaled',\n",
       " 'h.5.attn.7.attn_weight',\n",
       " 'h.5.attn.7.ctx_vec',\n",
       " 'h.5.attn.7.k_head',\n",
       " 'h.5.attn.7.key_transposed',\n",
       " 'h.5.attn.7.q_head',\n",
       " 'h.5.attn.7.v_head',\n",
       " 'h.5.attn.8.attn_score',\n",
       " 'h.5.attn.8.attn_score_scaled',\n",
       " 'h.5.attn.8.attn_weight',\n",
       " 'h.5.attn.8.ctx_vec',\n",
       " 'h.5.attn.8.k_head',\n",
       " 'h.5.attn.8.key_transposed',\n",
       " 'h.5.attn.8.q_head',\n",
       " 'h.5.attn.8.v_head',\n",
       " 'h.5.attn.9.attn_score',\n",
       " 'h.5.attn.9.attn_score_scaled',\n",
       " 'h.5.attn.9.attn_weight',\n",
       " 'h.5.attn.9.ctx_vec',\n",
       " 'h.5.attn.9.k_head',\n",
       " 'h.5.attn.9.key_transposed',\n",
       " 'h.5.attn.9.q_head',\n",
       " 'h.5.attn.9.v_head',\n",
       " 'h.5.attn.c_attn.a',\n",
       " 'h.5.attn.c_attn.output',\n",
       " 'h.5.attn.c_proj.a',\n",
       " 'h.5.attn.c_proj.output',\n",
       " 'h.5.attn.concat_vecs',\n",
       " 'h.5.attn.k',\n",
       " 'h.5.attn.output',\n",
       " 'h.5.attn.q',\n",
       " 'h.5.attn.v',\n",
       " 'h.5.input_embedding',\n",
       " 'h.5.ln_1',\n",
       " 'h.5.ln_1.mean_var',\n",
       " 'h.5.ln_1.x_norm',\n",
       " 'h.5.ln_1.x_norm_scaled',\n",
       " 'h.5.ln_1.x_norm_shifted',\n",
       " 'h.5.ln_2',\n",
       " 'h.5.ln_2.mean_var',\n",
       " 'h.5.ln_2.x_norm',\n",
       " 'h.5.ln_2.x_norm_scaled',\n",
       " 'h.5.ln_2.x_norm_shifted',\n",
       " 'h.5.mlp.c_fc.a',\n",
       " 'h.5.mlp.c_fc.output',\n",
       " 'h.5.mlp.c_proj.a',\n",
       " 'h.5.mlp.c_proj.output',\n",
       " 'h.5.mlp.gelu',\n",
       " 'h.5.mlp.output',\n",
       " 'h.5.output',\n",
       " 'h.5.resid.1',\n",
       " ...]"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(output_c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "d4a3d0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_1 weight shape: torch.Size([768])\n",
      "ln_1 bias shape:   torch.Size([768])\n",
      "\n",
      "ln_1 weight:\n",
      "Parameter containing:\n",
      "tensor([0.2232, 0.1820, 0.1534, 0.1917, 0.2036, 0.1948, 0.1467, 0.1865, 0.2143,\n",
      "        0.1956, 0.2118, 0.2153, 0.1882, 0.2074, 0.1871, 0.2040, 0.2044, 0.1900,\n",
      "        0.1952, 0.0475, 0.1909, 0.2115, 0.1971, 0.2202, 0.1998, 0.2108, 0.2303,\n",
      "        0.1879, 0.1939, 0.2018, 0.1891, 0.1861, 0.1958, 0.1832, 0.1978, 0.2243,\n",
      "        0.0706, 0.1958, 0.1943, 0.1939, 0.1978, 0.1951, 0.1995, 0.1912, 0.2083,\n",
      "        0.2037, 0.1849, 0.1945, 0.2189, 0.0419, 0.1977, 0.1979, 0.0608, 0.1824,\n",
      "        0.2055, 0.0476, 0.1892, 0.2079, 0.2047, 0.2233, 0.2097, 0.2075, 0.2076,\n",
      "        0.1793, 0.1312, 0.1841, 0.1939, 0.1561, 0.0577, 0.1948, 0.2048, 0.1717,\n",
      "        0.1942, 0.1708, 0.1989, 0.1993, 0.2082, 0.1071, 0.1968, 0.1770, 0.2164,\n",
      "        0.1864, 0.1938, 0.2184, 0.1343, 0.1707, 0.0683, 0.1401, 0.1823, 0.2045,\n",
      "        0.2007, 0.1853, 0.1783, 0.1889, 0.1870, 0.1975, 0.2114, 0.2108, 0.2083,\n",
      "        0.2409, 0.1938, 0.2022, 0.0857, 0.1823, 0.1879, 0.1979, 0.1850, 0.1029,\n",
      "        0.1762, 0.1953, 0.2231, 0.2006, 0.2022, 0.2134, 0.1970, 0.1820, 0.0568,\n",
      "        0.2269, 0.1882, 0.1770, 0.1880, 0.1910, 0.1872, 0.1613, 0.1946, 0.1930,\n",
      "        0.1981, 0.2030, 0.1848, 0.2341, 0.1832, 0.1893, 0.2368, 0.2085, 0.1833,\n",
      "        0.2083, 0.2009, 0.2212, 0.1342, 0.0614, 0.1913, 0.1812, 0.1041, 0.1957,\n",
      "        0.1902, 0.1355, 0.2145, 0.1974, 0.1904, 0.1997, 0.1849, 0.1776, 0.2038,\n",
      "        0.1773, 0.1878, 0.1793, 0.1960, 0.1935, 0.1786, 0.1532, 0.1185, 0.2015,\n",
      "        0.1907, 0.2112, 0.1967, 0.2037, 0.1994, 0.0528, 0.1832, 0.1633, 0.1812,\n",
      "        0.1988, 0.1742, 0.2177, 0.1901, 0.1778, 0.0706, 0.1987, 0.2417, 0.1658,\n",
      "        0.1840, 0.1763, 0.1950, 0.2085, 0.1906, 0.2025, 0.1713, 0.2475, 0.1939,\n",
      "        0.1755, 0.1929, 0.1378, 0.1944, 0.1803, 0.1839, 0.1617, 0.1919, 0.1710,\n",
      "        0.1861, 0.1589, 0.2092, 0.2252, 0.1949, 0.2080, 0.1775, 0.1984, 0.1842,\n",
      "        0.1919, 0.2261, 0.1953, 0.1940, 0.2496, 0.2153, 0.0501, 0.1797, 0.2050,\n",
      "        0.2279, 0.1993, 0.2056, 0.2081, 0.1783, 0.1789, 0.1948, 0.1909, 0.1657,\n",
      "        0.1894, 0.1901, 0.1939, 0.1997, 0.1939, 0.1790, 0.2071, 0.1217, 0.1811,\n",
      "        0.1743, 0.2202, 0.1910, 0.1884, 0.2246, 0.1929, 0.2057, 0.1751, 0.1968,\n",
      "        0.1799, 0.1807, 0.1856, 0.1968, 0.2126, 0.1744, 0.2287, 0.1813, 0.2130,\n",
      "        0.2163, 0.2274, 0.1855, 0.1663, 0.1714, 0.1806, 0.1870, 0.2162, 0.1918,\n",
      "        0.1621, 0.1802, 0.2120, 0.1645, 0.2075, 0.1612, 0.1757, 0.1968, 0.2067,\n",
      "        0.0430, 0.0715, 0.1992, 0.1713, 0.2130, 0.2078, 0.0492, 0.1846, 0.2049,\n",
      "        0.1995, 0.1914, 0.1975, 0.1758, 0.2057, 0.1663, 0.2204, 0.2045, 0.1877,\n",
      "        0.0608, 0.0551, 0.2212, 0.1949, 0.1891, 0.2025, 0.1979, 0.1851, 0.1910,\n",
      "        0.1713, 0.1884, 0.1987, 0.0643, 0.1914, 0.2395, 0.1831, 0.1902, 0.1741,\n",
      "        0.1919, 0.1812, 0.0681, 0.2024, 0.1959, 0.0526, 0.1893, 0.2065, 0.0969,\n",
      "        0.1988, 0.1940, 0.1956, 0.2085, 0.2012, 0.0678, 0.1812, 0.1821, 0.1736,\n",
      "        0.1892, 0.1933, 0.0839, 0.1738, 0.2093, 0.1908, 0.1714, 0.1975, 0.2014,\n",
      "        0.1891, 0.1953, 0.2019, 0.2165, 0.1870, 0.1935, 0.2164, 0.1846, 0.1841,\n",
      "        0.1762, 0.2349, 0.1905, 0.1667, 0.1910, 0.2093, 0.1944, 0.2072, 0.2027,\n",
      "        0.0504, 0.1939, 0.2013, 0.1845, 0.1919, 0.0686, 0.1734, 0.1742, 0.1937,\n",
      "        0.2194, 0.0626, 0.0836, 0.1880, 0.1772, 0.0583, 0.2104, 0.1748, 0.1763,\n",
      "        0.1865, 0.2027, 0.1951, 0.2061, 0.1503, 0.0895, 0.1831, 0.1987, 0.0482,\n",
      "        0.1990, 0.2252, 0.2008, 0.1842, 0.1812, 0.2108, 0.2153, 0.1854, 0.2347,\n",
      "        0.1963, 0.2036, 0.1302, 0.2048, 0.2017, 0.2270, 0.1640, 0.1787, 0.1707,\n",
      "        0.2168, 0.1831, 0.1928, 0.1789, 0.1783, 0.1881, 0.0647, 0.1870, 0.1860,\n",
      "        0.1690, 0.1924, 0.1874, 0.0577, 0.1904, 0.1991, 0.1979, 0.2137, 0.1969,\n",
      "        0.2312, 0.2329, 0.2039, 0.2342, 0.2162, 0.1860, 0.0571, 0.2367, 0.2002,\n",
      "        0.1645, 0.1950, 0.1864, 0.1786, 0.1941, 0.1652, 0.1923, 0.0941, 0.1935,\n",
      "        0.1858, 0.1917, 0.1904, 0.1812, 0.1970, 0.1902, 0.2242, 0.0453, 0.1761,\n",
      "        0.1957, 0.1196, 0.2123, 0.2282, 0.1851, 0.1870, 0.1732, 0.1953, 0.1897,\n",
      "        0.2083, 0.2125, 0.1858, 0.0535, 0.1648, 0.0619, 0.1551, 0.2008, 0.1811,\n",
      "        0.0545, 0.2079, 0.2315, 0.1818, 0.2017, 0.2527, 0.2056, 0.1843, 0.1974,\n",
      "        0.1881, 0.1583, 0.1754, 0.1782, 0.2075, 0.1854, 0.1876, 0.2021, 0.1741,\n",
      "        0.1988, 0.1639, 0.0706, 0.1697, 0.1536, 0.1837, 0.1958, 0.2207, 0.1851,\n",
      "        0.2083, 0.1908, 0.1790, 0.1866, 0.1981, 0.2217, 0.1850, 0.2018, 0.1804,\n",
      "        0.1811, 0.0897, 0.0504, 0.1903, 0.1849, 0.1886, 0.1822, 0.2249, 0.0515,\n",
      "        0.2133, 0.1970, 0.1880, 0.1645, 0.2115, 0.2060, 0.1685, 0.0671, 0.1452,\n",
      "        0.1879, 0.2007, 0.2288, 0.1855, 0.1356, 0.2140, 0.1950, 0.1842, 0.1831,\n",
      "        0.1929, 0.2049, 0.1987, 0.0560, 0.1442, 0.0772, 0.0702, 0.1894, 0.1527,\n",
      "        0.1880, 0.1961, 0.1884, 0.1899, 0.2223, 0.1764, 0.2137, 0.2381, 0.1812,\n",
      "        0.0716, 0.2010, 0.2064, 0.1348, 0.1861, 0.1878, 0.1995, 0.1821, 0.1721,\n",
      "        0.1723, 0.1858, 0.0688, 0.1868, 0.2088, 0.0535, 0.1821, 0.2078, 0.1963,\n",
      "        0.2006, 0.1939, 0.1900, 0.1911, 0.1919, 0.1931, 0.1833, 0.2168, 0.1037,\n",
      "        0.1767, 0.2095, 0.2026, 0.1883, 0.2183, 0.1596, 0.1794, 0.1921, 0.2233,\n",
      "        0.1810, 0.2124, 0.2177, 0.1778, 0.1906, 0.1173, 0.2197, 0.1997, 0.2035,\n",
      "        0.1987, 0.1990, 0.2253, 0.1719, 0.1909, 0.1948, 0.1862, 0.1891, 0.2097,\n",
      "        0.1701, 0.2036, 0.1947, 0.1861, 0.1945, 0.1988, 0.1749, 0.2077, 0.1736,\n",
      "        0.1737, 0.1986, 0.1911, 0.2105, 0.1889, 0.0738, 0.1929, 0.1940, 0.1841,\n",
      "        0.1855, 0.1835, 0.1813, 0.1406, 0.1530, 0.1979, 0.1714, 0.1960, 0.1860,\n",
      "        0.1949, 0.1453, 0.0617, 0.2033, 0.1796, 0.1870, 0.0911, 0.1966, 0.1989,\n",
      "        0.1957, 0.1977, 0.1685, 0.1876, 0.2109, 0.1345, 0.1739, 0.1812, 0.1926,\n",
      "        0.2075, 0.1283, 0.1852, 0.2235, 0.1659, 0.1813, 0.1904, 0.1695, 0.2283,\n",
      "        0.1757, 0.1257, 0.1890, 0.2510, 0.2075, 0.2020, 0.1907, 0.0488, 0.1909,\n",
      "        0.2222, 0.1852, 0.1104, 0.1842, 0.1834, 0.1956, 0.2032, 0.1930, 0.1589,\n",
      "        0.1968, 0.1738, 0.1488, 0.1451, 0.0612, 0.1753, 0.1900, 0.2045, 0.0680,\n",
      "        0.1960, 0.1844, 0.1951, 0.1602, 0.0764, 0.1589, 0.1931, 0.1980, 0.1960,\n",
      "        0.1934, 0.2310, 0.1961, 0.1950, 0.1968, 0.1938, 0.1951, 0.1818, 0.1880,\n",
      "        0.1713, 0.1785, 0.1962, 0.1850, 0.1964, 0.2008, 0.0666, 0.1917, 0.1670,\n",
      "        0.2063, 0.1179, 0.1951, 0.1983, 0.1292, 0.1857, 0.1833, 0.0886, 0.2428,\n",
      "        0.1872, 0.2067, 0.1996, 0.1881, 0.1901, 0.1885, 0.1984, 0.0754, 0.2066,\n",
      "        0.0607, 0.0754, 0.2060, 0.2116, 0.0556, 0.1792, 0.1748, 0.1841, 0.1743,\n",
      "        0.1662, 0.1982, 0.1582, 0.1935, 0.2182, 0.2067, 0.1855, 0.1778, 0.1900,\n",
      "        0.2124, 0.1215, 0.2092, 0.1929, 0.2434, 0.1936, 0.1948, 0.0622, 0.1852,\n",
      "        0.1868, 0.2035, 0.2310, 0.1794, 0.1655, 0.1756, 0.2074, 0.2194, 0.2152,\n",
      "        0.0502, 0.2294, 0.1950, 0.2149, 0.2024, 0.1727, 0.0657, 0.1919, 0.1847,\n",
      "        0.1900, 0.1825, 0.1898], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "# Load GPT-2 small (124M)\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# First transformer block\n",
    "block0 = model.h[0]\n",
    "\n",
    "# First LayerNorm in the block\n",
    "ln1 = block0.ln_1\n",
    "\n",
    "# Print shapes\n",
    "print(\"ln_1 weight shape:\", ln1.weight.shape)\n",
    "print(\"ln_1 bias shape:  \", ln1.bias.shape)\n",
    "\n",
    "# # Print values\n",
    "print(\"\\nln_1 weight:\")\n",
    "print(ln1.weight)\n",
    "\n",
    "# print(\"\\nln_1 bias:\")\n",
    "# print(ln1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "2481c409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID 1127: ines\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def token_to_text(token_id):\n",
    "    # Convert token ID to its token string\n",
    "    return tokenizer.convert_ids_to_tokens(token_id)\n",
    "\n",
    "# Example\n",
    "token_id = 1127\n",
    "\n",
    "token_text = token_to_text(token_id)\n",
    "print(f\"Token ID {token_id}: {token_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "9f239dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# File to store vocab in fixed 64-byte slots\n",
    "with open(\"gpt2_vocab.bin\", \"wb\") as f:\n",
    "    for token_id in range(tokenizer.vocab_size):\n",
    "        # Get decoded token\n",
    "        decoded_text = tokenizer.decode([token_id])\n",
    "        \n",
    "        # Encode as UTF-8 bytes\n",
    "        token_bytes = decoded_text.encode('utf-8')\n",
    "        \n",
    "        # Truncate if longer than 64 bytes\n",
    "        if len(token_bytes) > 64:\n",
    "            token_bytes = token_bytes[:64]\n",
    "        \n",
    "        # Pad with zeros to make it exactly 64 bytes\n",
    "        token_bytes = token_bytes.ljust(64, b'\\x00')\n",
    "        \n",
    "        # Write to file\n",
    "        f.write(token_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753734c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caerus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
